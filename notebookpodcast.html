<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Solution to copy Notebook to do a podcast">

<title>NotebookME – Agents</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="NotebookME – Agents">
<meta property="og:description" content="Solution to copy Notebook to do a podcast">
<meta property="og:site_name" content="Agents">
<meta name="twitter:title" content="NotebookME – Agents">
<meta name="twitter:description" content="Solution to copy Notebook to do a podcast">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Agents</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./notebookpodcast.html">NotebookME</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Agents</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./databricks_text2sql.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Databricks text2sql agent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning_agent copy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PlanningAgent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./realtimestt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">TTS-parler</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./precios.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Precios</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clients.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Clients</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notebookpodcast.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">NotebookME</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#pdf-pre-processing" id="toc-pdf-pre-processing" class="nav-link active" data-scroll-target="#pdf-pre-processing">1: PDF Pre-Processing</a>
  <ul class="collapse">
  <li><a href="#validate_pdf" id="toc-validate_pdf" class="nav-link" data-scroll-target="#validate_pdf">validate_pdf</a></li>
  <li><a href="#pdfprocessor" id="toc-pdfprocessor" class="nav-link" data-scroll-target="#pdfprocessor">PDFProcessor</a></li>
  <li><a href="#pdfprocessor.process_chunks" id="toc-pdfprocessor.process_chunks" class="nav-link" data-scroll-target="#pdfprocessor.process_chunks">PDFProcessor.process_chunks</a></li>
  </ul></li>
  <li><a href="#instantiating-the-pdfprocessor-and-processing-file" id="toc-instantiating-the-pdfprocessor-and-processing-file" class="nav-link" data-scroll-target="#instantiating-the-pdfprocessor-and-processing-file">Instantiating the PDFProcessor, and processing file</a>
  <ul class="collapse">
  <li><a href="#llama-pre-processing" id="toc-llama-pre-processing" class="nav-link" data-scroll-target="#llama-pre-processing">Llama Pre-Processing</a></li>
  </ul></li>
  <li><a href="#transcript-writer" id="toc-transcript-writer" class="nav-link" data-scroll-target="#transcript-writer">2: Transcript Writer</a>
  <ul class="collapse">
  <li><a href="#podcastwriter" id="toc-podcastwriter" class="nav-link" data-scroll-target="#podcastwriter">PodcastWriter</a></li>
  <li><a href="#podcastwriter.generate_podcast" id="toc-podcastwriter.generate_podcast" class="nav-link" data-scroll-target="#podcastwriter.generate_podcast">PodcastWriter.generate_podcast</a></li>
  </ul></li>
  <li><a href="#transcript-re-writer" id="toc-transcript-re-writer" class="nav-link" data-scroll-target="#transcript-re-writer">3: Transcript Re-writer</a>
  <ul class="collapse">
  <li><a href="#podcastwriter.rewrite_podcast" id="toc-podcastwriter.rewrite_podcast" class="nav-link" data-scroll-target="#podcastwriter.rewrite_podcast">PodcastWriter.rewrite_podcast</a></li>
  </ul></li>
  <li><a href="#tts-workflow" id="toc-tts-workflow" class="nav-link" data-scroll-target="#tts-workflow">4: TTS Workflow</a>
  <ul class="collapse">
  <li><a href="#testing-the-audio-generation" id="toc-testing-the-audio-generation" class="nav-link" data-scroll-target="#testing-the-audio-generation">Testing the Audio Generation</a></li>
  </ul></li>
  <li><a href="#bringing-it-together-making-the-podcast" id="toc-bringing-it-together-making-the-podcast" class="nav-link" data-scroll-target="#bringing-it-together-making-the-podcast">Bringing it together: Making the Podcast</a>
  <ul class="collapse">
  <li><a href="#numpy_to_audio_segment" id="toc-numpy_to_audio_segment" class="nav-link" data-scroll-target="#numpy_to_audio_segment">numpy_to_audio_segment</a></li>
  <li><a href="#output-the-podcast" id="toc-output-the-podcast" class="nav-link" data-scroll-target="#output-the-podcast">Output the Podcast</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/Dmaturana81/Agents/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="notebookpodcast.html.md"><i class="bi bi-file-code"></i>CommonMark</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">NotebookME</h1>
</div>

<div>
  <div class="description">
    Solution to copy Notebook to do a podcast
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div id="26dd2282" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>sys.path.append(<span class="st">".."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will encapsulate the whole process in a single notebook. developing some classes that allows us to decide which LLM to use for each step in a easy way. The process and encapsulation will be as follow: 1. One class for the PDF processing, starting from a pdf file or any other docuument, return the text. 2. One class for the podcast writting, using the text, return a podcast transcript. 3. One class for the audio generation, using the transcript, return a podcast audio.</p>
<section id="pdf-pre-processing" class="level2">
<h2 class="anchored" data-anchor-id="pdf-pre-processing">1: PDF Pre-Processing</h2>
<p>The Architecture is:</p>
<p>– Class that will handle the PDF reading - Class will extract the text from the PDF - Class will chunk the text into smaller chunks - Class will process the chunks with the LLM to correct any errors</p>
<p>Assuming you have a PDF uploaded on the same machine, please set the path for the file.</p>
<hr>
<p><a href="https://github.com/Dmaturana81/Agents/blob/main/Agents/notebookAI/preprocess.py#L30" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="validate_pdf" class="level3">
<h3 class="anchored" data-anchor-id="validate_pdf">validate_pdf</h3>
<blockquote class="blockquote">
<pre><code> validate_pdf (file_path:str|pathlib.Path)</code></pre>
</blockquote>
<p>*Validate if the file exists and is a PDF</p>
<p>Arguments: file_path – Path to the file to validate</p>
<p>Returns: bool – True if the file exists and is a PDF, False otherwise*</p>
<hr>
<p><a href="https://github.com/Dmaturana81/Agents/blob/main/Agents/notebookAI/preprocess.py#L49" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="pdfprocessor" class="level3">
<h3 class="anchored" data-anchor-id="pdfprocessor">PDFProcessor</h3>
<blockquote class="blockquote">
<pre><code> PDFProcessor (model:str='llama3.2', provider:str='ollama')</code></pre>
</blockquote>
<p><em>Class to handle PDF processing including text extraction and chunking</em></p>
<hr>
<p><a href="https://github.com/Dmaturana81/Agents/blob/main/Agents/notebookAI/preprocess.py#L168" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="pdfprocessor.process_chunks" class="level3">
<h3 class="anchored" data-anchor-id="pdfprocessor.process_chunks">PDFProcessor.process_chunks</h3>
<blockquote class="blockquote">
<pre><code> PDFProcessor.process_chunks (files:list[str]|None=None)</code></pre>
</blockquote>
<p><em>Process a chunk of text and return both input and output for verification</em></p>
</section>
</section>
<section id="instantiating-the-pdfprocessor-and-processing-file" class="level2">
<h2 class="anchored" data-anchor-id="instantiating-the-pdfprocessor-and-processing-file">Instantiating the PDFProcessor, and processing file</h2>
<p>Once the class is instantiated, we can add the file path and process the chunks: the method <code>add_filepath</code> will add the file path to the class and process the chunks.</p>
<div id="65cd7542" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>file_processor <span class="op">=</span> PDFProcessor()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>file_processor.add_filepath(<span class="st">"./pdfs/ichigo.pdf"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>file_processor.add_filepath(<span class="st">"./pdfs/omniparser.pdf"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cd4068ed" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>file_processor.chunks_num</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>83</code></pre>
</div>
</div>
<div id="f6bc5f1f" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>file_processor.file_paths</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['./pdfs/ichigo.pdf',
 './pdfs/omniparser.pdf',
 './pdfs/ichigo.pdf',
 './pdfs/omniparser.pdf']</code></pre>
</div>
</div>
<div id="1a015017" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>file_processor.num_files</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>4</code></pre>
</div>
</div>
<p>Once a file is added, the processed text is stored in <code>processed_docs</code> attribute of the class:</p>
<div id="71c06b74" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>file_processor.processed_docs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>{'./pdfs/ichigo.pdf': '4 2 0 2\nt c O 0 2\n] L C . s c [\n1 v 6 1 3 5 1 . 0 1 4 2 : v i X r a\nIchigo: Mixed-Modal Early-Fusion Realtime Voice Assistant\nAlan Dao (Gia Tuan Dao)*, Dinh Bach Vu*, Huy Hoang Ha* HomebrewResearch *Equal contribution\nalan@homebrew.ltd\nOctober 22, 2024\nAbstract\nLarge Language Models (LLMs) have revolutionized natural language processing, but their appli- cation to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modali- ties. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state- of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.\n1\n1\nIntroduction\nLarge Language Models (LLMs) have become powerful tools for solving general tasks, helping people in daily life through conversations [OpenAI et al., 2024, Brown, 2020, Hoffmann et al., 2022, Touvron et al., 2023, Radford et al., 2019]. While these models have transformed text-based interactions, audio remains essential for human communication, carrying information that often exceeds written text.\nMost voice assistants use a cascaded system architecture. In this approach, a user triggers an automatic speech recognition (ASR) system for transcribing the request to text. Then, A natural language understand- ing (NLU) pipeline converts this query into a structured format, used to generate a text answer through natural language generation (NLG). Finally, a text-to-speech (TTS) system vocalizes the answer to the user. This process, with its multiple steps, often leads to high latency that reduce user experience.\nDespite improvements, these interfaces still fall short of natural conversations. First, The multiple steps in these systems add up to several seconds of delay. This contrasts with natural conversations, where responses typically come within milliseconds. Second, complexity deployment in edge device (model compatible with conventional method and not cascaded system).\nRecent models that handle multiple types of data have become popular, but they still process different data types separately. This can limit how well they combine information from different sources and create documents that mix speech and text.\nIn this paper, we present Ichigo, a mixed-modal model capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and speech content. This approach allows for complete modeling of documents with multiple data types, expanding on tasks like understanding speech and text-only language models.\nWhile similar ideas have been tested with images, they haven’t been fully explored with audio [Team, 2024a]. Previous research has trained entire models from scratch, including the LLM. Although this method is optimal, it is expensive and challenging for many research labs to adapt and build upon. Our approach, in contrast, utilizes current strong open-source LLMs and extends their capability to speech through continual pre-training. This solution not only achieves the goal of introducing a new modality to the model but also offers greater flexibility for adaptation with other LLMs family in the field.\nIchigo is designed to be mixed-modal from the start, employing a uniform architecture in an end-to-end fashion on an interleaved mixture of modalities: speech and text. By quantizing speech into discrete tokens, allowing us to use a decoder-only transformer architecture for both speech and text tokens, without adding a speech encoder and a speech adaptor [Fang et al., 2024, Chu et al., 2024, Tang et al., 2023, Ramesh et al., 2022]. This approach projected different data types into a shared representational space from the start, allows for smooth reasoning and generation across modalities. It represents a significant advancement over traditional cascaded systems and even recent multimodal models that treat modalities separately.\nWe summarize our contributions as follows:\n1. We present Ichigo, an tokenized early-fusion multimodal model capable of reasoning over and generating interleaved speech-text documents.\n2. We introduce training techniques for tokenized early-fusion multimodal models without starting from scratch, making our approach more accessible and adaptable.\n3. We present a recovering capability training method and techniques to stabilize cross-modality training, enhancing the robustness of our model.\n4. We construct and release Instruction Speech [HomebrewResearch, 2024], a large-scale English speech- text cross-modal instruction-following dataset featuring multi-turn interactions, reasoning tasks, and refusal scenarios. We also provide the training and inference code to facilitate further research in this area.\n2\n2 Model Architecture\n2.1 Tokenized Early Fusion\nThis methodology presents a unified framework leveraging token-based representations for both speech and textual modalities (Figure 1). By quantizing continuous speech into discrete tokens, similar to words in text, we can utilize the same transformer architecture to sequences of both speech and text tokens. This eliminates the need for separate speech/text encoders or domain-specific decoders. By projecting all modalities into a shared representational space from the outset, this method facilitates cross-modal reasoning and generation.\n2.2 Tokenization Process\nFor speech tokenization, we employ WhisperVQ, a component of WhisperSpeech [Collabora, 2024]. This model utilizes a codebook of 512 tokens with a codebook dimension of 64. Based on the Whisper Medium model, WhisperVQ processes speech input resampled to 16 kHz, achieving a frame rate of 25 Hz.\nInitially, the audio is converted to a log-mel spectrogram and processed by a Whisper encoder [Radford et al., 2022], producing continuous embeddings. These embeddings undergo downsampling and refinement before a vector quantization step maps them to a finite codebook, producing a sequence of discrete tokens representing the audio content.\nFigure 1. Ichigo represents speech and text modalities as discrete tokens and uses a uniform transformer- based architecture. It uses WhisperVQ to quantize speech into discrete tokens in the same manner with original text modality.\n2.3 Expanding the Language Model\nTo incorporate multimodal discrete representations into pre-trained LLMs, we expand the vocabulary with new modality-specific tokens. This expansion necessitates extending the corresponding embeddings and prediction layer, with newly incorporated parameters initialized randomly. The combined tokens from all modalities form a new vocabulary, where each modality is trained within the language model to align in a shared representational space.\nThis approach allows us to compress multimodal data into discrete token sequences, which the language model can then train using next token prediction loss. Consequently, this enables the LLM to unify tasks such as understanding, reasoning, and generation in an autoregressive manner.\n3\n2.4 Model Implementation Details\nWe use Llama-3.1-8B-Instruct as our backbone model, which has been pre-trained on 15 trillion text tokens and performs well across benchmarks [Dubey et al., 2024]. Apart from reshaping the embedding matrix to accommodate the new tokens, the rest of the language model remains unaltered. The tokens generated by WhisperVQ are converted to the format &lt;|sound_dddd|&gt;, where ’dddd’ represents the position of the cor- responding code. Additionally, we introduce two new special tokens, &lt;|sound_start|&gt; and &lt;|sound_end|&gt;, to delimit audio file inputs.\nInitially, we attempted to use the default new token initialization from the HuggingFace codebase. However, this approach resulted in slow convergence of the loss curve. To address this issue, we switched to initial- izing new token embeddings by averaging all embeddings of the current vocabulary [Hewitt, 2021]. This modification significantly improved the speed of convergence and enhanced training stability.\n4\n3 Datasets\nTo enable Ichigo to process and understand audio signals, we have curated a comprehensive dataset com- prising two main components: the Pre-training Dataset and the Instruction Speech Dataset. The former facilitates the LLM’s understanding of audio signals, while the latter enables cross-modal instruction tuning. This section provides a detailed overview of our data collection and processing methodologies.\n3.1 Pre-training Dataset\nTo align the embeddings of text and audio, we assembled a diverse collection of public Automatic Speech Recognition (ASR) datasets spanning eight languages: English, German, Dutch, Spanish, French, Italian, Portuguese, and Polish. We obtained English from the MLS English 10k dataset [Pratap et al., 2020] and other languages from the Multilingual LibriSpeech dataset [Pratap et al., 2020].\nThe training dataset encompasses approximately 10,000 hours of English audio and an additional 6,000 hours distributed across the other languages. The majority of this audio content originates from audiobooks available on LibriVox and OpenSLR [Panayotov et al., 2015]. Subsequently, we employed WhisperVQ to convert these audio files into discrete sound tokens.\n3.2 Post-training Dataset\n3.2.1 Text Instruction Data\nOur training dataset comprises a blend of high-quality, open-source data available on HuggingFace [Xu et al., 2024, HuggingFaceTB, 2024, PJMixers, 2024, euclaise, 2024, Intel, 2024, routellm, 2024, nomic ai, 2024, Microsoft, 2024, for AI, 2024, Open-Orca, 2024, Magpie-Align, 2024, qiaojin, 2024, Undi95, 2024, HannahRoseKirk, 2024, BAAI, 2024]. These datasets span a wide array of topics, thereby diversifying the input data for our model. We implemented a two-step filtering process to ensure data quality and relevance. The main steps are Language Identification and Deduplication.\nLanguage Identification: We applied the FastText model [Bojanowski et al., 2017] as a language identifier at the document level, retaining only English documents with a confidence threshold of (0.9). This decision aligns the model’s distribution more closely with the original multilingual training of the base LLM.\nDeduplication: We removed duplicate entries to prevent overfitting and ensure a diverse training set. Despite the tokenizer’s capacity to handle eight languages, we opted to focus primarily on English for this training iteration. This decision was motivated by the relative scarcity of high-quality instruction data in other languages and the low-resource nature of these languages in our dataset.\n3.2.2 Speech-Text Instruction Data\nBuilding upon the Text Instruction Dataset, we conducted further filtering to create a dataset more suitable for Instruction Speech Dataset (Figure 2).\nLength Filtering: To prevent exceeding the LLM’s context length, we filtered out text instructions longer than 64 tokens. This threshold was established based on empirical observations of typical user interactions with audio assistants.\nQuality Filtering: We eliminated samples that would be challenging to pronounce as speech, such as URLs, mathematical symbols, and code snippets.\nSynthetic Data Generation Pipeline: We implemented a two-stage process to convert our text-based Instruction dataset into discrete sound tokens suitable for audio input. We utilized the WhisperSpeech text-to-speech (TTS) model to generate audio files from the instruction dataset’s questions. Subsequently,\n5\nwe employed the WhisperVQ model to transform these audio files into discrete sound tokens. Figure 2 illustrates the overview of the synthetic data generation pipeline.\nFigure 2. Data Processing Pipeline for Speech Instruction Dataset Generation. This chart illustrates the multi-stage filtering and conversion process, starting from 6M samples of multiple open-source instruction text datasets. The data undergoes filtering process results in 2.2M samples. Finally, these samples are converted to speech instruction data using WhisperSpeech (TTS) and WhisperVQ (speech to semantic tokens), creating the 1.3M pairs of Speech instruction and Text answer.\nThis process was applied only to the input questions, while the corresponding answers were maintained in their original text format. The resulting dataset comprised 2000 hours of tokenized speech audio data paired with text responses.\nThis approach allowed us to create a rich, multimodal dataset that closely mimics real-world interactions with audio-based AI assistants, enhancing our model’s ability to process and respond to spoken instructions.\n3.2.3 Transcribe Data\nFor transcription tasks, we created a specialized transcribe instruction dataset derived from our ASR dataset. We introduced a signal to help the model identify transcription tasks. Initially, we experimented with a special token &lt;|transcribe|&gt;, but this approach led to catastrophic forgetting in the model (Table 6).\nTo address this issue, we transitioned to using pure instructions. We incorporated six instruction sentences for transcription tasks, which improved the model’s ability to map sound token patterns to corresponding text while minimizing the reduction in the model’s text capabilities. Examples of these instructions are provided in Table 5.\n3.2.4 Noise Audio Data\nDuring model training, we recognized the need for noise data to prevent the model from being overly sensitive to inaudible inputs. Our initial approach of creating a synthetic dataset of random environmental noises\n6\nproved challenging to scale.\nWe hypothesized that meaningful speech follows certain patterns and utilized this insight to generate in- audible input data. Using the 512 sound tokens from the WhisperVQ codebook, we randomized them into patterned sequences. This method allowed us to generate a vast amount of inaudible input data with a wide distribution. We then employed the Qwen2.5-72B model [Team, 2024b] to generate diverse synthetic answers for those inaudible inputs.\nWith an average speech input of about 50 sound tokens, there are 51350 possible arrangements, of which only a tiny fraction would constitute meaningful speech. By exposing our model to a wide range of these chaotic arrangements, we taught it to distinguish between audible and inaudible inputs effectively.\nWe also performed sequence length distribution matching between inaudible and audible data to ensure a balanced representation of both types of inputs in our training set. This approach involved sampling inaudible data samples to match the token count distribution of the original data, contributing to a more robust and generalizable model.\n7\n4 Training\nThe training process for our model was conducted in multiple stages, each designed to optimize differ- ent aspects of performance and functionality. This section details the software infrastructure, pre-training methodology, and post-training refinements employed in our research.\n4.1 Pre-training Methodology\nOur pre-training approach was rooted in the fundamental principle of language model training: converting text into processable tokens and enabling the model to learn patterns and relationships within the data. In this phase, we aimed to introduce speech representation into new tokens, facilitating the model’s development of basic concepts regarding these additional tokens.\nWe utilized AdamW Fused optimize [Loshchilov and Hutter, 2019, Paszke et al., 2019] with a weight decay of 0.01, momentum decay of 0.9, and squared gradient decay of 0.95. Although we experimented with alternative optimizers such as Adam-mini and Lion during hyperparameters tuning, these attempts resulted in unstable training and frequent loss explosions, prompting our return to AdamW Fused.\nAll models were trained on our internal cluster comprising 10 NVIDIA A6000-48GB GPUs, employing FSDP 2 [torchtune maintainers and contributors, 2024] and activation checkpointing. The training consisted of 8,064 steps with a batch size of 480 and a context length of 512. We implemented a Cosine learning rate schedule [torchtune maintainers and contributors, 2024] initiating at 2e-4 with warmup over 50 steps.\nTable 1 provides an overview of the hyper-parameters and configurations used in the three-stage training phases. In the Pre-training stage, we maximize the global batch size to ensure more general learning of the model. During Instruction Fine-tuning and Enhancement Fine-tuning, we reduce the learning rate to stabilize the training loss curve. Additionally, we increase the context length to 4096 tokens in these later stages, providing the model with more space to respond to user requests.\nTable 1. Training Hyper-parameters for Ichigo’s three-stage process.\nParameter\nPre-training\nInstruction FT Enhancement FT\nWeight Decay Learning Scheduler Optimizer Precision\n0.005 Cosine AdamW Fused bf16\nHardware Train time Steps Global batch size Learning Rate Warmup Steps Max length\n10x A6000 45h 8064 480 2 × 10−4 50 512\n8x H100 10h 7400 256 7 × 10−5 73 4096\n8x H100 3h 644 256 1.5 × 10−5 8 4096\n4.2 Post-training Refinements\nThe post-training phase was divided into two distinct stages: instruction fine-tuning and enhancement fine- tuning. The former focused on honing the model’s question-answering capabilities, while the latter expanded its proficiency in multi-turn conversations and appropriate responses to inaudible inputs.\n8\n4.2.1\nInstruction Fine-tuning\nBuilding upon the model from the previous stage, we concentrated on developing its question-answering abilities. Our research revealed the critical importance of balancing modalities during the Supervised Fine- Tuning (SFT) stage to maintain the model’s original performance. We observed that significant imbalances between modality pairings could lead to unconditional priors, resulting in either muted or exaggerated generation of specific modalities.\nTo address this, we carefully curated our dataset, comprising 70% speech instruction prompts, 20% speech transcription prompts, and 10% text-only prompts. This distribution was determined through extensive permutation testing to achieve an optimal balance between speech understanding, transcription capabilities, and general language skills. Figure 3 shows the data distribution proportion for this training stage.\nFigure 3. a. Distribution of data types in the Instruction Fine-tuning dataset. The goal of this specific distribution was to enhance speech comprehension while maintaining robust general language abilities. b. Distribution of data samples used in the enhancement fine-tuning stage. This specific distribution improves Ichigo robustness in handling multi-turn conversations and inaudible inputs.\n4.2.2 Enhancement Fine-tuning\nThe enhancement fine-tuning stage involved data augmentation to simulate real-world user interactions, thereby improving Ichigo’s robustness in various scenarios. We focused on two key areas: multi-turn con- versations with speech input and appropriate responses to inaudible inputs. These enhancements aimed to create more fluid dialogues and improve the model’s interactive capabilities.\nTo achieve this, we fine-tuned the model using a dataset of 158,000 samples. The dataset for enhancing refusal capabilities was carefully balanced, comprising only 0.5% of the total multi-turn data. This proportion was determined through experimentation, as we found that a higher percentage led to an increased tendency for the model to refuse inputs. Figure 3 illustrates the data distribution ratios.\n9\n5 Results\nIn this section, we present the experimental outcomes for Ichigo. We evaluate its performance across multiple dimensions, including question-answering capabilities, response latency, degradation recovery, and practical cases. Our analysis provides a comprehensive assessment of Ichigo’s capabilities in comparison to other well-known speech language models.\n5.1 SpeechBench Evaluation\nWe first assess Ichigo’s speech question-answering (SQA) ability in comparison to other well-known speech language models. Table 2 presents the results on two SQA scores from AudioBench [Wang et al., 2024], using the robust LLaMA-3 70B model [Dubey et al., 2024] as the judge for evaluation.\nTable 2. A comparative results of Ichigo against three representative Speech Language Models and a cascade system.\nModel\nOpenHermes-Audio ALPACA-Audio\nWhisper + Llama-3 8B\n63.0\n70.8\nSALMONN Qwen2-Audio WavLM Ichigo instruct v0.3 (Phase 3)\n19.2 44.8 22.4 67.8\n12.4 52.0 21.6 67.2\nNote: Higher scores indicate better performance.\nIt is important to note that during our evaluation, we encountered an error in the judge model’s output affecting the ’Rating score’. The model provided ratings in the middle of its responses rather than at the end as expected, resulting in lowered scores. To address this issue, we implemented a backfilling procedure for missing ratings, ensuring a more accurate representation of model performance.\nOur results demonstrate that Ichigo outperforms existing open-source speech language models, particularly those utilizing Non-Tokenized Early Fusion (NTEF) approaches [Wadekar et al., 2024]. Compared to other end-to-end models, Ichigo’s performance is particularly impressive. It outperforms Qwen2-Audio [Chu et al., 2024], the next best performer among end-to-end models, by 23 points on OpenHermes-Audio and 15.2 points on ALPACA-Audio. This substantial improvement underscores the effectiveness of Ichigo’s architecture and training approach in capturing the nuances of speech-language interactions.\nOn the OpenHermes-Audio benchmark, Ichigo achieves a score of 67.8, surpassing even the cascaded system (63.0). This performance is especially noteworthy given that cascaded systems often benefit from specialized components for transcription and language modeling.\nFor the ALPACA-Audio benchmark, Ichigo maintains its strong performance with a score of 67.2. While this is lower than the cascaded system (70.8), it’s important to note that Ichigo achieves this as an end-to- end model, without the need for separate transcription and language modeling phases. This demonstrates Ichigo’s ability to effectively integrate speech understanding and language generation in a single model.\n5.2 Latency to first token\nTo validate the efficiency of Ichigo’s Tokenized Early Fusion architecture, we conducted a comparative analysis of its latency to first token against current speech models and cascaded systems. Our benchmarking was performed on a single NVIDIA A6000-48GB GPU, performing 10 iterations of the latency test. The test set comprised 10 diverse audio files with durations ranging from 1 to 5 seconds (average length: 5.4 ±\n10\n2.79 seconds), reflecting real-world usage scenarios. This setup ensures a comprehensive evaluation across various audio lengths.\nTable 3. The comparative results of latency to first token and VRAM usage across different models and systems\nModel\nLatency (avg.) VRAM usage\n(ms)\n(GB)\n317.45 ± 8.30 Qwen2-Audio Cascaded system 453.18 ± 15.02 111.52 ± 7.73 Ichigo\n32 19 19\nEfficiency of Direct Generation: Our pipeline, which generates responses directly from the model, significantly reduces the latency to first response compared to the cascaded system. Ichigo achieves an average latency of 111.52 ± 7.73 ms, which is approximately 4 times faster than the cascaded Whisper + Llama-3 8B system (453.18 ± 15.02 ms).\nComparison with Other Speech Language Models: Benefiting from LLM-specific inference engines, Ichigo outperforms other speech language models in terms of latency. Ichigo achieves a 110 ms latency to first response, which is nearly 3 times faster than Qwen2-Audio (317.45 ± 8.30 ms).\nVRAM Efficiency: Ichigo maintains a lower VRAM footprint (19 GB) compared to Qwen2-Audio (32 GB) and the cascaded system (19 GB). This demonstrates Ichigo’s exceptional efficiency in balancing high performance with resource utilization.\n5.3 Degradation recovery\nIn the process of training multi-modal models, a critical concern is not only how well the model learns new modalities but also how effectively it retains the capabilities of its original language model. To assess this, we evaluated Ichigo on three popular LLM benchmarks spanning a wide range of topics including General Knowledge, Reasoning, and Mathematics, using the LM Evaluation Harness [Gao et al., 2024].\nTable 4 presents the comparative results of Ichigo across different versions and the original Llama3 8B Instruct model. The metrics used are MMLU (5-shot) [Hendrycks et al., 2020], GPQA (0-shot) [Rein et al., 2023], and GSM-8K (Chain-of-Thought, 8-shot) [Cobbe et al., 2021], which provide a comprehensive evaluation of the model’s capabilities across various domains.\nOur findings reveal a significant improvement in performance retention from earlier versions to the latest Ichigo instruct v0.3 (phase 3). Notably, the final training phase of Ichigo achieved a reduction in performance degradation from 29.3% (in v0.2) to only 8.4% (in v0.3 phase 3) compared to the original Llama3 8B Instruct model. This substantial recovery is primarily attributed to our refined training strategy, which incorporates a mixed proportion of text-only and sound token data.\nPerformance Recovery: Ichigo instruct v0.3 (phase 3) demonstrates remarkable recovery across all bench- marks. For instance, on the MMLU benchmark, it achieves a score of 63.79, significantly closer to the original Llama3 8B Instruct’s 69.4, compared to the 50.27 scored by v0.2.\nConsistent Improvement: We observe a consistent upward trend in performance from v0.2 to v0.3 (phase 3), indicating the effectiveness of our iterative training approach. This indicates that with extended training time and more computational resources, we can achieve higher performance.\nPre-training Challenges: It’s worth noting that during the initial pre-training phase, which focused solely on sound tokens with next token prediction, we observed a significant degradation in the model’s performance on text-based tasks, particularly in mathematics and coding. This highlights the challenges in maintaining cross-modal capabilities during specialized training.\n11\nTable 4. Results of Ichigo across different versions and the original Llama3 8B Instruct model.\nModel\nMMLU GPQA (0-shot) (5-shots)\nGSM-8K (CoT) (8-shots)\nAvg.\nLlama3 8B Instruct\n69.4\n30.4\n84.5\n61.43\nIchigo base v0.2 Ichigo instruct v0.2 Ichigo base v0.3 Ichigo instruct v0.3 (phase 2) Ichigo instruct v0.3 (phase 3)\n47.66 50.27 42.11 63.08\n63.79\n28.13 26.56 28.57 28.35\n29.69\nN/A* 53.58 N/A* 76.50\n75.28\nN/A* 43.47 N/A* 55.98\n56.25\nN/A: Not applicable due to significant performance degradation in mathematical and coding tasks during pre-training on sound tokens with next token prediction.\n5.4 Instruction following cross modality\nIn addition to the quantitative results presented earlier, we conducted practical experiments with Ichigo in real-world conversational scenarios. These experiments aimed to assess the model’s ability to follow system prompts and maintain coherent multi-turn dialogues across different modalities (text and speech).\nFigure 4. The system prompt used for Ichigo during inference.\nCross-Modal Instruction Following: Ichigo demonstrated a robust ability to follow text-based system prompts while engaging in speech-based conversations with users. This highlights the model’s capacity to generalize instructions across modalities, a crucial feature for versatile AI assistants.\nAs shown in Figure 5, the model consistently maintained its prescribed identity as "Ichigo" when questioned, adhering to the system prompt instructions. This behavior persisted regardless of whether the input was in text or speech format, demonstrating the model’s ability to maintain context across different input modalities.\nMulti-Turn Dialogue Coherence: Ichigo exhibited proficiency in managing multi-turn conversations, seamlessly understanding and responding to both speech and text inputs without apparent difficulties. Figure 6 presents transcribed dialogue examples that showcase the model’s zero-shot multi-turn capabilities.\nHandling Unclear Inputs: In scenarios where user input was inaudible, unclear, or affected by back- ground noise, Ichigo demonstrated appropriate behavior by refusing to provide random answers. Instead, as illustrated in Figure 6, the model politely requested the user to repeat their query, ensuring accurate and\n12\nFigure 5. The model follows text-based system prompts during speech-based conversations with users.\nrelevant responses.\nFigure 6. a. Transcribed dialogue examples using Ichigo. The user-turn is audio input. The examples illustrate zero-shot multi-turn capabilities. b. Ichigo requests clarification from the user when unable to understand the question clearly.\nThese experiments complement our quantitative findings, demonstrating Ichigo’s practical capabilities in real- world scenarios. Ichigo’s abilities in cross-modal instruction following, multi-turn dialogues, and handling unclear inputs make it a promising candidate for advanced, user-friendly voice AI applications.\n13\n6 Related works\n6.1 Early Audio Language Models\nThe success of language models in natural language processing [Radford et al., 2019, Raffel et al., 2020, OpenAI et al., 2024] has inspired researchers to explore similar approaches for modeling speech and audio. Initial efforts in audio language modeling focused on training models using semantic or acoustic tokens derived from audio data, enabling audio generation without the need for text input [Borsos et al., 2023, Nguyen et al., 2023, Lakhotia et al., 2021]. Subsequent advancements led to the joint training of speech tokens and text, resulting in decoder-only models such as VALL-E [Wang et al., 2023a, Chen et al., 2024] and VioLA [Wang et al., 2023b]. These models demonstrated capabilities in speech recognition, translation, and synthesis. However, these early models were not built upon Large Language Models (LLMs). To harness the power of LLMs, researchers have explored various approaches to building speech-language models based on LLM architectures.\n6.2 LLM-Based Audio-Language Models\nRecent research has focused on two primary approaches to integrating speech and audio capabilities with LLMs: non-tokenized early fusion and tokenized early fusion.\n6.2.1 Non-Tokenized Early Fusion\nThe most common approach to enable cross-modal perception in LLMs is to connect pre-trained encoders of other modalities as adaptors. This method involves adding a speech encoder before the LLM and fine- tuning the entire model for speech understanding capabilities. These models excel in tasks such as speech recognition, speech translation, and general speech-to-text tasks [Chu et al., 2024, Tang et al., 2023, Shu et al., 2023, Deshmukh et al., 2023, Hu et al., 2024, Das et al., 2024, Fang et al., 2024]. Notable examples of this approach are Llama-omni [Fang et al., 2024] and LLaSM [Shu et al., 2023], which extend LLM capabilities to audio modality by integrating a pre-trained speech encoder, a speech adaptor, and a streaming speech decoder. SALMONN [Tang et al., 2023] takes a step further in capturing both speech and non-speech audio information using dual auditory encoders. Qwen2 Audio [Chu et al., 2024] introduce the new architecture to combine an audio encoder with a large language model, training to maximize next text token probability conditioned on audio representations.\nThis NTEF tends to be more cost-effective, as it involves multiple training phases where most components are frozen, and it can be effective even when training with Parameter-Efficient Fine-Tuning techniques [Hu et al., 2021].\n6.2.2 Tokenized Early Fusion\nThis approach involves tokenizing multimodal inputs using either a common tokenizer or modality-specific tokenizers. The tokenized inputs are then processed by a pre-trained LLM or an encoder-decoder trans- former model, enabling multimodal output generation [Wadekar et al., 2024]. Examples of this approach include Chameleon [Team, 2024a], which represents images and text as a series of discrete tokens within a unified transformer, trained from scratch with modified transformer architecture. AudioPALM [Rubenstein et al., 2023] and VoxTLM [Maiti et al., 2024], which utilize pre-trained language models and extend their vocabularies with discrete semantic audio tokens, focus on translation speech to speech tasks. AnyGPT [Zhan et al., 2024] leverages LLMs to enable inherent cross-modal conversation capabilities through Speech- Tokenizer [Zhang et al., 2023], MusicTokenizer [Défossez et al., 2022], and ImageTokenizer [Ge et al., 2023]. Unlike previous works, our approach retains the entire architecture of current LLMs while incorporating WhisperVQ to preserve most of the OpenAI Whisper encoder block. This allows us to generate embeddings,\n14\nwhich are then quantized to obtain semantic tokens. Additionally, we address a key challenge is to stabilize the loss in cross-modality training.\nAnother new approach is Moshi [Défossez et al., 2024] is a real-time native multimodal foundation model designed for seamless audio-text interactions. It employs a 7B parameter multimodal language model that processes speech input and output concurrently, generating text tokens and audio codecs. Moshi’s innovative approach allows it to handle two audio streams simultaneously, enabling it to listen and talk in real-time while maintaining a flow of textual thoughts.\n7 Conclusion\nIn this paper, we introduced Ichigo, an early-fusion token-based speech model that sets a new approach for Multi-modal Models. By learning a unified representation space over interleaved speech and text tokens, Ichigo achieves strong performance across a wide range of speech-language benchmarks while enabling novel mixed-modal reasoning and generation capabilities.\nThe key to Ichigo’s success lies in its fully token-based architecture, which allows for seamless information integration across modalities. By quantizing speech into discrete tokens and utilizing a strong base LLM, Ichigo learns to jointly reason over speech and text in a way that surpasses late-fusion architectures or models that maintain separate encoders for each modality.\nCrucially, our meticulous approach to mixing training data has allowed us to largely preserve the original performance of the original LLM while extending its capabilities to the speech domain. As a result, Ichigo outperforms other end-to-end Speech Language Models in speech-based question-answering tasks, marking a significant step forward in multimodal AI.\nImportantly, Ichigo demonstrates a real-time speech system with a latency of 110 milliseconds to first re- sponse. This opens up new possibilities for speech systems and significantly reduces the complexity of deploying such systems in production environments.\nWe believe that this paper will empower smaller research teams - like ourselves - to contribute more con- fidently and prolifically to the open-source community. By demonstrating that significant advancements can be achieved with limited resources, we hope to inspire broader participation in this critical area of AI research.\n8 Limitations and Future work\nWhile Ichigo represents a significant step forward in multimodal language modeling, several limitations and areas for future work remain:\nToken Stability: Similar to challenges faced by models like Chameleon, we encountered fluctuating loss when training with acoustic tokens, which led us to shift towards semantic tokens to achieve stable loss. This highlights the difficulty in training with rich, acoustic information. Future work should explore methods to stabilize training with acoustic tokens, potentially unlocking even more powerful models.\nEmotional Understanding: The current architecture does not fully account for emotional compre- hension. Future iterations should focus on enhancing the model’s ability to understand and respond to user emotions, allowing for more nuanced and context-appropriate responses.\nContext Length: Multimodal content, especially audio, often spans extensive sequences. Ichigo currently limits modeling to 10 seconds of speech input and performs well for 4-5 turns of conversation. Extending the context window would allow for modeling of longer audio segments and handling of more complex, multi-turn conversations.\n15\nReferences\nBAAI. Infinity-instruct, 2024. URL https://huggingface.co/datasets/BAAI/Infinity-Instruct.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword\ninformation. Transactions of the association for computational linguistics, 5:135–146, 2017.\nZalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Do- minik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a language modeling approach to audio generation. IEEE/ACM transactions on audio, speech, and language processing, 31: 2523–2533, 2023.\nTom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nSanyuan Chen, Shujie Liu, Long Zhou, Yanqing Liu, Xu Tan, Jinyu Li, Sheng Zhao, Yao Qian, and Furu Wei. Vall-e 2: Neural codec language models are human parity zero-shot text to speech synthesizers. arXiv preprint arXiv:2406.05370, 2024.\nYunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng\nHe, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nCollabora. Whisperspeech, 2024. Accessed: 19 October 2024.\nNilaksh Das, Saket Dingliwal, Srikanth Ronanki, Rohit Paturi, David Huang, Prashant Mathur, Jie Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, et al. Speechverse: A large-scale generalizable audio language model. arXiv preprint arXiv:2405.08295, 2024.\nAlexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: a speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024.\nSoham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio language model\nfor audio tasks. Advances in Neural Information Processing Systems, 36:18090–18108, 2023.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\nAlexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression.\narXiv preprint arXiv:2210.13438, 2022.\neuclaise. gsm8k_multiturn, 2024. URL https://huggingface.co/datasets/euclaise/gsm8k_multiturn.\nQingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni: Seamless\nspeech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024.\nAllen Institute for AI. Wildchat-1m, 2024.\nURL https://huggingface.co/datasets/allenai/\nWildChat-1M.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Lau- rence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602.\nYuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see\nand draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023.\n16\nHannahRoseKirk. prism-alignment, 2024. URL https://huggingface.co/datasets/HannahRoseKirk/\nprism-alignment.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nJohn Hewitt. Initializing new word embeddings for pretrained language models, 2021. URL https:/nlp.\nstanford.edu/~johnhew//vocab-expansion.html.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nHomebrewResearch. Instruction speech. Hugging Face Dataset, 2024.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\nShujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et al. Wavllm: Towards robust and adaptive speech large language model. arXiv preprint arXiv:2404.00656, 2024.\nHuggingFaceTB. Everyday-conversations-llama3.1-2k, 2024. URL https://huggingface.co/datasets/\nHuggingFaceTB/everyday-conversations-llama3.1-2k.\nIntel. orca_dpo_pairs, 2024. URL https://huggingface.co/datasets/Intel/orca_dpo_pairs.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336–1354, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/\nabs/1711.05101.\nMagpie-Align. Magpie-pro-300k-filtered, 2024. URL https://huggingface.co/datasets/Magpie-Align/\nMagpie-Pro-300K-Filtered.\nSoumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. Voxtlm: Unified decoder-only models for consolidating speech recognition, synthesis and speech, text continuation tasks. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 13326–13330. IEEE, 2024.\nMicrosoft. orca-math-word-problems-200k, 2024. URL https://huggingface.co/datasets/microsoft/\norca-math-word-problems-200k.\nTu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken dialogue language mod- eling. Transactions of the Association for Computational Linguistics, 11:250–266, 2023.\nnomic ai.\ngpt4all-j-prompt-generations, 2024. URL https://huggingface.co/datasets/nomic-ai/\ngpt4all-j-prompt-generations.\nOpen-Orca. oo-gpt4-200k, 2024. URL https://huggingface.co/datasets/Open-Orca/oo-gpt4-200k.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine\n17\nBoyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Ben- jamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre- ston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Pytorch: An imperative style, high- Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. performance deep learning library. Information Processing Systems 32, pages 8024–8035. Curran Associates, URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.\nIn Advances\nin Neural\nInc., 2019.\nPJMixers. Math-multiturn-10k-sharegpt, 2024. URL https://huggingface.co/datasets/PJMixers/\nMath-Multiturn-10K-ShareGPT.\n18\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A large-scale\nmultilingual dataset for speech research. ArXiv, abs/2012.03411, 2020.\nqiaojin. Pubmedqa, 2024. URL https://huggingface.co/datasets/qiaojin/PubMedQA.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. 2022. URL https://arxiv.org/abs/2212.04356.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&amp;a benchmark. arXiv preprint arXiv:2311.12022, 2023.\nroutellm. gpt4_dataset, 2024. URL https://huggingface.co/datasets/routellm/gpt4_dataset.\nPaul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Audiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023.\nYu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin\nShi. Llasm: Large language and speech model. arXiv preprint arXiv:2308.15930, 2023.\nChangli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023.\nChameleon Team.\nChameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024a.\nQwen Team. Qwen2.5: A party of foundation models, September 2024b. URL https://qwenlm.github.\nio/blog/qwen2.5/.\ntorchtune maintainers and contributors. torchtune: Pytorch’s finetuning library, 2024. URL https//github.\ncom/pytorch/torchtune.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nUndi95.\nCapybara-sharegpt,\n2024.\nURL\nhttps://huggingface.co/datasets/Undi95/\nCapybara-ShareGPT.\nShakti N Wadekar, Abhishek Chaurasia, Aman Chadha, and Eugenio Culurciello. The evolution of multi-\nmodal model architectures. arXiv preprint arXiv:2405.17927, 2024.\nBin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy F Chen. Audiobench: A universal benchmark for audio large language models. arXiv preprint arXiv:2406.16020, 2024.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023a.\n19\nTianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu Wei. Viola: Unified codec language models for speech recognition, synthesis, and translation. arXiv preprint arXiv:2305.16107, 2023b.\nZhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024.\nJun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024.\nXin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer\nfor speech large language models. arXiv preprint arXiv:2308.16692, 2023.\n20\nA Additional Data and Analysis\nThis appendix provides supplementary information on the Audio Speech Recognition (ASR) prompt library and ablation studies conducted during our research.\nA.1 ASR Prompt Library\nTable 5 presents a collection of prompts used for the Ichigo Model transcription tasks. These prompts were designed to elicit accurate speech-to-text conversions across various contexts.\nTable 5. Audio Speech Recognition (ASR) Prompt Library for Ichigo Model Transcription Tasks\nTranscribe Prompts\nTranscribe the following audio clip: &lt;speech&gt; Convert the spoken words to text: &lt;speech&gt; What is being said in this audio clip: &lt;speech&gt; Transcribe the speech in this audio sample: &lt;speech&gt; Please write down what is being said in the audio clip: &lt;speech&gt; Generate a transcript from this sound file: &lt;speech&gt; Recognize the speech in this audio clip: &lt;speech&gt; Produce a text version of this audio recording: &lt;speech&gt;\nA.2 Ablation Studies\nWe conducted a series of ablation studies to investigate the impact of different training configurations on the model’s performance. Table 6 summarizes the results of these experiments.\nTable 6. Ablations on training model with/without introducing new transcribe token\nTest Name\nTranscribe token SpeechQA Instruction Transcription MMLU\nRecovery test 1 Recovery test 2 Recovery test 3\n1 1 0\n1 1 1\n1 1 1\n0 1 1\n0.515 0.480 0.630\nThe results from our ablation studies provide interesting insights into the role of transcription tokens and data in model performance. Notably, Test 3, which used transcription prompts without a specific transcription token, achieved the highest MMLU score of 0.63. This suggests that the inclusion of diverse transcription prompts in the training data may be more beneficial than using a dedicated transcription token.\nInterestingly, Test 1, which excluded transcription data entirely, outperformed Test 2, which included both transcription tokens and data. This unexpected result warrants further investigation and may indicate potential interactions between different types of training data that affect model performance.\nThese findings highlight the complex relationships between training data composition, token usage, and model performance. Future work could explore these relationships in more detail, potentially leading to improved strategies for training multi-modal language models.\n21',
 './pdfs/omniparser.pdf': '4 2 0 2\ng u A 1\n]\nV C . s c [\n1 v 3 0 2 0 0 . 8 0 4 2 : v i X r a\nOmniParser for Pure Vision Based GUI Agent\nYadong Lu1, Jianwei Yang1, Yelong Shen2, Ahmed Awadallah1\n1Microsoft Research\n2 Microsoft Gen AI\n{yadonglu, jianwei.yang, yeshe, ahmed.awadallah}@microsoft.com\nAbstract\nThe recent success of large vision language models shows great potential in driv- ing the agent system operating on user interfaces. However, we argue that the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, we introduce OMNIPARSER, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. We first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. OMNIPARSER significantly improves GPT-4V’s performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, OMNIPARSER with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot.\n1\nIntroduction\nLarge language models have shown great success in their understanding and reasoning capabilities. More recent works have explored the use of large vision-language models (VLMs) as agents to perform complex tasks on the user interface (UI) with the aim of completing tedious tasks to replace human efforts [YZL+23, YYZ+23, DGZ+23, ZGK+24, HWL+23, YZS+24, WXJ+24, GFH+24, CSC+24]. Despite the promising results, there remains a significant gap between current state-of-the- arts and creating widely usable agents that can work across multiple platforms, e.g. Windows/MacOS, IOS/Android and multiple applications (Web broswer Office365, PhotoShop, Adobe), with most previous work focusing on limiting applications or platforms. While large multimodal models like GPT-4V and other models trained on UI data [HWL+23, YZS+24, CSC+24] have demonstrated abilities to understand basic elements of the UI screenshot, action grounding remains one of the key challenges in converting the actions predicted by LLMs to the actual actions on screen in terms of keyboard/mouse movement or API call [ZGK+24]. It has been noted that GPT-4V is unable to produce the exact x-y coordinate of the button location, Set-of-Mark prompting [YZL+23] proposes to overlay a group of bounding boxes each with unique numeric IDs on to the original image, as a visual prompt sent to the GPT-4V model. By applying set-of-marks prompting, GPT-4V is able to ground the action into a specific bounding box which has ground truth location instead of a specific xy coordinate value, which greatly improves the robustness of the action grounding [ZGK+24]. However, the current solutions using SoM relies on parsed HTML information to extract bounding boxes for actionable elements such as buttons, which limits\nits usage to web browsing tasks. We aim to build a general approach that works on a variety of platforms and applications.\nIn this work, we argue that previous pure vision-based screen parsing techniques are not satisfactory, which lead to significant underestimation of GPT-4V model’s understanding capabilities. And a reliable vision-based screen parsing method that works well on general user interface is a key to improve the robustness of the agentic workflow on various operating systems and applications. We present OMNIPARSER, a general screen parsing tool to extract information from UI screenshot into structured bounding box and labels which enhances GPT-4V’s performance in action prediction in a variety of user tasks.\nWe list our contributions as follows:\nWe curate a interactable region detection dataset using bounding boxes extracted from DOM tree of popular webpages.\nWe propose OmniParser, a pure vision-based user interface screen parsing method that combines multiple finetuned models for better screen understanding and easier grounded action generation.\nWe evaluate our approach on ScreenSpot, Mind2Web and AITW benchmark, and demon- strated a significant improvement from the original GPT-4V baseline without requiring additional input other than screenshot.\n2 Related Works\n2.1 UI Screen Understanding\nThere has been a line of modeling works focusing on detailed understanding of UI screens, such as Screen2Words [WLZ+21], UI-BERT [BZX+21], WidgetCaptioning [LLH+20], Action- BERT [HSZ+21]. These works demonstrated effective usage of multimodal models for extracting semantics of user screen. But these models rely on additional information such as view hierarchy, or are trained for visual question answering tasks or screen summary tasks.\nThere are also a couple publicly available dataset that on UI screen understanding. Most notably the Rico dataset [DHF+17], which contains more than 66k unique UI screens and its view hierarchies. Later [SWL+22] auguments Rico by providing 500k human annotations on the original 66k RICO screens identifying various icons based on their shapes and semantics, and associations between selected general UI elements (like icons, form fields, radio buttons, text inputs) and their text labels. Same on mobile platform, PixelHelp [LHZ+20] provides a dataset that contains UI elements of screen spanning across 88 common tasks. In the same paper they also released RicoSCA which is a cleaned version of Rico. For the web and general OS domain, there are several works such Mind2Web [DGZ+23], MiniWob++[LGP+18], Visual-WebArena [KLJ+24, ZXZ+24], and OS- World [XZC+24] that provide simulated environment, but does not provide dataset explicitly for general screen understanding tasks such as interactable icon detection on real world websites.\nTo address the absence of a large-scale, general web UI understanding dataset, and to keep pace with the rapid evolution of UI design, we curated an icon detection dataset using the DOM information from popular URLs avaialbe on the Web. This dataset features the up-to-date design of icons and buttons, with their bounding boxes retrieved from the DOM tree, providing ground truth locations.\n2.2 Autonomous GUI Agent\nRecently there has been a lot of works on designing autonomous GUI agent to perform tasks in place of human users. One line of work is to train an end-to-end model to directly predict the next action, representative works include: Pixel2Act [SJC+23], WebGUM[FLN+24] in web domain, Ferret [YZS+24], CogAgent [HWL+23], and Fuyu [BEH+23] in Mobile domain. Another line of works involve leveraging existing multimodal models such as GPT-4V to perform user tasks. Representative works include MindAct agent [DGZ+23], SeeAct agent [ZGK+24] in web domain and agents in [YYZ+23, WXY+24, RLR+23] for mobile domain. These work often leverages the DOM information in web browser, or the view hierarchies in mobile apps to get the ground truth position of interactable elements of the screen, and use Set-Of-Marks[YZL+23] to overlay the\n2\nbounding boxes on top of the screenshot then feed into the vision-language models. However, ground truth information of interactable elements may not always be available when the goal is to build a general agent for cross-platforms and cross-applications tasks. Therefore, we focus on providing a systematic approach for getting structured elements from general user screens.\n3 Methods\nA complex task can usually be broken down into several steps of actions. Each step requires the model’s (e.g. GPT-4V) ability to: 1) understand the UI screen in the current step, i.e. analyzing what is the screen content overall, what are the functions of detected icons that are labeled with numeric ID, and 2) predict what is the next action on the current screen that is likely to help complete the whole task. Instead of trying to accomplish the two goals in one call, we found it beneficial to extract some of the information such as semantics in the screen parsing stage, to alleviate the burden of GPT-4V so that it can leverages more information from the parsed screen and focus more on the action prediction.\nHence we propose OMNIPARSER, which integrates the outputs from a finetuned interactable icon detection model, a finetuned icon description model, and an OCR module. This combination produces a structured, DOM-like representation of the UI and a screenshot overlaid with bounding boxes for potential interactable elements. We discuss each component of the OMNIPARSER in more details for the rest of the section.\n3.1\nInteractable Region Detection\nIdentifying interactable regions from the UI screen is a crucial step to reason about what actions should be performed given a user tasks. Instead of directly prompting GPT-4V to predict the xy coordinate value of the screen that it should operate on, we follow previous works to use the Set-of-Marks approach [YZL+23] to overlay bounding boxes of interactable icons on top of UI screenshot, and ask GPT-4V to generate the bounding box ID to perform action on. However, different from [ZGK+24, KLJ+24] which uses the ground truth button location retrieved from DOM tree in web browswer, and [YYZ+23] which uses labeled bounding boxes in the AITW dataset [RLR+23], we finetune a detection model to extract interactable icons/buttons.\nSpecifically, we curate a dataset of interactable icon detection dataset, containing 67k unique screen- shot images, each labeled with bounding boxes of interactable icons derived from DOM tree. We first took a 100k uniform sample of popular publicly availabe urls on the web [OXL+22], and collect bounding boxes of interactable regions of the webpage from the DOM tree of each urls. Some examples of the webpage and the interactable regions are shown in 2.\nApart from interactable region detection, we also have a OCR module to extract bounding boxes of texts. Then we merge the bounding boxes from OCR detection module and icon detection module while removing the boxes that have high overlap (we use over 90% as a threshold). For every bounding box, we label it with a unique ID next to it using a simple algorithm to minimizing the overlap between numeric labels and other bounding boxes.\n3.2\nIncorporating Local Semantics of Functionality\nWe found in a lot of cases where only inputting the UI screenshot overlayed with bounding boxes and associated IDs can be misleading to GPT-4V. We argue the limitation stems from GPT-4V’s constrained ability to simultaneously perform the composite tasks of identifying each icon’s semantic information and predicting the next action on a specific icon box. This has also been observed by several other works [YYZ+23, ZGK+24]. To address this issue, we incorporate the local semantics of functionality into the prompt, i.e. for each icons detected by the interactable region detection model, we use a finetuned model to generate description of functionality to the icons, and for each text boxes, we use the detected texts and its label.\nWe perform more detailed analysis for this topic in section 4.1. To the best of our knowledge, there is no public model that is specifically trained for up-to-date UI icon description, and is suitable for our purpose to provide fast and accurate local semantics for the UI screenshot. Therefore we curate a dataset of 7k icon-description pairs using GPT-4o, and finetune a BLIP-v2 model [LLSH23] on this\n3\nFigure 1: Examples of parsed screenshot image and local semantics by OMNIPARSER. The inputs to OmniParse are user task and UI screenshot, from which it will produce: 1) parsed screenshot image with bounding boxes and numeric IDs overlayed, and 2) local semantics contains both text extracted and icon description.\ndataset. Details of dataset and training can be found in Appendix 7.1. After finetuning, we found the model is much more reliable in its description to common app icons. Examples can be seen in figure 4. And in figure 3, we show it is helpful to incorporate the semantics of local bounding boxes in the form of text prompt along with the UI screenshot visual prompt.\n4 Experiments and Results\nWe conduct experiments on several benchmarks to demonstrate the effectiveness of OMNIPARSER. We start by a motivating experiments showing that current GPT-4V model with set of mark prompting [YZL+23] is prone to incorrectly assigning label ID to the referred bounding boxes. Then we evaluate on Seeclick benchmark and Mind2Web to further showcase OMNIPARSER with local semantics can improve the GPT-4V’s performance on real user tasks on different platforms and applications.\n4.1 Evaluation on SeeAssign Task\nTo test the ability of correctly predicting the label ID given the description of the bounding boxes for GPT-4v models, We handcrafted a dataset SeeAssign that contains 112 tasks consisting of samples\n4\nFigure 2: Examples from the Interactable Region Detection dataset. The bounding boxes are based on the interactable region extracted from the DOM tree of the webpage.\nfrom 3 different platforms: Mobile, Desktop and Web Browser. Each task includes a concise task description and a screenshot image. The task descriptions are manually created and we make sure each task refers to one of the detected bounding boxes, e.g. ’click on ’settings”, ’click on the minimize button’. During evaluation, GPT-4V is prompted to predict the bounding box ID associated to it. Detailed prompt are specified in Appendix. The task screenshot images are sampled from the ScreenSpot [CSC+24] benchmark, where they are labeled with set of marks using OMNIPARSER. The tasks are further divided into 3 sub-categories by difficulty: easy (less than 10 bounding boxes), medium (10-40 bounding boxes) and hard (more than 40 bounding boxes).\nFrom table 1, we see that GPT-4V often mistakenly assign the numeric ID to the table especially when there are a lot of bounding boxes over the screen. And by adding local semantics including texts within the boxes and short descriptions of the detected icons, GPT-4V’s ability of correctly assigning the icon improves from 0.705 to 0.938.\nFrom figure 3, we see that without the description of the referred icon in the task, GPT-4V often fails to link the icon required in the task and the ground truth icon ID in the SoM labeled screenshot, which leads to hallucination in the response. With fine-grain local semantics added in the text prompt, it makes it much easier for GPT-4V to find the correct icon ID for the referred icon.\nGPT-4V w.o. local semantics GPT-4V w. local semantics\nEasy Medium Hard Overall 0.705 0.913 0.938 1.00\n0.692 0.949\n0.620 0.900\nTable 1: Comparison of GPT-4V with and without local semantics\n4.2 Evaluation on ScreenSpot\nScreenSpot dataset [CSC+24] is a benchmark dataset that includes over 600 interface screenshots from mobile (iOS, Android), desktop (macOS, Windows), and web platforms. The task instructions are manually created so that each instruction corresponds to an actionable elements on the UI screen. We first evaluate the performance of OMNIPARSER using the this benchmark. In table 2, we can see across the 3 different platforms: Mobile, Desktop and Web, OMNIPARSER significantly improves the GPT-4V baseline. Noticeably, OMNIPARSER’s performance even surpasses models\n5\nFigure 3: Examples from the SeeAssign evaluation. We can see that fine-grain local semantics improves the GPT-4V’s ability to assign correct labels to the referred icon.\nthat are specifically finetuned on GUI dataset including SeeClick, CogAgent and Fuyu by a large margin. We also note that incorporating the local semantics (OMNIPARSER w. LS in the table) further improves the overall performance. This corroborates with the finds in section 4.1 that incorporating local semantics of the UI screenshot in text format, i.e. adding OCR text and descriptions of the icon bounding boxes further helps GPT-4V to accurately identify the correct element to operate on. Furthermore, our findings indicate that the interactable region detection (ID) model we finetuned improves overall accuracy by an additional 4.3% compared to using the raw Grounding DINO model. This underscores the importance of accurately detecting interactable elements for the success of UI tasks. Overall, the results demonstrate that the UI screen understanding capability of GPT-4V is significantly underestimated and can be greatly enhanced with more accurate interactable elements detection and the incorporation of functional local semantics.\nMethods\nFuyu CogAgent SeeClick MiniGPT-v2 Qwen-VL GPT-4V OmniParser (w.o. LS, w. GD) OmniParser (w. LS + GD) OmniParser (w. LS + ID)\nModel Size\n8B 18B 9.6B 7B 9.6B - - - -\nText 41.0% 67.0% 78.0% 8.4% 9.5% 22.6% 92.7% 94.8% 93.9%\nMobile\nIcon/Widget 1.3% 24.0% 52.0% 6.6% 4.8% 24.5% 49.4% 53.7% 57.0%\nText 33.0% 74.2% 72.2% 6.2% 5.7% 20.2% 64.9% 89.3% 91.3%\nDesktop\nIcon/Widget 3.6% 20.0% 30.0% 2.9% 5.0% 11.8% 26.3% 44.9% 63.6%\nText 33.9% 70.4% 55.7% 6.5% 3.5% 9.2% 77.3% 83.0% 81.3\nWeb Icon/Widget 4.4% 28.6% 32.5% 3.4% 2.4% 8.8% 39.7% 45.1% 51.0%\nTable 2: Comparison of different approaches on ScreenSpot Benchmark. LS is short for local semantics of functionality, GD is short for Grounding DINO, and ID is short for the interactable region detection model we finetune.\n4.3 Evaluation on Mind2Web\nIn order to test how OMNIPARSER is helpful to the web navigation secnario, We evaluate on [DGZ+23] benchmark. There are 3 different categories of task in the test set: Cross-Domain, Cross- Website, and Cross-Tasks. We used a cleaned version of Mind2Web tests set processed from the raw HTML dump which eliminates a small number of samples that has incorrect bounding boxes. In total we have 867, 167, 242 tasks in the test set from Cross-Domain, Cross-Website, and Cross-Tasks category respectively. During evaluation, we feed both the parsed screen results and the action history as text prompt, and SOM labeled screenshot to GPT-4V similar to the prompting strategy in [YYZ+23, ZGK+24]. Following the original paper, we perform offline evaluation focusing on the element accuracy, Operation F1 and step success rate averaged across the task.\n6\nAverage\n19.5% 47.4% 53.4% 5.7% 5.2% 16.2% 58.38% 68.7% 73.0%\nIn the first section of the table (row 1-3), We report numbers from a set of open source VL models as it appears in [ZGK+24, CSC+24]. Here CogAgent and Qwen-VL are not finetuned on the Mind2Web training set. More detailed information about model settings can be found in the Appendix7.4. In the second section of the table (row 4-9) we report numbers from Mind2web paper [DGZ+23] and SeeAct [ZGK+24] paper. In this section, all of the approaches use the HTML elements selected by a finetuned element proposal model on Mind2Web training set which produces top 50 relevant elements on the HTML page based on the user task. Additionally, GPT-4V+SOM and GPT-4V+textual choices corresponds to the SeeAct with image annotation, and textual choices grounding methods respectively. In GPT-4V+SOM, the set of mark (SOM) boxes are selected from the element proposal model, and are labeled with the ground truth location extracted from HTML. In contrast, GPT-4V+textual uses DOM information of the selected relevant elements directly in the text prompt, rather than overlaying bounding boxes on top of screenshot. The better performance of textual choice corroborates with the experiment results in 4.1.\nIn the last section (row 10-11), we report numbers from OMNIPARSER. We observe GPT-4V augumented with local semantics of icon functionality and the finetuned interactable region detection model (w. LS + ID) performs better than the model with raw grounding DINO model (w. LS + GD) in all of the categories.\nFurther, without using parsed HTML information, OMNIPARSER is able to outperform GPT-4’s performance that uses HTML in every sub-category by a large margin, suggesting the substantial benefit of the screen parsing results provided by OMNIPARSER. Additionally, OMNIPARSER outper- forms the GPT-4V+SOM by a large margin. Compared to GPT-4V+textual choices, OMNIPARSER significantly outperforms in Cross-Website and Cross-Domain category (+4.1% and +5.2%), while underperforming (-0.8%) slightly in the Cross-Task category, which indicates that OMNIPARSER provides higher quality information compared ground truth element information from DOM and top-k relevant elemnt proposal used by the GPT-4V+textual choices set-up, and make the GPT-4V easier to make a accurate action prediction. Lastly, OMNIPARSER with GPT-4V significantly outperform all the other trained models using only UI screenshot such as SeeClick and Qwen-VL.\nMethods\nInput Types\nCross-Website\nCross-Domain\nCross-Task\nStep SR CogAgent 17.6 Qwen-VL 12.0 SeeClick 25.5 MindAct (gen) 11.9 39.6 MindAct GPT-3.5-Turbo 18.6 GPT-4 36.2 GPT-4V+som 20.3 GPT-4V+textual choice 40.2 OmniParser (w. LS + GD) 38.7 OmniParser (w. LS + ID) 39.4 Table 3: Comparison of different methods across various categories on Mind2Web benchmark.\nHTML free ✓ ✓ ✓ × × × × × × ✓ ✓\nimage Ele.Acc Op.F1 42.2 18.4 83.5 13.2 80.6 21.4 44.7 13.9 42.0 65.2 48.8 19.3 51.1 35.8 - - 67.8 38.0 83.2 41.5 84.8 41.0\nStep SR Ele.Acc Op.F1 42.0 20.6 84.3 14.1 84.8 23.2 44.7 14.2 66.5 42.1 52.8 21.6 46.5 37.1 - - 69.3 42.4 80.6 44.9 85.7 45.5\nStep SR Ele.Acc Op.F1 53.0 22.4 84.3 14.1 87.0 28.3 44.7 14.2 66.5 42.1 52.8 21.6 60.6 41.6 - - 46.4 73.4 86.7 42.3 87.6 42.4\n✓ ✓ ✓ × × × × ✓ ✓ ✓ ✓\n13.4 9.2 16.4 11.0 38.9 16.2 30.1 32.7 32.4 36.1 36.5\n15.5 12.0 20.8 11.9 39.6 18.6 26.4 23.7 36.8 36.8 42.0\n4.4 Evaluation on AITW\nIn additional to evaluation on multi-step web browsing tasks, we assess OMNIPARSER on the mobile navigating benchmark AITW [RLR+23], which contains 30k instructions and 715k trajectories. We use the same train/test split as in [CSC+24] based on instructions, which retain only one trajectory for each instructions and no intersection between train and test. For a fair comparison, we only use their test split for evaluation and discard the train set as our method does not require finetuing. In table 4, we report the GPT-4V baseline in [YYZ+23] paper, which corresponds to the best performing set up (GPT-4V ZS+history) that uses UI elements detected by IconNet [SWL+22] through set-of-marks prompting [YZL+23] for each screenshot at every step of the evaluation. The detected UI elements consist of either OCR-detected text or an icon class label, which is one of the 96 possible icon types identified by IconNet. Additionally, action history is also incorporated at each step’s prompt as well. We used the exact same prompt format in [YYZ+23] except the results from the IconNet model is replaced with the output of the finetuned interactable region detection (ID) model. Interestingly, we found that the ID model can generalize well to mobile screen. By replacing the IconNet with the interactable region detection (ID) model we finetuned on the collected webpages, and incorporating local semantics of icon functionality (LS), we find OMNIPARSER\n7\ndelivers significantly improved performance across most sub-categories, and a 4.7% increase in the overall score compared to the best performing GPT-4V + history baseline.\nMethods ChatGPT-CoT PaLM2-CoT GPT-4V image-only GPT-4V + history OmniParser (w. LS + ID)\nModality General\nInstall GoogleApps\nSingle WebShopping Overall 8.4 - 45.7 48.2 52.9\n4.4 - 42.6 46.1 57.8 Table 4: Comparison of different methods across various tasks and overall performance in AITW benchmark.\nText Text Image Image Image\n5.9 - 41.7 43.0 48.3\n10.5 - 49.8 49.2 51.6\n9.4 - 72.8 78.3 77.4\n7.7 39.6 50.5 53.0 57.7\n5 Discussions\nIn this section, we discuss a couple of common failure cases of OMNIPARSER with examples and potential approach to improve.\nRepeated Icons/Texts From analysis of the the GPT-4V’s response log, we found that GPT-4V often fails to make the correct prediction when the results of the OMNIPARSER contains multiple repeated icons/texts, which will lead to failure if the user task requires clicking on one of the buttons. This is illustrated by the figure 7 (Left) in the Appendix. A potential solution to this is to add finer grain descriptions to the repeated elements in the UI screenshot, so that the GPT-4V is aware of the existence of repeated elements and take it into account when predicting next action.\nCorase Prediction of Bounding Boxes One common failure case of OMNIPARSER is that it fails to detect the bounding boxes with correct granularity. In figure 7 (Right), the task is to click on the text ’MORE’. The OCR module of OMNIPARSER detects text bounding box 8 which encompass the desired text. But since it uses center of the box as predicted click point, it falls outside of the ground truth bounding box. This is essentially due to the fact that the OCR module we use does not have a notion of which text region are hyperlink and clickable. Hence we plan to train a model that combines OCR and interactable region detection into one module so that it can better detect the clickable text/hyperlinks.\nIcon Misinterpretation We found that in some cases the icon with similar shape can have different meanings depending on the UI screenshot. For example, in figure 8, the task is to find button related to ’More information’, where the ground truth is to click the three dots icon in the upper right part of the screenshot. OMNIPARSER successfully detects all the relevant bounding boxes, but the icon description model interpret it as: "a loading or buffering indicator". We think this is due to the fact that the icon description model is only able to see each icon cropped from image, while not able to see the whole picture during both training and inference. So without knowing the full context of the image, a symbol of three dots can indeed mean loading buffer in other scenarios. A potential fix to this is to train an icon description model that is aware of the full context of the image.\n6 Conclusion\nIn this report, We propose OMNIPARSER, a general vision only approach that parse UI screenshots into structured elements. OMNIPARSER encompasses two finetuned models: an icon detection model and a functional description models. To train them, we curated an interactable region detection dataset using popular webpages, and an icon functional description dataset. We demonstrate that with the parsed results, the performance of GPT-4V is greatly improved on ScreenSpot benchmarks. It achieves better performance compared to GPT-4V agent that uses HTML extracted information on Mind2Web, and outperforms GPT-4V augmented with specialized Android icon detection model on AITW benchmark. We hope OMNIPARSER can serve as a general and easy-to-use tool that has the capability to parse general user screen across both PC and mobile platforms without any dependency on extra information such as HTML and view hierarchy in Android.\n8\nAcknowledgement\nWe would like to thank Corby Rosset and authors of ClueWeb22 for providing the seed urls for which we use to collect data to finetune the interactable region detection model. The data collection pipeline adapted AutoGen’s multimodal websurfer code for extracting interatable elements in DOM, for which we thank Adam Fourney. We also thank Dillon DuPont for providing the processed version of mind2web benchmark.\nReferences\n[BEH+23] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa˘gnak Ta¸sırlar. Introducing our multimodal models, 2023. [BZX+21] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise Aguera y Arcas. Uibert: Learning generic multimodal representations for ui understanding, 2021.\n[CSC+24] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents, 2024. [DGZ+23] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web, 2023. [DHF+17] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, UIST ’17, page 845–854, New York, NY, USA, 2017. Association for Computing Machinery.\n[FLN+24] Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction- finetuned foundation models, 2024.\n[GFH+24]\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Dou- glas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis, 2024.\n[HSZ+21] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby Lee, Jindong Chen, and Blaise Agüera y Arcas. Actionbert: Leveraging user actions for semantic understanding of user interfaces, 2021.\n[HWL+23] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.\n[KLJ+24]\nJing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po- Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.\n[LGP+18] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Rein- forcement learning on web interfaces using workflow-guided exploration. In Interna- tional Conference on Learning Representations (ICLR), 2018.\n[LHZ+20] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile UI action sequences. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8198–8210, Online, July 2020. Association for Computational Linguistics.\n[LLH+20] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements, 2020.\n[LLSH23]\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models, 2023.\n9\n[OXL+22] Arnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron VandenBerg, and Jamie Callan.\nClueweb22: 10 billion web documents with visual and semantic information, 2022.\n[RLR+23] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android device control, 2023. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces, 2023.\n[SJC+23]\n[SWL+22] Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Jindong Chen, Abhanshu Sharma, and James Stout. Towards better semantic understanding of mobile interfaces. CoRR, abs/2210.02663, 2022.\n[WLZ+21] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li.\n[WXJ+24]\nScreen2words: Automatic mobile ui summarization with multimodal learning, 2021. Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration, 2024.\n[WXY+24] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception, 2024.\n[XZC+24] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024.\n[YYZ+23] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. Gpt- 4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation, 2023. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, 2023.\n[YZL+23]\n[YZS+24] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms, 2024.\n[ZGK+24] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a\ngeneralist web agent, if grounded, 2024.\n[ZXZ+24] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xi- anyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. ICLR, 2024.\n10\n7 Appendix\n7.1 Details of Icon-Description Dataset\nIn figure 4, we see that the original BLIP-2 model tend to focus on describing shapes and colors of app icons, while struggling to recognize the semantics of the icon. This motivates us to finetune this model on an icon description dataset. For the dataset, we use the result of parsed icon bounding boxes inferenced by the interactable icon detection model on the ScreenSpot dataset since it contains screenshots on both mobile and PC. For the description, we ask GPT-4o whether the object presented in the parsed bounding box is an app icon. If GPT-4o decides the image is an icon, it outputs one- sentence description of the icon about the potential functionality. And if not, GPT-4o will output ’this is not an icon’, while still including this in the dataset. In the end, we collected 7185 icon-description pairs for finetuning. We finetune BLIP-2 model for 1 epoch on the generated dataset with constant learning rate of 1e−5, no weight decay and Adam optimizer. We show a few of the qualitative examples of finetuned model vs the original model in figure 4.\nFigure 4: Example comparisons of icon description model using BLIP-2 (Left) and its finetuned version (Right). Original BLIP-2 model tend to focus on describing shapes and colors of app icons. After finetuning on the functionality semantics dataset, the model is able to show understanding of semantics of some common app icons.\n7.2 Training details of Interactable Icon Region Detection Model\nAs introduced in 3.1, we train a YOLOv8 model on the interactable icon region detection dataset. We collect in total of 66990 samples where we split 95% (63641) for training, and 5% (3349) for validation. We train for 20 epochs with batch size of 256, learning rate of 1e−3, and the Adam optimizer on 4 GPUs. We show the training curve in figure 5.\n7.3 Details of SeeAssign Evaluation\n7.3.1 Prompt Used for GPT-4V\nGPT-4V without local semantics:\nHere is a UI screenshot image with bounding boxes and corresponding labeled ID\noverlayed on top of it, your task is {task}. Which icon box label you should operate on? Give a brief analysis, then put your answer in the format of \\n‘‘‘ Box with label ID: [xx]‘‘‘\\n\nGPT-4V with local semantics:\nHere is a UI screenshot image with bounding boxes and corresponding labeled ID overlayed on top of it, and here is a list of icon/text box description: {\n11\nFigure 5: Training curves of interactable icon region detection model.\nparsed_local_semantics}. Your task is {task}. Which bounding box label you should operate on? Give a brief analysis, then put your answer in the format of\n\\n‘‘‘Box with label ID: [xx]‘‘‘\\n\n7.4 Details of Mind2Web Evaluation\nHere we list more details of each baseline in table 3. SeeClick, QWen-VL SeeClick is a finetuned version of Qwen-VL on the Mind2Web training set and we report both of their numbers in their paper. CogAgent CogAgent number is taken from the SEEAct paper [ZGK+24], where they report cogagent- chat-hf checkpoint that is not fine-tuned on Mind2Web for experiments. MindAct(Gen), MindAct, GPT-3.5-Turbo, GPT-4 The numbers for these baseline are taken from the Mind2Web [DGZ+23] paper, where they use HTML information to augument the corresponding web agent. GPT-4V+som This model corresponds to the image annotation grounding method in SeeAct paper, where the som boxes extracted from the selelcted HTML elements are provided to GPT-4V to make action prediction. GPT-4V+textual choice This corresponds to the best performing scenario in SeeAct paper (except the Oracle), that uses the selected HTML elments information in a multi-choice question format as input to the GPT-4V agent.\n7.4.1 Qualitative Examples\nWe list a few more examples to demonstrate local semantics of icon function description helps GPT-4V make better action prediction in figure 6.\n12\nFigure 6: More examples of local semantics of icon functionality help with GPT-4V in grounding actions\nFigure 7: Analysis of failure cases. All the bounding boxes are labeled by which relies only on the screenshot. Left: There are in total 7 similar enable button for 7 different alarm times in the parsed screenshot. And the correct Icon ID corresponding to alarm 7:30 is 27. GPT-4V fails to make the correct prediction. Right: The ground truth region to click is the text ’MORE’ inside bounding box 8. We can see that the OCR fails to detect the text ’MORE’ in bold, and only detects the bounding box 8, which encompasses ’MORE’. Since the predicts the click point as the center of the box, so it the predicted click point falls outside of the ground truth region, which leads to failure in this task.\n13\nFigure 8: Analysis of failure cases. The task is to find button related to ’More information’, and the ground truth is to click the three dots icon in the upper right part of the screenshot. The the icon functional description model does not take into account the context of this page and interpret it as: "a loading or buffering indicator" which causes the failure.\n14'}</code></pre>
</div>
</div>
<section id="llama-pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="llama-pre-processing">Llama Pre-Processing</h3>
<p>After processing the PDF files, we’ll use Llama to clean up the extracted text rather than using regex.</p>
<p>This approach handles messy PDF extracts containing formatting, LaTeX, tables etc. more elegantly.</p>
<p>Again, these is a methid of the class <code>process_chunks</code> that will process the chunks of text and return the cleaned up text.</p>
<p>The <code>SYS_PROMPT</code> is hard coded into the class but can be altered to improve the output.</p>
<p>Instead of having the model process the entire file at once, the file is chunkced using the <code>SentenceTransformersTokenTextSplitter</code> to avoid the LLM context length limit and to cut the words or tokens in the middle of a sentence.</p>
<p>Another option is to use a SentenceSplitter, to cut by sentences, which can keep the meaning in a more natural way. For this can be used the <code>Spacy</code> library and the <code>en_core_web_sm</code> model.</p>
<div id="8c8f5611" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> file_processor.process_chunks()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="40e1b87b" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['large language models have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. this paper introduces ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. utilizing a tokenized early-fusion approach, ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities.\n\nichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. notably, ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models.',
 'cascaded systems the current approach involves an asr system followed by an nlu pipeline and then an nlg system for generating a response but this results in high latency due to the multiple steps involved our new model ichigo aims to improve upon some models have been proposed previously but we will discuss those later first some research has focused on reducing latency and improving user experience while one major limitation is that they typically address only a single type of data such as text or speech some newer models attempt to handle multiple data types simultaneously however these systems still require complex deployment on edge devices',
 'systems add up to several seconds of delay. this contrasts with natural conversations, where responses typically come within milliseconds. \nsecond, complexity deployment in edge device ( model compatible with conventional method and not cascaded system ). recent models that handle multiple types of data have become popular, but they still process different data types separately. \nthis can limit how well they combine information from different sources and create documents that mix speech and text. \nin this paper, we present ichigo, a mixed - modal model capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and speech content.\nichigo allows for complete modeling of documents with multiple data types, expanding on tasks like understanding speech and text - only language models.\nprevious research has trained entire models from scratch, including the llm. although this method is optimal, it is expensive and challenging for many research labs to adapt and build upon. \nour approach, in contrast, utilizes current strong open - source llms and extends their capability to speech through continual pre - training.\nichigo is designed to be mixed - modal from the start, employing a uniform architecture in an end - to - end fashion on an interleaved mixture of modalities : speech and text.\nby quantizing speech into discrete tokens, allowing us to use a decoder - only transformer architecture for both speech and text tokens.',
 'uniform architecture in an end - to - end fashion on an interleaved mixture of modalities : speech and text. by quantizing speech into discrete tokens, allowing us to use a decoder - only transformer architecture for both speech and text tokens, without adding a speech encoder and a speech adaptor \n\nthis approach projected different data types into a shared representational space from the start, allows for smooth reasoning and generation across modalities\n\nour contributions include \n1. ichigo, an tokenized early fusion multimodal model capable of reasoning over and generating interleaved speech - text documents\n2. training techniques without starting from scratch, making our approach more accessible and adaptable\n3. recovering capability training method to stabilize cross - modality training, enhancing the robustness of our model\n\nwe introduce instruction speech [ homebrewresearch, 2024 ], a large - scale english speech - text cross - modal instruction - following dataset featuring multi - turn interactions',
 'following dataset featuring multi-turn interactions, reasoning tasks, and refusal scenarios, providing training and inference code for further research. unified framework using token-based representations for both speech and textual modalities, leveraging a transformer architecture. by quantizing continuous speech into discrete tokens, similar to words in text, this facilitates cross-modal reasoning and generation. \n\ntokenization process for speech uses whispervq, a component of whisperspeech, processing audio resampled to 16 kHz with a frame rate of 25 Hz. initially, the audio is converted to a log-mel spectrogram before being processed by a whisper encoder, producing continuous embeddings. these embeddings undergo downsampling and refinement, then vector quantization, resulting in discrete tokens representing the audio content.\n\nthe dataset represents speech and text modalities as discrete tokens, utilizing a uniform transformer-based architecture.',
 'figure 1 ichigo represents speech and text modalities as discrete tokens and uses a uniform transformer-based architecture it uses whispervq to quantize speech into discrete tokens in the same manner with original text modality. \n2 expanding the language model to incorporate multimodal discrete representations into pre-trained llms we expand the vocabulary \nwith new modality-specific tokens this expansion necessitates extending the corresponding embeddings and prediction layer \nwith newly incorporated parameters initialized randomly\n3 2.4 model implementation details we use llama-3-8b-instruct as our backbone model which has been pre-trained on 15 trillion text tokens and performs well across benchmarks dubey et al 2024. \napart from reshaping the embedding matrix to accommodate the new tokens the rest of the language model remains unaltered \n\n(removed fluff)\nreplaced latex math with text\ncut at unnecessary words',
 "the embedding matrix to accommodate the new tokens, the rest of the language model remains unaltered. we introduce two new special tokens, &lt; | sound _ start | &gt; and &lt; | sound _ end | &gt;, to delimit audio file inputs. \nwe've modified our approach to initializing new token embeddings by averaging all existing vocabulary embeddings. this change improved training speed and stability. \nour dataset comprises a comprehensive collection of public automatic speech recognition (asr) datasets in eight languages: english, german, dutch, spanish, french, italian, portuguese, and polish. \nthe dataset spans approximately 16,000 hours of audio, including 10,000 hours of english audio from the mls english 10k dataset and additional hours distributed across other languages from the multilingual librispeech dataset",
 'public automatic speech recognition (ASR) datasets spanning eight languages: English, German, Dutch, Spanish, French, Italian, Portuguese, and Polish. We obtained English from the MLS English 10k dataset [Pratap et al., 2020] and other languages from the multilingual Librispeech dataset [not specified]. The training dataset encompasses approximately 10,000 hours of English audio and an additional 5,000 hours distributed across the other languages, mostly sourced from audiobooks available on Librivox and OpenSLR. \n\nOur post-training dataset comprises a diverse selection of high-quality data including sources from HuggingFace [Xu et al., 2024], Open-Orca, Magpie-align, etc.',
 'rosekirk, 2024, baai, 2024 ]. these datasets span a wide array of topics, thereby diversifying the input data for our model. \nwe implemented a two - step filtering process to ensure data quality and relevance. language identification : we applied the fasttext model [ bojanowski et al., 2017 ] as a language identifier at the document level, retaining only english documents with a confidence threshold of ( 0. 9 ). this decision aligns the model ’ s distribution more closely with the original multilingual training of the base llm. \ndeduplication : we removed duplicate entries to prevent overfitting and ensure a diverse training set. despite the tokenizer ’ s capacity to handle eight languages, we opted to focus primarily on english for this training iteration. \nthis decision was motivated by the relative scarcity of high - quality instruction data in other languages and the low - resource nature of these languages in our dataset. \n3. 2. 2 speech - text instruction data to prevent exceeding the llm ’ s context length, we filtered out text instructions longer than 64 tokens. this threshold was established based on empirical observations of typical user interactions with audio assistants. quality filtering : we eliminated samples that would be challenging to pronounce as speech, such as urls, mathematical symbols, and code snippets. \nsynthetic data generation pipeline : we implemented a two - stage process to convert our text - based instruction dataset into discrete sound tokens suitable for audio input.',
 'assistants require quality filtering to eliminate challenging samples such as urls and code snippets. \na two-stage process converts text-based instruction datasets into discrete sound tokens suitable for audio input\nthe model utilizes whisperspeech tts to generate audio files from the instruction dataset’s questions\nfive employs whispervq model to transform these audio files into discrete sound tokens \n\na data processing chart illustrates the multi-stage filtering and conversion process starting from 6m samples of multiple open-source instruction text datasets\nafter filtering, 2.2m samples are converted to speech instruction and text answer pairs \ncreating a rich multimodal dataset that mimics real-world interactions with audio-based ai assistants',
 'we experimented with a specialized transcribe instruction dataset derived from our asr dataset incorporating six instruction sentences for transcription tasks such as \ntranscribe the sound of rain falling on the roof transcribe the sound of ocean waves crashing against the shore or \ntranscribe any other sound you hear in nature  additionally we introduced signal to help model identify transcription tasks by using special token &lt; | transceive | &gt; initial approach led to catastrophic forgetting issue with model but transitioned to use pure instructions later',
 'Our team developed a diverse synthetic answer generation model using the 5-72b architecture to tackle inaudible inputs in speech recognition tasks. \nExposing our model to chaotic data arrangements allowed us to effectively distinguish between audible and inaudible inputs.\nWe also applied sequence length distribution matching between inaudible and audible data to ensure a balanced representation of both types of inputs.\n\nOur training process consisted of multiple stages, each optimizing distinct performance aspects. \nWe employed a pre-training methodology rooted in language model principles, introducing new tokens to represent speech characteristics and facilitating the development of basic concepts.\nWe utilized AdamW fused optimizer with specific hyperparameters.\n\nOur models were trained on our internal cluster with 10 NVIDIA A6000 GPUs.',
 'Training hyperparameters for Ichigo’s three-stage process \nParameter   Pre-training    Instruction FT    Enhancement FT    Weight decay    Learning scheduler    Optimizer    Precision \n0.005      Cosine         AdamW fused     BF16       Hardware               Steps               Global batch size     Learning rate        Warmup steps             Max length 10x A6000        45h           8064          480       2 × 10−4                          50                512                 8x h100        10h           7400         256       7 × 10−5                          73              4096               8x h100      3h             644',
 "ichigo's three-stage process. The first stage involves parameter pre-training and instruction, where the model is trained on a large corpus of text with 10x more parameters than regular models. This allows the model to learn effective representations for the input data.\n\nThe second stage is instruction fine-tuning, which focuses on developing the model's question-answering capabilities. To achieve this, we balance modalities during supervised fine-tuning (SFT) and use a carefully curated dataset comprising 70% speech instruction prompts and 20% text-only prompts. This distribution has been optimized through extensive permutation testing to maintain robust general language abilities while enhancing speech comprehension.\n\nThe third stage involves enhancement fine-tuning, where the model is trained on multi-turn conversations and responses to inaudible inputs. This stage builds upon the previous stage's model modifications, but I wont say what the improvements are",
 "speech transcription prompts, 20 % of the dataset, aiming to balance speech understanding, transcription capabilities, and general language skills. \n\n figure 3 shows the data distribution proportion for this training stage. a distribution of data types in the instruction fine-tuning dataset aimed to enhance speech comprehension while maintaining robust general language abilities. \nthe goal was to improve ichigo's robustness in handling multi-turn conversations and inaudible inputs.\n\nwe fine-tuned the model using a dataset of 158,000 samples and simulated real-world user interactions through data augmentation to improve ichigo's interactive capabilities. \n\n our experimental outcomes for ichigo evaluate its performance across multiple dimensions including question-answering capabilities, response latency and practical cases.",
 "ichigo’s performance across multiple dimensions includes question-answering capabilities and response latency, providing a comprehensive assessment of its capabilities in comparison to well-known speech language models. The experimental outcomes show that ichigo outperforms existing open-source speech language models, particularly those utilizing non-tokenized early fusion approaches. In the Speechbench evaluation, ichigo scores 67.8 on robust llama-3 70b model's SQA score, demonstrating its superior performance in comparison to other models.",
 'provided ratings in the middle of its responses rather than at the end as expected, resulting in lowered scores. to address this issue, we implemented a backfilling procedure for missing ratings, ensuring a more accurate representation of model performance. our results demonstrate that ichigo outperforms existing open source speech language models, particularly those utilizing non tokenized early fusion ntef approaches. compared to other end to end models ichigos performance is particularly impressive it outperforms qwen2 audio the next best performer among end to end models by 23 points on openhermes audio and 15.2 points on alpaca audio this substantial improvement underscores the effectiveness of ichigos architecture and training approach in capturing the nuances of speech language interactions',
 "to note that Ichigo achieves this as an end-to-end model without the need for separate transcription and language modeling phases, demonstrating its ability to effectively integrate speech understanding and language generation in a single model. \nThe latency to first token confirms the efficiency of Ichigo's tokenized early fusion architecture by comparing it to current speech models and cascaded systems. \nA comparative analysis was conducted on a single NVIDIA A6000-48GB GPU performing 10 iterations of the latency test using 10 diverse audio files with durations ranging from 1 to 5 seconds, reflecting real-world usage scenarios.\nThe results show that Ichigo achieves an average latency of 19ms in vram usage.",
 "_latency analysis:_\nlatency of approximately 110 ms, nearly 3 times faster than qwen2 - audio.\n\n_Speech Language Model Comparison:_\nichigo outperforms other speech language models in terms of latency, achieving a 4 times faster speed compared to the cascaded whisper + llama-3.8b system.\n\n_VRAM Efficiency:_\nimpressive vram footprint of approximately 19 GB.\n\n_Degradation Recovery:_\na critical concern for multi-modal models is retaining their original capabilities; ichigo's performance remains consistent across three popular llm benchmarks with significant improvement from earlier versions to v0.3.",
 "model capabilities evaluation across various domains provide a comprehensive study of the ichigo instruct v0. 3 ( phase 3 ) model's performance retention and recovery comparison to earlier versions like llama3 8b instruct model.\n\nthe findings show a significant improvement in performance from v0. 2 to v0. 3 ( phase 3 ), with notable reduction in performance degradation \n\nthe improvements are attributed to the refined training strategy that incorporates mixed proportion of text - only and sound token data, demonstrating remarkable recovery across all benchmarks including mmlu \nwhere ichigo instruct v0. 3 achieves a score of 63.79 compared to llama3 8b instruct's 69.4 on the same benchmark. \n\nthis upward trend in performance is consistent up to v0. 3 \nmeaning that extended training time and computational resources are necessary to achieve higher performance.\n\ninitial pre - training phase presents challenges in maintaining cross - modal capabilities during specialized training",
 "this indicates that with extended training time and more computational resources, we can achieve higher performance. pre-training challenges : it's worth noting that during the initial pre-training phase, which focused solely on sound tokens with next token prediction, we observed a significant degradation in the model's performance on text-based tasks, particularly in mathematics and coding. this highlights the challenges in maintaining cross-modal capabilities during specialized training. results across different versions of our model, llama3 8b instruct, and ichigo demonstrate improved performance in various metrics. notable improvements include instruction following capabilities, as demonstrated in real-world conversational scenarios.",
 'ichigo demonstrated a robust ability to follow text-based system prompts while engaging in speech-based conversations with users. this highlights the model\'s capacity to generalize instructions across modalities, a crucial feature for versatile AI assistants.\n\nthe model consistently maintained its prescribed identity as " ichigo " when questioned, adhering to the system prompt instructions. this behavior persisted regardless of whether the input was in text or speech format, demonstrating the model\'s ability to maintain context across different input modalities.\n\nichigo exhibited proficiency in managing multi-turn conversations, seamlessly understanding and responding to both speech and text inputs without apparent difficulties.\n\nthe model handled unclear inputs by refusing to provide random answers and instead politely requested the user to repeat their query.',
 "was inaudible, unclear, or affected by background noise. Ichigo demonstrated appropriate behavior by refusing to provide random answers.\n\nThe model politely requested the user to repeat their query, ensuring accurate responses.\nIn zero-shot multi-turn capabilities, the model follows text-based system prompts during speech-based conversations with users.\nThese experiments complement our quantitative findings, demonstrating Ichigo’s practical capabilities in real-world scenarios.\nIchigo's abilities in cross-modal instruction following, multi-turn dialogues, and handling unclear inputs make it a promising candidate for advanced, user-friendly voice AI applications.\n\nEarly research on audio language models drew inspiration from the success of language models in natural language processing (Radford et al., 2019; Raffel et al., 2020; OpenAI et al., 2024).\nThese early efforts explored training models using semantic or acoustic tokens derived from audio data, enabling audio generation without text input:\nBorsos et al. (2023); Nguyen et al. (2023); Lakhotia et al. (2021).\n\nAdvancing to joint training of speech and text led to decoder-only models such as Vall-E (Wang et al., 2023a; Chen et al., 2024) and Viola (Wang et al., 2023b).\nThese initial models demonstrated capabilities in speech recognition, translation, and synthesis",
 'al., 2021. Subsequent advancements led to the joint training of speech tokens and text, resulting in decoder-only models such as Vall-E [ wang et al., 2023a ] and Viola [ wang et al., 2023b ]. These models demonstrated capabilities in speech recognition, translation, and synthesis. However, these early models were not built upon large language models (LLMs). Researchers have explored various approaches to building speech-language models based on LLM architectures.\n\n6.2 LLM-based audio-language models\n\nRecent research has focused on two primary approaches to integrating speech and audio capabilities with LLMs: non-tokenized early fusion and tokenized early fusion.\n\nIn the non-tokenized early fusion approach, pre-trained encoders of other modalities are connected as adaptors to enable cross-modal perception. This method involves adding a speech encoder before the LLM and fine-tuning the entire model for speech understanding capabilities. These models excel in tasks such as speech recognition, speech translation, and general speech-to-text tasks [ chu et al., 2024, tang et al., 2023, shu et al., 2023, deshmukh et al., 2023, hu et al., 2024, das et al., 2024, fang et al., 2024 ].',
 '2023, deshmukh et al., 2023, hu et al., 2024, das et al., 2024, fang et al., 2024. Notable examples of this approach include Fang et al. 2024 (Llama - Omni), Shu et al. 2023 (LLASM), and Salmonn (Tang et al. 2023). These methods extend Large Language Model (LLM) capabilities to audio modality by integrating speech encoder, adaptor, and decoder. Qwen2 Audio (Chu et al. 2024) introduces an architecture combining audio encoder with LLM, training to maximize next text token probability conditioned on audio representations.',
 'former model, enabling multimodal output generation [ wadekar et al., 2024 ]. examples include chameleon [ team, 2024a ], audiopalm [ rubenstein et al., 2023 ] and voxtlm [ maiti et al., 2024 ], which utilize pre - trained language models. anygpt [ zhan et al., 2024 ] leverages llms to enable cross - modal conversation capabilities through speech-tokenizer, musictokenizer and imagetokenizer. unlike previous works, our approach retains current llms architecture while incorporating whispervq to preserve most of the openai whisper encoder block. this allows us to generate embeddings which are then quantized to obtain semantic tokens. stabilization of loss in cross-modality training is a key challenge addressed in our work.',
 "here is the cleaned up text:\n\nnative multimodal foundation model designed for seamless audio-text interactions\nit employs a 7b parameter multimodal language model, processing speech input and output concurrently, generating text tokens and audio codecs.\nichigo, an early-fusion token-based speech model sets a new approach for multi-modal models\nby learning a unified representation space over interleaved speech and text tokens, ichigo achieves strong performance across speech-languages benchmarks\nkey to ichigo's success lies in its fully token-based architecture allowing seamless information integration across modalities \nby quantizing speech into discrete tokens utilizing a strong base llm, ichigo jointly reasons over speech and text surpassing late-fusion architectures \nour meticulous approach preserves the original llm performance extending capabilities to the speech domain\nichigo outperforms other end-to-end speech language models in speech-based question-answering tasks \ndemonstrates real-time speech system with a latency of 110 milliseconds to first response",
 "end speech language models in speech-based question-answering tasks, marking a significant step forward in multimodal AI. \n Ichigo demonstrates a real-time speech system with a latency of 110 milliseconds to first response.\n This opens up new possibilities for speech systems and significantly reduces the complexity of deploying such systems in production environments.\n We believe that this paper will empower smaller research teams to contribute more confidently and prolifically to the open-source community.\n By demonstrating that significant advancements can be achieved with limited resources, we hope to inspire broader participation in this critical area of AI research.\n\n Several limitations and areas for future work remain:\n \n Token stability was a challenge when training with acoustic tokens, leading us to shift towards semantic tokens to achieve stable loss. This highlights the difficulty in training with rich, acoustic information. Future work should explore methods to stabilize training with acoustic tokens, potentially unlocking even more powerful models.\n Emotional understanding is currently not fully accounted for in the model's ability to understand and respond to user emotions. Future iterations should focus on enhancing the model's ability to handle emotional contexts and provide more nuanced responses.\n Context length is a limitation as Ichigo models 10 seconds of speech input which performs well but may struggle with longer audio segments or complex, multi-turn conversations.\n\n15 references \n BAII. Infinity-Instruct, 2024",
 "contextualized language models can be effective in handling multimodal content and complex conversations.\nthe 10-second speech input limit for ichigo may be a constraint for full utilization of its capabilities.\nenlarging the context window could significantly improve the model's performance and enable it to handle longer audio segments and more intricate multi-turn interactions.",
 'tan, jinyu li, sheng zhao, yao qian, and furu wei. vall - e 2 \nneural codec language models are human parity zero - shot text to speech synthesizers. arxiv preprint arxiv : 2406. 05370, 2024. \nyunfei chu, jin xu, qian yang, haojie wei, xipin wei, zhifang guo, yichong leng, yuanjun lv, jinzheng he, junyang lin, et al.\nqwen2 - audio technical report. arxiv preprint arxiv : 2407. 10759, 2024. \nkarl cobbe, vineet kosaraju, mohammad bavarian, mark chen, heewoo jun, lukasz kaiser, matthias plappert, jerry tworek, jacob hilton, reiichiro nakano, et al.\ntraining verifiers to solve math word problems. arxiv preprint arxiv : 2110. 14168, 2021. \ncollabora. whisperspeech, 2024. accessed : 19 october 2024. \nnilaksh das, saket dingliwal, srikanth ronanki, rohit paturi, david huang, prashant mathur, jie yuan, dhanush bekal, xing niu, sai muralidhar jayanthi, et al.\nspeechverse : a large - scale generalizable audio language model. arxiv preprint arxiv : 2405. 08295, 2024. \nalexandre defossez, laurent mazare, manu orsini',
 'speech verse : a large scale generalizable audio language model\narxiv preprint arxiv : 2405.08295, 2024\nalexandre defossez et al \nmoshi : a speech text foundation model for real time dialogue\narxiv preprint arxiv : 2410.00037, 2024\nsoham deshmukh et al\npengi : an audio language model for audio tasks\nadvances in neural information processing systems, 36 : 18090 – 18108, 2023\nabhimanyu dubey et al \nthe llama 3 herd of models\narxiv preprint arxiv : 2407.21783, 2024\nalexandre defossez et jade copet\nhigh fidelity neural audio compression\narxiv preprint arxiv : 2210.13438, 2022\nqingkai fang et al \ngsm8k multiturn \nurl https : / / huggingface. co / datasets / euclaise / gsm8k _ multiturn',
 '13438, 2022. euclaise. gsm8k _ multiturn, 2024. url https :/huggingface.co/datasets/euclaise/gsm8k_multiturn. qingkai fang, shoutao guo, yan zhou, zhengrui ma, shaolei zhang, and yang feng. \nllama - omni : seamless speech interaction with large language models.\narxiv preprint arxiv : 2409.06666, 2024. allen institute for ai.\nwildchat - 1m, 2024. url https :/huggingface.co/datasets/allenai/wildchat-1m.\nleo gao, jonathan tow, baber abbasi, stella biderman, sid black, anthony dipofi, charles foster, lau-rence golding, jeffrey hsu, alain le noac’h, haonan li, kyle mcdonell, niklas muennighoff, chris ociepa, jason phang, laria reynolds, hailey schoelkopf, aviya skowron, lintang sutawika, eric tang, anish thite, ben wang, kevin wang, and andy zou.\na framework for few-shot language model evaluation, 07 2024. url https :/zenodo.org/records/12608602.\n\nmaking llama see and draw with seed tokenizer.\narxiv preprint arxiv : 2310.01218, 2023. \n16',
 'Making Llama See and Draw with Seed Tokenizer Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan.\n\nPrism Alignment HannahRoseKirk. \nAlignment URL https://huggingface.co/datasets/hannahrosekirk/prism-alignment\n\nMassive Multitask Language Understanding Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring Massive Multitask Language Understanding Arxiv : 2009.03300, 2020. URL https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n\nTraining Compute Optimal Large Language Models Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las Casas, Lisa Anne Hendricks, Johannes Welbl, and Aidan Clark.\n\nInstruction Speech HomebrewResearch.\nHugging Face Dataset, 2024\n\n Low-Rank Adaptation of Large Language Models Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nArxiv : 2106.09685, 2021.\n\n Small updates from Shujie Hu, Long Zhou',
 'This is a paper on generative models for speech data, specifically large language models and their applications, including low-rank adaptation, robustness, and adaptation to raw audio. The authors discuss the advancements in this field and highlight various architectures such as Lora and wavllm. They also mention datasets like HuggingFacetb, Intel\'s OrcA-DPO-Pairs, and recent publications by Kushal Lakhotia et al., including a paper in TAACL on "On Generative Spoken Language Modeling from Raw Audio".',
 'yossi adi, adam polyak, benjamin bolte, tu anh nguyen, jade copet, alexei baevski, abdelrahman mohamed, et al. on generative spoken language modeling from raw audio. \ndecoupled weight decay regularization \n\nvottlm : unified decoder - only models for consolidating speech recognition, synthesis and speech , text continuation tasks.\n\nmicrosoft. orca. \n\ntu anh nguyen, eugene kharitonov, jade copet, yossi adi, wei-ning hsu, ali elkahky, paden tomasello, robin algayres',
 "An interesting-looking text snippet. Initial processing reveals it consists of citations and names involved in research related to computational linguistics and natural language processing.\n\nNames associated with the mentioned publications:\n\ntomasello robin algayres benoit sagot abdelrahman mohamed yossi adi jade copet wei ning hsu eugene kharitonov paden tomasello \n\nAdditionally, there's an indication of a new resource being developed, specifically in the area of GPT and spoken dialogue language models.\n\nprocesssed names:\n \n robin algayres\n benoit sagot\n abdelrahman mohamed\n yossi adi\n jade copet\n wei ning hsu\n eugene kharitonov",
 'irwan bello, jake berdine, gabriel bernadett - shapiro, christopher berner, lenny bogdonoff, oleg boiko, madelaine 17 boyd, anna - luisa brakman, greg brockman, tim brooks, miles brundage, kevin button, trevor cai, rosie campbell, andrew cann, brittany carey, chelsea carlson, rory carmichael, brooke chan, che chang, fotis chantzis, derek chen, sully chen, ruby chen, jason chen, mark chen, ben chess, chester cho, casey chu, hyung won chung, dave cummings, jeremiah currier, yunxing dai',
 'mike heaton johnnes heidecke chris hesse alan hickey wade hickey peter hoeschele brandon houghton kenny hsu shengli hu xin hu joost huizinga shantanu jain shawn jain joanne jang angela jiang roger jiang haozhun jin denny jin shino jomoto billie jonn heewoo jun tomer kaftan lukasz kaiser ali kamali ingmar kanitscheider nitish shirish keskar tabarak khan logan kilpatrick jong wook kim christina kim yongjik kim jan hendrik kirchner jamie kiros matt knight daniel kokotajlo lukasz kondraciuk andrew kondrich aris konstantinidis kyle kosic',
 'Scott Mayer McKinney, Jacob Menick and David Medina \n\n Scott Mayer McKinney and  Christine McLeavey \n Scott Mayer McKinney, Christine McLeaves and Paul McMullen. Scott Mayer Mekinny \n\nDavid Medina\nAalok Mehta \nAlok Mehta\nJacob Menick \nLuke Metz \nAndrey Mishchenko \nPamela Mishkin \nVinnie Monaco \nEvan Morikawa',
 'kar, girish sastry, heather schmidt, david schnurr, john schulman, daniel selsam, kyla sheppard, toki sherbakov, jessica shieh, sarah shoker, pranav shyam, szymon sidor, eric sigler, maddie simens, jordan sitkin, katarina slama, ian sohl, ben - jamin sokolowsky, yang song, natalie staudacher, felipe petroski, natalie summers, ilya sutskever, jie tang, nikolas tezak, madeleine b. thompson, phil tillet, amin tootoonchian, elizabeth tseng, pre - ston tuggle, nick turley, juan felipe ceron uribe, andrea vallone, arun vijayvergiya, chelsea voss, carroll wainwright, justin jay wang, alvin wang, ben wang, jonathan ward, jason wei, cj weinmann, akila welihinda, peter welinder, jiayi weng, lilian weng, matt wiethoff, dave willner, clemens winter, samuel wolrich, hannah wong, lauren workman, sherwin wu, jeff wu, michael wu',
 'chong zhang, marvin zhang, shengjia zhao, tianhao zheng, juntang zhuang, william zhuk, barret zoph. gpt - 4 technical report, 2024. url https : / / arxiv. org / abs / 2303. 08774. vassil panayotov, guoguo chen, daniel povey, sanjeev khudanpur. librispeech : an asr corpus based on public domain audio books. in 2015 ieee international conference on acoustics, speech and signal processing ( icassp ). performance deep learning library. information processing systems 32, pages 8024 – 8035. \n\npjmixers. math - multiturn - 10k - sharegpt.',
 'high-performance deep-learning library for advances in neural information processing systems, presented by pjmixers, is researched on multiturn dialogues with sharegpt model, published on arxiv and huggingface dataset URLs \n\narxiv articles, including "mls: a large-scale multilingual dataset for speech research", present various methods to process and utilize the mls dataset \n PubmedQA  is introduced in arxiv publication as language model unsupervised task for question answer datasets',
 "colin raffel, noam shazeer, adam roberts, katherine lee, sharan narang, michael matena, yanqi zhou, wei li, and peter j liu are leading researchers who have explored the limits of transfer learning with a unified text-to-text transformer. their work has been featured in top conferences including journal of machine learning research and arxiv preprint.\n\nresearchers aditya ramesh, prafulla dhariwal, alex nichol, casey chu, and mark chen have developed hierarchical text-conditional image generation using clip latents. another notable effort comes from david rein and team who introduced gpqa: a graduate-level google-proof q&amp;a benchmark that will serve as a crucial resource for measuring human-augmented language models.\n\nrecent advancements include routellm's gpt4 dataset, a new benchmarking framework for gpt models, as well as paul k rubenstein's audiopalm, a large language model capable of speaking and listening.",
 "dalia el badawy, wei han, eugene kharitonov, et al. \naudiopalm: a large language model that can speak and listen.\narxiv preprint arxiv: 2306.12925, 2023.\n\nyu shu, siwei dong, guangyao chen, wenhao huang, ruihua zhang, daochen shi, qiqi xiang, and yemin shi. \nllasm: large language and speech model.\narxiv preprint arxiv: 2308.15930, 2023.\n\nchangli tang, wenyi yu, guangzhi sun, xianzhao chen, tian tan, wei li, lu lu, zejun ma, and chao zhang. \nsalmonn: toward general hearing abilities for large language models.\narxiv preprint arxiv: 2310.13289, 2023.\n\nchameleon team. \nchameleon: mixed-modal early-fusion foundation models.\narxiv preprint arxiv: 2405.09818, 2024a.\n\nqwen team. qwen2.5: a party of foundation models, september 2024b. url https://qwenlm.github.io/blog/qwen2.5/. \n\ntorchtune maintainers and contributors. \ntorchtune: pytorch's finetuning library, 2024. url https://github.com/pytorch/torchtune.\n\nhugo touvron, thibaut lavril, gautier izacard, xavier martinet, marie-anne lachaux, timothee lacroix, baptiste roziere, naman goyal, eric hambro, faisal azhar, et al. \nllama: open and efficient system",
 "pytorch's finetuning library provides a simple and efficient way to fine-tune pre-trained models on various downstream tasks.\n\nResearchers have been actively exploring the development of multi-modal model architectures, which can effectively leverage multiple input sources such as text, images, and audio to improve performance on complex tasks. \n\nA universal benchmark for audio large language models has also been proposed, with applications in audio-to-text and speech synthesis.\n\nRecent advancements in foundation language models have enabled the creation of more efficient and effective architectures, including the LLaMA model. Additionally, researchers have explored the use of zero-shot text-to-speech synthesizers using neural codecs.",
 'Neural codec language models are zero-shot text-to-speech synthesizers. \nUnified codec language models for speech recognition, synthesis and translation. \nAlignment data synthesis from scratch by prompting aligned LLMs with nothing. \nUnified multimodal LLM with discrete sequence modeling. \nUnified speech tokenizer for speech large language models.',
 "anygpt : unified multimodal llm with discrete sequence modeling. arxiv preprint arxiv : 2402. 12226, 2024. xin zhang, dong zhang, shimin li, yaqian zhou, and xipeng qiu.\n\nspeechtokenizer : unified speech tokenizer for speech large language models. arxiv preprint arxiv : 2308. 16692, 2023.\n\nadditional data and analysis this appendix provides supplementary information on the audio speech recognition (asr) prompt library and ablation studies conducted during our research.\n\na.1 asr prompt library table 5 presents a collection of prompts used for the ichigo model transcription tasks. \nthese prompts were designed to elicit accurate speech-to-text conversions across various contexts.\n\ntable 5. audio speech recognition (asr) prompt library for ichigo model transcription tasks\ntranscribe prompts: \nto transcribe : &lt;speech&gt; : convert the spoken words to text \nin this clip : &lt;speech&gt;\nwhat is being said in this audio clip : &lt;speech&gt; \ntranscript the following audio sample : &lt;speech&gt;\nplease write down what is being said : &lt;speech&gt; \ngenerate a transcript from this sound file : \nrecognize speech : &lt;speech&gt;\nproduce a text version of the recording : \n\na.2 ablation studies \n\nconducting ablation studies to investigate impact of different training configurations on model's performance\n\ntable 6 summarizes results of experiments\ntranscribe token: speechqa instruction: transcription mmlu recovery test 1 : recovery: test 2 : recovery : test 3 :\n1  : test 1 with/without introducing new transcribe token : \ntest name:\nrecovery test 2  : 0.515 mmlu \nrecovery test3 :  0.480",
 'a. our ablation studies investigated the impact of different training configurations on model performance.\nwe experimented with various training setups and analyzed the results in table 6.\n\ntable 6.\n\nablations\ntranscribe token speechqa instruction transcription mmlu recovery test 1 0 1 1 1 1 0 0.515\ntranscription token speechqa instruction transcription mmlu recovery test 2 1 0 1 0.480 1 0.630\ntranscription token speechqa instruction mmlu recovery test 3 1 0 0 0.63\ntest 4 without transcription tokens and data results were lower\n\nthe key finding from our experiments is that excluding transcription data entirely can outperform including it, indicating potential interactions between training data types.\n \nour study highlights the importance of considering various components when developing strategies for multi-modal language models.',
 "the recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. however, we argue that the power of multimodal models like gpt-4v as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique.\n\nwe introduce omniparser, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of gpt-4v to generate actions that can be accurately grounded in the corresponding regions of the interface. our approach involves two key stages: \n\n1) interactable icon detection and 2) semantic analysis of various elements in a screenshot.\n\nto demonstrate the effectiveness of omniparser, we created datasets for interactable icon detection and semantic description of elements. these datasets were used to fine-tune specialized models, including a detection model and a caption model. our results show that omniparser substantially improves gpt-4v's performance on screenspot benchmark and outperforms gpt-4v's baseline on mind2web and aitw benchmaks using only screenshots as input.",
 'These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen. OMniparser significantly improves GPT-4v’s performance on screenspot benchmark, outperforming GPT-4v baselines using only screenshots as input. Large language models have shown successful understanding and reasoning capabilities, but there is a significant gap between current technology and creating widely usable agents across multiple platforms. The ability to understand basic UI elements via screenshot has demonstrated abilities, however, action grounding remains one of the key challenges in converting predicted actions to actual keyboard/mouse movement or API calls.',
 'While large multimodal models like GPT-4v and other models trained on UI data have demonstrated abilities to understand basic elements of the UI screenshot, action grounding remains one of the key challenges. It has been noted that GPT-4v is unable to produce the exact x-y coordinate of the button location, resulting in limited accuracy. However, by applying set-of-marks prompting approach, where bounding boxes are overlaid on top of the original image, models like GPT-4v can ground actions into a specific bounding box with grounding truth location, showcasing improved robustness.\n\nHowever, current solutions relying on parsed HTML information for extracting actionable elements pose limitations on usage. We aim to develop a general approach applicable across various platforms and applications, highlighting the inadequacy of pure vision-based screen parsing techniques used in previous studies, which lead to understated model understanding capabilities. The development of a reliable, general-purpose vision-based screen parsing method is crucial for enhancing the robustness of agentic workflow across diverse operating systems and applications.\n\nomnparser, a comprehensive screen parsing tool, enables the extraction of structured bounding boxes from UI screenshots. By providing a refined understanding of user interface elements through bounding box information, omnparser bridges the gap in achieving reliable action grounding for GPT-4v models and beyond, enabling seamless integration into an array of platforms and applications.',
 'we argue that previous pure vision-based screen parsing techniques are not satisfactory, which lead to significant underestimation of gpt-4v model’s understanding capabilities. we present omniparser, a general screen parsing tool to extract information from UI screenshot into structured bounding box and labels which enhances gpt-4v’s performance in action prediction in various user tasks.\n\nwe list our contributions as follows : \n\nwe curate interactable region detection dataset using bounding boxes extracted from dom tree of popular webpages.\n we propose omniparser, a pure vision-based user interface screen parsing method that combines multiple finetuned models for better screen understanding and easier grounded action generation.\n we evaluate our approach on screenspot, mind2web and aitw benchmark, and demonstrated significant improvement from the original gpt-4v baseline without requiring additional input other than screenshot. \n\nnote : \nthese related works focus on detailed understanding of UI screens but rely on addilitional information or are trained for specific tasks.\n examples include screen2words, ui-bert, widgetcaptioning, action-bert and rico dataset which contains over 66k unique ui screens',
 'these works demonstrated effective usage of multimodal models for extracting semantics of user screen. but these models rely on additional information such as view hierarchy, or are trained for visual question answering tasks or screen summary tasks. however, there is a lack of publicly available dataset for ui screen understanding.\n\none notable example is the rico dataset which contains over 66k unique ui screens and its view hierarchies. a dataset called pixelhelp provides a similar insight on mobile platform, spanning across 88 common tasks, as well as cleaned version called ricosca.\n\nthere are also several other works aiming to provide simulated environment, such as mind2web, miniwob, visual - webarena, os - world, however they do not explicitly provide dataset for general screen understanding tasks.',
 'here is processed text for this portion: \n\nautonomous gui agents are being designed to perform tasks in place of human users with one line of work focusing on training an end-to-end model to directly predict the next action as seen in pixel2act, webgum, ferret, cogagent and fuyu. another line of work leverages multimodal models like gpt-4v to perform user tasks with examples including mindact agent, seeact agent, agents from yyz, wxy, rlr research groups',
 'we focus on providing a systematic approach for getting structured information from general user screens, especially for cross-platform and cross-application tasks.\n\nwe propose omniparser, an integrated model that leverages the outputs from three key components:\n\n*   interactable icon detection model\n*   icon description model\n*   ocr module\n\nthis combination produces a structured, DOM-like representation of the UI, complete with screenshot overlays highlighting potential interactable elements. \n\nwe break down the task into several steps, each requiring our model to:\n\n1.  analyze the screen content overall and detect functional icons labeled with numeric IDs.\n2.  predict the next action that would help complete the entire task.\n\nby extracting certain information, such as semantics, during the screen parsing stage, we alleviate the burden on our primary model and enable it to focus on action prediction. \n\nkey benefits of omniparser include:\n\n*   improved efficiency in handling complex tasks\n*   enhanced accuracy in identifying interactable elements\n*   streamlined approach for obtaining structured UI data',
 'om parser which integrates outputs from finetuned icon detection model, description model, and OCR module produces structured DOM like representation of UI with screenshot overlaid with bounding boxes for potential interactable elements discuss each component in more details this is a step to reason about what actions should be performed given user tasks instead of directly prompting gpt - 4v predict xy coordinate value use set - of - marks approach overlay bounding boxes on ui screenshot and ask gpt - 4v generate bounding box id to perform action',
 'buttons specifically we curate a dataset of interactable icon detection dataset containing 67k unique screen shot images each labeled with bounding boxes of interactable icons derived from dom tree we first took a 100k uniform sample of popular publicly available urls on the web and collect bounding boxes of interactable regions of the webpage from the dom tree of each url some examples of the webpage and the interactable regions are shown in 2 apart from interactable region detection we also have ocr module to extract bounding boxes of texts then we merge the bounding boxes from ocr detection module and icon detection module while removing the boxes that have high overlap we use over 90 as a threshold for every bounding box we label it with a unique id next to it using a simple algorithm',
 'been observed by several other works. to address this issue, we incorporate the local semantics of functionality into the prompt, i. e. for each icons detected by the interactable region detection model, we use a finetuned model to generate description of functionality to the icons, and for each text boxes, we use the detected texts and its label. \nno public model is specifically trained for up - to - date ui icon description, and is suitable for our purpose to provide fast and accurate local semantics for the ui screenshot. a dataset of 7k icon - description pairs is curated using gpt - 4o, and finetune a blip - v2 model on this dataset. \nexamples of parsed screenshot image and local semantics by omniparser are shown in figure 4, which demonstrates its reliability on common app icons. \nexperiments demonstrate the effectiveness of omniparser on several benchmarks, including results from experiments with current gpt - 4v models and set of mark prompting.',
 "reliable in its description to common app icons. We show that incorporating semantics of local bounding boxes is helpful with UI screenshot visual prompt. Examples can be seen in figure 4.\n\nWe conduct experiments on several benchmarks to demonstrate the effectiveness of omniparser. Our first experiment shows current gpt-4v model with set of mark prompting [yzl + 23] is prone to incorrectly assigning label id to referred bounding boxes.\n\nWe evaluate on SEEClick benchmark and Mind2Web to further showcase omniparser's performance with local semantics, improving previous models' results on real user tasks.",
 'Each task refers to one of the detected bounding boxes, e.g., ‘click on’, ‘settings’. During evaluation, GPT-4v is prompted to predict the bounding box ID associated with it. Detailed prompts are specified in Appendix.\n\nTasks are sampled from the Screenshots [csc + 24] benchmark, labeled with a set of marks using Omniparser. These tasks are divided into 3 sub-categories by difficulty: easy (&lt;10 boxes), medium (10-40 boxes), and hard (&gt;40 boxes).\n\nGPT-4v often mistakenly assigns a numeric ID to tables, especially when there are many boxes over the screen. Adding local semantics, including text within boxes and short descriptions of detected icons, improves its ability to correctly assign IDs from 0.705 to 0.938.\n\nWithout task descriptions, GPT-4v frequently fails to link required icons to ground truth icons in screenshots, leading to hallucinations. With fine-grain local semantics included in the prompt, it becomes easier for GPT-4v to find correct icon IDs for referred icons.\n\nComparison of GPT-4v with and without local semantics: \n\n| Difficulty | Without Local Semantics | With Local Semantics |\n| --- | --- | --- |\n| Easy    | 0.705                   | 0.938               |\n| Medium   | 0.913                  | 0.949               |\n| Hard     | 0.692                  | 0.949               |\n| Overall | 0.920                  | 0.939               |\n\nTable 1: Comparison of GPT-4v with and without Local Semantics\n Screenshots:',
 "it makes it much easier for gpt-4v to find the correct icon id for the referred icon.\n\ngpt-4v w/o local semantics : 0.705\ngpt-4v w. local semantics (eas)y medium hard overall : 0.938 \ngpt-4v w. easy medium hard overall : 1.00\n0.692\n0.949\n0.620\n0.900\n\ntable 1: comparison of gpt - 4v with and without local semantics.\n \nGPT-4V without local semantics: significantly improves the baseline performance across various platforms.\n\nFigure 3: examples from the seeassign evaluation:\n\n Fine-grain local semantics improves GPT-4V's ability to assign correct labels to referred icons by a large margin. Fine-tuned models on GUI dataset, including Seeclick, CogAgent, and Fuyu, outperform GPT-4V by a significant margin.\n\nIncorporating local semantics enhances overall performance, corroborating findings in section 4.1, which suggest adding OCR text and descriptions of icon bounding boxes aids GPT-4V's ability to accurately identify correct elements to operate on.",
 'and fuyu by a large margin. we also note that incorporating the local semantics further improves the overall performance. this corroborates with the finds in section 4. 1 that incorporating local semantics of the ui screenshot improves gpt - 4v to accurately identify the correct element to operate on. furthermore, our findings indicate that interactable region detection model we finetuned improves overall accuracy. compared to using raw grounding dino model. this underscores the importance of accurately detecting interactable elements for the success of ui tasks. overall, the results demonstrate that ui screen understanding capability of gpt - 4v is significantly underestimated and can be greatly enhanced with more accurate interactable elements detection and incorporation of functional local semantics. methods fuyu cogagent seeclick minigpt - v2 qwen - vl gpt - 4v omniparser ( w. ls, w. gd ) methods fuyu cogagent seeclick minigpt - v2 qwen - vl gpt - 4v results for mobile icon / widget 1. 3 % 24. 0 % 52. 0 % 6. 6 % and results for text 33. 7 % 73. 5',
 '0 % 67. 0 % 78. 0 % 8. 4 % 9. 5 % 22. 6 % 92. 7 % 94. 8 % 93. 9 % \nmobile icon / widget 1. 3 % 24. 0 % 52. 0 % 6. 6 % 4. 8 % 24. 5 % 49. 4 % 53. 7 % 57. 0 % \ntext 33. 0 % 74. 2 % 72. 2 % 6. 2 % 5. 7 % 20. 2 % 64. 9 % 89. 3 % 91. 3 % \ndesktop icon / widget 3. 6 % 20. 0 % 30. 0 % 2. 9 % 5. 0 % 11. 8 % 26. 3 % 44. 9 % 63. 6 % \ntext 33. 9 % 70. 4 % 55. 7 % 6. 5 % 3. 5 % 9. 2 % 77. 3 % 83. 0 % web icon / widget \n\n**Technical Details Removed:**\n- All mobile and desktop icon percentages\n- Raw text without relevant numbers (33.0, etc.)\n- Raw table data \nRemoved unnecessary details:\n- \'ls\' \'gd\' \'id\'\n- The full mention of the "screenspot benchmark"',
 'we evaluate on a benchmark test set for web navigation with three task categories: cross-domain, cross-website, and cross-tasks. \nthe test set consists of approximately 877 tasks: \n867 from the cleaned mind2web tests set (cross-domain)\n167 from the cross-website category\n242 from the cross-tasks category.',
 'cogagent and qwen - vl are not finetuned on the mind2web training set. more detailed information about model settings can be found in appendix7.  \nwe report numbers from mind2web paper [dgz + 23] and seeact [zgk + 24] paper in this section, all of the approaches use the html elements selected by a finetuned element proposal model on mind2web training set which produces top 50 relevant elements on the html page based on the user task.  \ngpt - 4v + som and gpt - 4v + textual choices corresponds to the seeact with image annotation, and textual choices grounding methods respectively.  \nin gpt - 4v + som, the set of mark (som) boxes are selected from the element proposal model, and are labeled with the ground truth location extracted from html.  \nin contrast, gpt - 4v + textual uses dom information of the selected relevant elements directly in the text prompt, rather than overlaying bounding boxes on top of screenshot.  \nthe better performance of textual choice corroborates with the experiment results in 4.1.  \nin the last section, we report numbers from omniparser.  \nwe observe gpt - 4v augmented with local semantics of icon functionality and the finetuned interactable region detection model (w. ls + id) performs better than the model with raw grounding dino model (w. ls + gd) in all of the categories.  \nfurther, without using parsed html information, omniparser is able to outperform gpt - 4’s performance that uses html in every sub-category by a large margin, suggesting the substantial benefit of the screen',
 'Augmented with local semantics of icon functionality and the finetuned interactable region detection model performs better than the model with raw grounding dino model in all categories further without using parsed html information omniparser is able to outperform gpt-4’s performance that uses html in every sub-category by a large margin suggesting substantial benefit of screen parsing results provided by omniparser additionally omniparser outperforms the gpt-4v + som by a large margin compared to gpt-4v + textual choices omniparser significantly outperforms in cross-website and cross-domain category by 4.1 and 5.2 respectively while underperforming slightly in cross-task category which indicates that omniparser provides higher quality information compared ground truth element information from dom and top-k relevant element proposal used by gpt-4v + textual choices set-up',
 'ick and qwen - vl. methods input types cross - website cross - domain cross - task step sr cogagent 17. \n6 qwen - vl 12. \n0 seeclick 25. \n5 mindact ( gen ) 11. \n9 39. \n6 mindact gpt - 3. \n5 - turbo 18. \n6 gpt - 4 36. \n2 gpt - 4v + som 20. \n3 gpt - 4v + textual choice 40. \n2 omniparser ( w. ls + gd ) 38. \n7 omniparser ( w. ls + id ) 39. \n4 table 3 : comparison of different methods across various categories on mind2web benchmark \n\nhtml free [UNK] [UNK] [UNK] × × × × × × [UNK] [UNK] image ele. acc op. f1 42.0 18.4 83.5 13.2 80.6 21.4 44.7 \n         13.9 x x\n         42.0 step sr ele. acc op. f1 65.2 48.8 \n         19.3 51.1',
 "Our topic appears to be a discussion on AI-powered mobile navigation and its performance in various tasks, likely related to artificial intelligence and deep learning. After processing the raw data, I'll provide you with a cleaner transcript.\n\nsr ele is not part of a full name we can use here\nacc op refers to accessibility  or operation status only leave out, add no spaces. f1 - f4 are numbers that will have meanings \nf 1 53 corresponds to an item in this reference 54   is next so I'll remove it until 80 and start at 0 \n\n80 22  \n22 14  \n14 87  \n87 28  \n28 44  \n44 7  \n7 14  \n14 66  \n66 5  \n5 42  \n\n  36.1 (next line)\n41 6   \n- - 46   skipped over some lines for fluff and just keep what is usable on next line\n42.4 skip the rest of lines until I see a different topic or item \nwe assess omniparser on the mobile navigating benchmark aitw [r [rl ] \n\nin table 4, we report the gpt - 4v baseline in [ yyz + 23 ] paper   omitted text we dont need",
 "in table 4, we report the gpt-4v baseline in [yyz + 23] paper, which corresponds to the best performing setup (gpt-4v zs + history) that uses ui elements detected by iconnet through set-of-marks prompting for each screenshot at every step of the evaluation. the detected UI elements consist of either ocr-detected text or an icon class label. additionally, action history is also incorporated at each step's prompt as well.\n\nwe used the exact same prompt format in [yyz + 23] except the results from the iconnet model is replaced with the output of the finetuned interactable region detection (id) model. interestingly, we found that the id model can generalize well to mobile screens by replacing iconnet with the interactable region detection (id) model we finetuned on collected webpages and incorporating local semantics of icon functionality.\n\nomniparser 7 delivers significantly improved performance across most sub-categories and a 4.7% increase in the overall score compared to the best performing gpt-4v + history baseline.",
 'repeated icons / texts from analysis of the gpt - 4v ’ s response log, we found that gpt - 4v often fails to make the correct prediction when the results of the omniparser contains multiple repeated icons / texts. a potential solution is to add finer grain descriptions to the repeated elements in the ui screenshot, so that the gpt - 4v is aware of the existence of repeated elements and take it into account when predicting next action. corase prediction of bounding boxes one common failure case of omniparser is that it fails to detect the bounding boxes with correct granularity',
 'elements and take it into account when predicting next action. corase prediction of bounding boxes one common failure case of omniparser is that it fails to detect the bounding boxes with correct granularity. in figure 7 ( right ), the task is to click on the text ’ more ’. the ocr module of omniparser detects text bounding box 8 which encompass the desired text. but since it uses center of the box as predicted click point, it falls outside of the ground truth bounding box. this issue due to the fact that the ocr module we use does not have a notion of which text region are hyperlink and clickable. hence we plan to train a model that combines ocr and interactable region detection into one module so that it can better detect the clickable text / hyperlinks. icon misinterpretation found in some cases icon with similar shape can have different meanings depending on ui screenshot. for example, in figure 8, task is to find button related to ’ more information ’, where ground truth is to click three dots icon in upper right part of screenshot. omniparser successfully detects all relevant bounding boxes but icon description model interpret it as " a loading or buffering indicator ". this issue due to fact that icon description model only able see each icon cropped from image while not able see full picture during both training and inference. potential fix is train an icon description model aware of full context of image, 6 conclusion in report propose omniparser general vision approach parse ui screenshots',
 'description model is only able to see each icon cropped from image, while not able to see the whole picture during both training and inference. a potential fix to this is to train an icon description model that is aware of the full context of the image. \n\nwe propose omniparser, a general vision approach that parse ui screenshots into structured elements.\n\nom-parser encompasses two finetuned models : \nan icon detection model and a functional description models. \nto train them, we curated an interactable region detection dataset using popular webpages, \nand an icon functional description dataset.\n\nwith the parsed results, \nthe performance of gpt - 4v is greatly improved on screenspot benchmarks.  \n\nit achieves better performance compared to gpt - 4v agent that uses html extracted information on mind2web, and outperforms gpt - 4v augmented with specialized android icon detection model on aitw benchmark.\n\nom-parser can serve as a general tool \nthat has the capability to parse user screen across both pc and mobile platforms without any dependency on extra information such as html \n\nand view hierarchy in android.',
 "rosset and authors of clueweb22 for providing the seed urls are used to collect data finetune the interactable region detection model. using adapting autogen's multimodal websurfer code extracting interatable elements in dom. thanks adam fourney and dillon dupont provided processed version of mind2web benchmark. \n references \nrohan bavishi erich elsen curtis hawthorne maxwell nye augustus odena arushi somani and others introducing our multimodal models\nchongyang bai xiaoxue zang ying xu srinivas sunkara abhinav rastogi jindong chen blaise aguera y arcas uibert: learning generic multimodal representations for ui understanding \nkanzhi cheng qiushi sun yougang chu fangzhi xu yantao li jianbing zhang zhiyong wu\nxiang deng yu gu boyuan zheng shijie chen samuel stevens boshi wang huan sun yu su \nmind2web: towards a generalist agent for the web biplab deka zifeng huang chad franzen joshua hibschman daniel afergan yang li jeffrey nichols ranjitha kumar rico: a mobile app dataset for building data-driven design applications",
 ' Towards a generalist agent for the web, 2023. Association for Computing Machinery.[fln + 24] \nHiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur.\n"Multimodal Web Navigation with Instruction - Finetuned Foundation Models", 2024.[gfh + 24] \nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust.\nA real-world webagent with planning, long context understanding, and program synthesis, 2024.[hsz + 21] \nZecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby Lee, Jindong Chen, and Blaise Aguera y Arcas.\nActionBert : leveraging user actions for semantic understanding of user interfaces, 2021.[hwl + 23] \nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, and Yuxiao',
 'van wichers et al, gabriel schubiner et al,\nactionbert : leveraging user actions for semantic understanding of user interfaces,\n2021.\n Wenyi hong et al,\ncowgagent and mllm- agent: leveraging multimodal language inputs\nfor graph neural networks.\n2023.\n\njing yu koh et al,\nvisualwebarena: evaluating multimodal agents on realistic visual web tasks.\narxiv preprint arxiv : 2401. \nevan zheran liu, kelvin guu, panupong pasupat, tianlin shi, and percy liang\nreinforcement learning on web interfaces using workflow-guided exploration \nICLR, 2018.',
 'yang li, jiacong he, xin zhou, yuan zhang, and jason baldridge. mapped natural language instructions to mobile ui action sequences.\n\nyang li, gang li, luheng he, jingjie zheng, hong li, and zhiwei guan. widget captioning: generating natural language description for mobile user interface elements.\n\njunnan li, dongxu li, silvio savarese, and steven hoi. blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models.\n\narnold overwijk, chenyan xiong, xiao liu, cameron vandenberg, and jamie callan. clueweb22: 10 billion web documents with visual and semantic information.\n\nchristopher rawles, alice li, daniel rodriguez, oriana riva, and timothy lillicrap. android in the wild: a large-scale dataset for android device control.\n\npeter shaw, mandar joshi, james cohan, jonathan berant, panupong pasupat, hexiang hu, urvashi khandelwal, kenton lee, kristina toutanova, and maria wang. from pixels to ui actions: learning to follow instructions via graphical user interfaces.',
 'From pixels to ui actions: learning to follow instructions via graphical user interfaces, 2023. \n authors: mandar joshi, james cohan, jonathan berant, panupong pasupat, hexiang hu, urvashi khandelwal, kenton lee, and kristina toutanova \n\nTowards better semantic understanding of mobile interfaces.\n authors: srinivas sunkara, maria wang, lijuan liu, gilles baechler, yu chung hsiao, jindong chen, abhanshu sharma, and james stout \n\n Automatic mobile ui summarization with multimodal learning, 2021.  \n \n Mobile-device operations assistant using multi-agent collaboration, 2024\n authors: junyang wang, haiyang xu, jiabo ye, ming yan, weizhou shen, fei huang, jitao sang',
 'autonomous multi-modal mobile device agent with visual perception for smartphone navigation and other tasks\nresearch aims to improve human-machine interaction in real environments using multimodal models like gpt-4v \n\nvarious studies on groundhinging techniques for multimodal models , \nincluding ferret-ui and set-of-mark prompting, which showed better visual grounding results \n researchers from osworld and gpt-4v projects contribute to the field',
 'floris weers, amanda swearngin, jeffrey nichols, yinfei yang, and zhe gan. ferret - ui : grounded mobile ui understanding with multimodal llms, 2024. [ zgk + 24 ] boyuan zheng, boyu gou, jihyung kil, huan sun, and yu su. gpt - 4v ( ision ) is a generalist web agent, if grounded, 2024. [ zxz + 24 ]\n\nteam : shuyan zhou, frank f xu, hao zhu, xuhui zhou, robert lo, abishek sridhar, xi - anyi cheng, yonatan bisk, daniel fried, uri alon, et al.\n\nwebarena : a realistic web environment for building autonomous agents. iclr, 2024.\n\nwe collect icon description pairs to finetune model on app icons. dataset: interactable icon detection model outputs bounding boxes from screenspot dataset. blurts - gt says object in box is icon or not and describes functionality.',
 'whether the object presented in the parsed bounding box is an app icon. if gpt - 4o decides the image is an icon, it outputs one- sentence description of the icon about its functionality. \n\nwe collected 7185 icon-description pairs for finetuning. we finetune blip - 2 model for 1 epoch on the generated dataset with constant learning rate of 1e−5, no weight decay and adam optimizer. \nfigure 4 : example comparisons of icon description \nmodel using blip - 2 ( left ) and its finetuned version ( right ). \nthe original blip - 2 model focus on describing shapes and colors of app icons.\n\nwe train a yolov8 model on the interactable icon region detection dataset. \nwe collect in total of 66990 samples where we split 95 % ( 63641 ) for training, and 5 % ( 3349 ) for validation.\nwe train for 20 epochs with batch size of 256',
 'we train for 20 epochs with batch size of 256, learning rate of 1e-3, and the adam optimizer on 4 gpus. \n\nthe task involves selecting a specific bounding box in a given ui screenshot with labeled id overlayed on top.\n\n\ngpt - 4v without local semantics : \n    the task is { task }.\nwhich icon box label you should operate on? \n\ngive a brief analysis, then put your answer in the format of \\ n ‘ ‘ ‘ box with label id : [ xx ] ‘ ‘ ‘',
 'seeclick, qwen - vl seeclick is a finetuned version of qwen - vl on the mind2web training set and we report both of their numbers in their paper. cogagent cogagent number is taken from the seeact paper [ zgk + 24 ], where they report cogagent - chat - hf checkpoint that is not fine - tuned on mind2web for experiments. baseline models are gpt - 3. 5 - turbo, gpt - 4 and their numbers are taken from the mind2web [ dgz + 23 ] paper. a variant of gpt - 4 used with som boxes corresponds to the image annotation grounding method in seeact paper, using selected html elements to make action prediction. another variant uses these elements in a multi- choice question format as input. some results are presented in figures 6 and 7, showing how this model benefits from icon descriptions in making better action predictions.',
 'action prediction fails to identify correct alarm time due to lack of contextual understanding of webpage content \nfailure in recognizing text \'more\' inside bounding box due to OCR issues\nmodel incorrectly predicts click point based on bounding box center instead of actual ground truth region, leading to task completion failure \nmodel misinterprets icon with functional description "a loading or buffering indicator"  as "three dots icon in upper right part of page"',
 'Abstract Large Language Models (LLMs) have revolutionized natural language processing; their application to speech-based tasks remains challenging due to integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text, utilizing a tokenized early-fusion approach. Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalties, enabling joint reasoning and generation across modalities without the need for separate adapters. The paper presents a comprehensive training methodology with a curated instruction dataset, allowing smaller research teams to contribute effectively to open-source speech-language models.',
 'cascaded systems are often used in voice assistants such as \nautomatic speech recognition for transcribing the request to text, natural language understandin \nconverting this query into a structured format, used to generate a text answer through natural language generation \n\nmost contemporary models have shown improvement but still struggle with conversational interactions and latency issues\n\na new approach called ichigo aims to improve these systems significantly by reducing latency',
 'systems add up to several seconds of delay. this contrasts with natural conversations, where responses typically come within milliseconds. second, complexity deployment in edge device ( model compatible with conventional method and not cascaded system ). recent models that handle multiple types of data have become popular, but they still process different data types separately. \n\nwe present ichigo, a mixed - modal model capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and speech content. this approach allows for complete modeling of documents with multiple data types, expanding on tasks like understanding speech and text - only language models. our approach utilizes current strong open - source llms and extends their capability to speech through continual pre - training. \n\nichigo is designed to be mixed - modal from the start, employing a uniform architecture in an end - to - end fashion on an interleaved mixture of modalities : speech and text. it achieves the goal of introducing a new modality to the model without adding extra components.',
 'uniform architecture in an end-to-end fashion on an interleaved mixture of modalities: speech and text. By quantizing speech into discrete tokens, allowing us to use a decoder-only transformer architecture for both speech and text tokens, without adding a speech encoder and a speech adaptor. This approach projects different data types into a shared representational space from the start, allows for smooth reasoning and generation across modalities.\n\nIt represents a significant advancement over traditional cascaded systems and even recent multimodal models that treat modalities separately.\n\nWe present Ichigo, an tokenized early-fusion multimodal model capable of reasoning over and generating interleaved speech-text documents. \n\nOur key contributions include:\n- Training techniques for tokenized early-fusion multimodal models without starting from scratch\n- A recovering capability training method to stabilize cross-modality training\n\nThis work also introduces the creation and release of Instruction Speech, a large-scale English speech-text cross-modal instruction-following dataset featuring multi-turn interactions, reasoning tasks, and refusal scenarios.',
 'following dataset featuring multi- turn interactions, reasoning tasks, and refusal scenarios we also provide the training and inference code to facilitate further research in this area  we present a unified framework leveraging token-based representations for both speech and textual modalities \n\nby quantizing continuous speech into discrete tokens similar to words in text we can utilize the same transformer architecture to sequences of both speech and text tokens eliminating the need for separate speech/text encoders or domain-specific decoders by projecting all modalities into a shared representational space from the outset this method facilitates cross-modal reasoning and generation \n\n tokenization process for speech we employ whispervq a component of whisperspeech utilizes a codebook of 512 tokens with a codebook dimension of 64 based on the whisper medium model whispervq processes speech input resampled to 16 khz achieving a frame rate of 25 hz initially the audio is converted to a log-mel spectrogram and processed by a whisper encoder producing continuous embeddings',
 'vector quantization step maps them to a finite codebook producing a sequence of discrete tokens representing the audio content figure ichigo represents speech and text modalities as discrete tokens using a uniform transformer-based architecture it uses whispervq to quantize speech into discrete tokens in the same manner with original text modality 2 expanding language model to incorporate multimodal discrete representations into pre-trained llms we expand vocabulary with new modality-specific tokens this expansion necessitates extending embeddings and prediction layer with newly incorporated parameters initialized randomly combined tokens from all modalities form new vocabulary where each modality is trained within language model to align in shared representational space approach allows us to compress multimodal data into discrete token sequences which language model can then train using next token prediction loss consequently this enables llm to unify tasks such as understanding reasoning and generation in autoregressive manner',
 'the embedding matrix to accommodate the new tokens, the rest of the language model remains unaltered. we introduce two new special tokens to delimit audio file inputs: &lt; | sound _ start | &gt; and &lt; | sound _ end | &gt;. initially, we attempted to use the default new token initialization from the huggingface codebase, but this approach resulted in slow convergence of the loss curve. switching to initial-izing new token embeddings by averaging all embeddings of the current vocabulary improved speed of convergence and enhanced training stability.\n\nwe have curated a comprehensive dataset comprising two main components: the pre-training dataset and the instruction speech dataset. 4 datasets are used to enable ichigo to process and understand audio signals, including 3 public automatic speech recognition (asr) datasets across eight languages. these datasets span english, german, dutch, spanish, french, italian, portuguese, and polish.',
 'public automatic speech recognition (asr) datasets spanning eight languages: english, german, dutch, spanish, french, italian, portuguese, and polish. we obtained english from the mls english 10k dataset [pratap et al., 2020] and other languages from the multilingual librispeech dataset [pratap et al., 2020]. the majority of this audio content origins from audiobooks available on librivox and openslr. subsequently, we employed whispervq to convert these audio files into discrete sound tokens.\n\nwe also utilized a blend of high-quality open-source data available on huggingface [xu et al., 2024, huggingfacetb, 2024, pjmixers, 2024, euclaise, 2024, intel, 2024, routellm, 2024, nomic ai, 2024, microsoft, 2024, for ai, 2024, open-orca, 2024, magpie-align, 2024].\nthese datasets span a wide array of topics, thereby diversifying the input data for our model.',
 "we implemented a two-step filtering process to ensure data quality and relevance. we applied the fasttext model as a language identifier at the document level, retaining only english documents with a confidence threshold of (0.9). \ndeduplication : we removed duplicate entries to prevent overfitting and ensure a diverse training set. \ndespite the tokenizer's capacity to handle eight languages, we opted to focus primarily on english for this training iteration.\n\nour filtering pipeline consisted of length filtering and quality filtering. \nlength filtering : we filtered out text instructions longer than 64 tokens.\nquality filtering : we eliminated samples that would be challenging to pronounce as speech, such as urls, mathematical symbols, and code snippets.\n\nthe dataset is built upon the text instruction dataset and underwent additional steps to create a usable audio input dataset.",
 'assistants quality filtering eliminated samples challenging pronounce speech such urls mathematical symbols code snippets synthetic data generation pipeline two stage process convert text based instruction dataset discrete sound tokens suitable audio input utilized whisperspeech text speech tts model generate audio files instruction dataset questions employed whispervq transform audio files discrete sound tokens figure illustrates overview synthetic data generation pipeline multi stage filtering conversion process multiple open source instruction datasets undergoes filtering process results samples 2000 hours tokenized speech audio paired text responses approach created multimodal dataset real world interactions audio based ai assistants enhancing model ability process respond spoken instructions',
 'world interactions with audio-based AI assistants\nto address the issue of catastrophic forgetting on transcribe tasks, we switched from using specialized tokens to pure instructions\nsix instruction sentences were incorporated into the model, which improved its ability to map sound token patterns to corresponding text while minimizing the reduction in its text capabilities\ntable 5 shows examples of these instructions \nnoise audio data during training, the initial approach of creating synthetic random noises was challenging to scale but using patterns and randomized sequence generation proved successful\ninability to generalize, as meaningful speech follows predictable patterns\nsynthetic answers from the qwen2. 5 -72b model were used to generate diverse inaudible input data\nteaching the model to distinguish between audible and inaudible inputs effectively was achieved by exposing it to a wide range of chaotic arrangements, with an average speech input of about 50 sound tokens',
 "a speech processing system was developed using the 72b model to generate diverse synthetic answers for inaudible inputs, with an average speech input of 50 sound tokens representing only a tiny fraction of possible arrangements. the system's performance was improved by exposing it to chaotic arrangements and performing sequence length distribution matching between audible and inaudible data.\n\nthe training process involved multiple stages, utilizing software infrastructure to optimize different aspects of the model's performance. \n\npre-training methodology focused on enabling language learning, introducing new tokens into speech representation for basic concepts development. adamw fused optimizer with weight decay, momentum decay, and squared gradient decay was used to facilitate robust training, with internal cluster comprising 10 nvidia a6000 GPUs",
 'hutter, 2019, paszke et al., 2019] with a weight decay of 0.01, momentum decay of 0.9, and squared gradient decay of 0.95. \nour models were trained on our internal cluster comprising 10 nvidia a6000 - 48gb gpus, employing fsdp 2 [torchtune maintainers and contributors, 2024] and activation checkpointing.\nwe tested alternative optimizers such as adam - mini and lion but these attempts resulted in unstable training.\nprompted us to return to adamw fused. \nour three-stage training phases consisted of a pre-training stage with maximized global batch size for more general learning, instruction fine-tuning, and enhancement fine-tuning with reduced learning rate and increased context length to stabilize the training loss curve and adapt to user requests.',
 "ichigo's three-stage process involved parameter pre-training, instruction, ft enhancement, and weight decay with various optimizers like AdamW, fused BF16, and a global batch size of 8064. During training, hardware constraints limited the optimal settings to 45 hours of training time within 10x A6000 batch. The learning rate was warmup-enabled along with max length features at 10X A6000 45h 80240, utilized for optimization at 7 x 10^-5 with precision set as 8064. In the second part, post-traning phase had two distinct stages of refinements including instruction fine-tuning and enhancement fine-tuning. Fine Tuning focused on enhancing question-answering ability while the latter expanded proficiency in multi-turn conversations.",
 "speech understanding, transcription capabilities, and general language skills. Figure 3 shows the data distribution proportion for this training stage.\n\na. Distribution of data types in the instruction fine-tuning dataset.\nthe goal was to enhance speech comprehension while maintaining robust general language abilities.\n\nb. Distribution of data samples used in the enhancement fine-tuning stage.\nthis specific distribution improves robustness in handling multi-turn conversations and inaudible inputs.\n\n4. \n2. 2\nenhancement fine-tuning involved data augmentation to simulate real-world user interactions, thereby improving robustness in various scenarios.\n\nwe focused on two key areas : multi-turn conversations with speech input and appropriate responses to inaudible inputs.\n\nthese enhancements aimed to create more fluid dialogues and improve the model's interactive capabilities.\n\nto achieve this, we fine-tuned the model using a dataset of 158,000 samples.\n\nthe dataset for enhancing refusal capabilities was carefully balanced, comprising only 0. 5 % of the total multi-turn data.\n\nthis proportion was determined through experimentation, as we found that a higher percentage led to an increased tendency for the model to refuse inputs.\n\nFigure 3 illustrates the data distribution ratios.\n\n9 \nresults\nwe present the experimental outcomes for ichigo.\n\nwe evaluate its performance across multiple dimensions, including question-augmenting capabilities, response latency, degradation recovery, and practical cases.\n\nour analysis provides a comprehensive assessment of ichigo's.",
 "Here's the processed text:\n\nour analysis provides a comprehensive assessment of ichigo 's capabilities in comparison to other well-known speech language models, using specific dimensions such as question-answering capabilities and response latency. we first assess ichigo 's sqa ability in comparison to other well-known speech language models. table 2 presents the results on two sqa scores from audiobench, with a robust llama-3 70b model serving as the judge for evaluation.\n\ntable 2:\ncomparative results of ichigo against representative speech language models\nmodel openhermes - audio alpaca - audio whisper + llama - 3 8b\nscore: 63.0\nichigo instruct v0. 3 ( phase 3 )\nscore: 22.4 67.8 12.4\n\nnote : higher scores indicate better performance. \nwe encountered an error in the judge model's output affecting the'rating score'. \nto address this issue, we implemented a backfilling procedure for missing ratings, ensuring more accurate representation of model performance.\n\nour results demonstrate that ichigo outperforms existing open-source speech language models, particularly those utilizing ntef approaches. compared to other end-to-end models,i have no information on any speaker mentioned (ichigo)",
 "provided ratings in the middle of its responses rather than at the end as expected, resulting in lowered scores. to address this issue, we implemented a backfilling procedure for missing ratings, ensuring a more accurate representation of model performance. our results demonstrate that ichigo outperforms existing open - source speech language models, particularly those utilizing non - tokenized early fusion (ntef) approaches [wadekar et al., 2024]. compared to other end - to - end models, ichigo’s performance is particularly impressive. it outperforms qwen2-audio by 23 points on openhermes-audio and 15.2 points on alpaca-audio.\n\nthis substantial improvement underscores the effectiveness of ichigo's architecture and training approach in capturing the nuances of speech-language interactions. on the openhermes-audio benchmark, ichigo achieves a score of 67.8, surpassing even the cascaded system (63.0) with an additional 4.8 points score is especially noteworthy given that cascaded systems often benefit from specialized components for transcription and language modeling. for the alpaca-audio benchmark, ichigo maintains its strong performance with a score of 67.2, while the cascaded system scores 70.8\n\nthis demonstrates ichigo's ability to effectively integrate speech understanding and language generation in a single model without the need for separate phases \n\n5 \n\nlatency to first token, we conducted a comparative analysis of its latency \n\nconducted",
 "To achieve state-of-the-art performance, Ichigo effectively integrates speech understanding and language generation in a single model. \n\nIchigo's tokenized early fusion architecture reduces latency by 4 times compared to current speech models and cascaded systems. On average, Ichgo achieves 32ms latency to first token.",
 "ichigo outperforms other speech language models in terms of latency, achieving a 110 ms latency to first response, which is nearly 3 times faster than qwen2 - audio (317. 45 ± 8. 30 ms). comparison with the cascaded whisper + llama - 3 8b system results in a latency reduction of about 4 times. vram efficiency also favors ichigo , maintaining a lower vram footprint at 19 gb compared to other systems mentioned, showcasing its exceptional efficiency while balancing performance and resource utilization. \n\ndegradation recovery has been evaluated across three llm benchmarks for general knowledge, reasoning, and mathematics, providing insights into the model's ability to retain capabilities of its original language model and adapt to new modality learning tasks.",
 'ichigo instruct v0. 3 ( phase 3 ) demonstrates remarkable recovery across all benchmarks, including mmlu which achieves a score of 63. 79 significantly closer to the original llama3 8b instruct ’ s 69. 4 compared to the 50. 27 scored by v0. 2 consistent improvement we observe an upward trend in performance from v0. 2 to v0. 3 ( phase 3 ) indicating the effectiveness of our iterative training approach this indicates with extended training time and more computational resources we can achieve higher performance key improvements and challenges include a substantial reduction in performance degradation from 29. 3 % to 8. 4 %, primarily attributed to our refined training strategy which incorporates mixed proportions of text-only and sound token data',
 "Our initial pre-training phase focused on sound tokens with next token prediction, resulting in a significant degradation in the model's performance on text-based tasks, particularly mathematics and coding. This highlights the challenges in maintaining cross-modal capabilities during specialized training. \n\n Model MMLU GPQA (0-shot) (5-shots) GSA-8k (COT) (8-shots) \nAvg: Llama3 8B Instruct - 69.4 - 30.4 - 84.5 - 61.43 \nIchigo Base V0.2 - 47.66 - 50.27 - 42.11 - 63.08 - 63.79 \nIchigo Instruct V0.3 (Phase 2) - 28.13 - 26.56 - 28.57 - 28.35 - 29.69 \nCross-Modality: Ichigo was tested in real-world conversational scenarios, demonstrating its ability to follow system prompts and maintain coherent multi-turn dialogues across text and speech modalities.",
 "the current model's performance with sound tokens has led to significant degradation in mathematical and coding tasks, primarily due to difficulties in handling next token predictions.\n\nwhile this limitation affects instruction following, practical experiments were conducted to assess the model's ability to follow system prompts across different modalities. ichigo was used to evaluate the model's capacity for versatile ai assistants.\n\nthe results demonstrate ichigo's robust ability to follow text-based system prompted conversations while engaging in speech-based talk with users and maintain coherent multi-turn dialogues, despite input format differences.",
 'ichigo demonstrated appropriate behavior by refusing to provide random answers. instead, as illustrated in figure 6, the model politely requested the user to repeat their query, ensuring accurate responses. \n\nthe model follows text-based system prompts during speech-based conversations with users and provides relevant responses.\n\nfigure 6.\na. transcribed dialogue examples using ichigo. \nthe user-turn is audio input. the examples illustrate zero-shot multi-turn capabilities.\nb. ichigo requests clarification from the user when unable to understand the question clearly.\n\nthese experiments complement quantitative findings, demonstrating ichigo’s practical capabilities in real-world scenarios and handling unclear inputs with cross-modal instruction following and multi-turn dialogues.',
 'al., 2023, lakhotia et al., 2021. subsequent advancements led to the joint training of speech tokens and text, resulting in decoder-only models such as those using vall-e [wang et al., 2023a, chen et al., 2024] for speech recognition, translation, and synthesis. however, these early models were not built upon large language models (llms). researchers have explored various approaches to building speech-language models based on llm architectures. \n6.2 llm-based audio-language models recent research has focused on two primary approaches to integrating speech and audio capabilities with llms: non-tokenized early fusion and tokenized early fusion. 6.2.1 non-tokenized early fusion involves adding a pre-trained speech encoder before the llm and fine-tuning the entire model for speech understanding capabilities. notable examples of this approach include llama-omni [fang et al., 2024] and llasm [shu et al., 2023], which excel in tasks such as speech recognition and speech-to-text, with improvements over traditional models in translation and synthesis.',
 'notable examples of this approach are llama - omni and llasm, which extend llm capabilities to audio modality by integrating a pre-trained speech encoder, a speech adaptor, and a streaming speech decoder. salmonn takes this approach further in capturing both speech and non-speech audio information using dual auditory encoders. qwen2 audio introduces a new architecture combining an audio encoder with a large language model, training to maximize next text token probability conditioned on audio representations.\n\n6. 2. 2 tokenized early fusion involves tokenizing multimodal inputs using either a common tokenizer or modality-specific tokenizers, enabling multiple approaches such as chameleon which represents images and text as discrete tokens within a unified transformer trained from scratch with modified transformer architecture. audiol palm and vox tlm utilize pre-trained language models, extending their capabilities in audio processing.',
 'here is the processed text\n\nformer model approaches include chameleon which represents images and text as a series of discrete tokens within a unified transformer recently developed models for translation tasks such as audiopalm and voxtlm which utilize pre trained language models to add semantic audio tokens\n\nour approach builds on these by retaining the entire architecture of current llms while incorporating whispervq to preserve the openai whisper encoder block this allows us to generate embeddings which are then quantized to obtain semantic tokens addressing a key challenge is stabilizing loss in cross modality training',
 "real-time native multimodal foundation model designed for seamless audio-text interactions, using a 7B parameter multimodal language model that processes speech input and output concurrently to generate text tokens and audio codecs. \n\nthe proposed ichigo model handles two audio streams simultaneously, allowing real-time listening and speaking while maintaining textual thoughts flow. \n\nthe key to ichigo's success lies in its fully token-based architecture providing seamless information integration across modalities by quantizing speech into discrete tokens utilizing a strong base LLM.",
 'end speech language models in speech - based question - answering tasks, marking a significant step forward in multimodal ai. \nichigo demonstrates a real-time speech system with a latency of 110 milliseconds to first reponse.\nthis opens up new possibilities for speech systems and significantly reduces the complexity of deploying such systems in production environments.\n\nwe believe that this paper will empower smaller research teams to contribute more confidently and prolifically to the open-source community.\nby demonstrating that significant advancements can be achieved with limited resources, \nwe hope to inspire broader participation in this critical area of ai research.',
 '- context length limits modeling possibilities \n ichigo focuses on short speech input and one - turn conversations currently \n extending the model would allow for longer audio segments \n and handling complex multi- turn conversations\n https:/ / huggingface. co / datasets / baai / infinity - instruct',
 'tan et al. introduce the "vall-e" framework which utilizes neural codec language models to achieve human parity-zero shot text-to-speech synthesizers according to yunfei chu et al., who also explore qwen2, an audio technical report that builds upon vall-e concepts. in contrast, karl cobbe et al. focus on developing "training verifiers" to solve math word problems based on ar Shiviv 21014148. these findings were showcased at "whisperspeech" 2024 and are accessible up to date.',
 'nthi et al. have been pushing the boundaries of audio language models.  their work builds upon previous advancements in the field.  \n\nMoshi represents a significant step forward from existing foundation models, while Pengi takes an alternative approach to tackle various audio tasks.  The Llama 3 herd brings fresh ideas and perspectives, with the goal of achieving high fidelity in neural compression.\n\nA key area of focus has been explored in the development of seamless speech interaction capabilities for large language models, leveraging models like LLaMA- Omni.',
 'seamless speech interaction with large language models is a promising approach for few-shot language model evaluation, which enables the development of more sophisticated AI models. recent advances in this field have focused on using pre-trained large language models to improve performance on various natural language processing tasks.\n\nResearchers from the Allen Institute for AI and Hugging Face developed wildchat, a dataset containing over 1 million conversations, designed to evaluate speech generation capabilities of large language models. \n\nanother study focused on using Seed Tokenizer to enable Llama models to "see" and "draw", essentially creating more nuanced human-like interactions. the researchers aimed to address limitations in current chatbots that primarily rely on text input rather than incorporating image-based conversation.\n\nresearchers have also developed frameworks for few-shot language model evaluation, which can help create AI models specifically designed to perform better on low-resource tasks or less common domains.  this framework allows for more precise analysis and evaluation of the capabilities of these models.',
 'making llama see and draw with seed tokenizer by yuying ge et al. arxiv preprint arxiv : 2310. 01218, 2023.\n\n Alignments for multimodal models such as prism - alignment by hannahrosekirk. \n\nmeasuring massive multitask language understanding by dan hendrycks et al. arxiv preprint arxiv : 2009. 03300, 2020.\n\nInitializing new word embeddings for pretrained language models by john hewitt from nlp.stanford.edu. \n\ntraining large language models with optimal compute resources by jordan hoffmann and associates.\n\nA low-rank adaptation of large language models called lora, developed by edward j hu et al',
 'Hugging Face dataset for large language models including Lora, WavLLM, HuggingFaceTb and others in the area of generative spoken language modeling from raw audio.',
 'u Yossi Adi Adam Polyak Benjamin Bolte Tu Anh Nguyen Jade Copet Alexei Baevski Abdelrahman Mohamed et al on generative spoken language modeling from raw audio Transactions of the Association for Computational Linguistics Vol 9 No 1336-1354 2021 Ilya Loshchilov and Frank Hutter Decoupled weight decay regularization 2019 url https arxiv.org abs 171105101 Magpie - align Magpie - pro \n300k - filtered 2024 url huggingface co datasets magpie - align magpie - pro \n300k - filtered Soumi Maiti Yifan Peng Shukjae Choi Jee-Weon Jung Xuankai Chang and Shinji Watanabe VoxTLM unified decoder - only models for consolidating speech recognition synthesis and speech text continuation tasks In ICASSP 2024 - 2024 IEEE International Conference on Acoustics Speech and Signal Processing pages 13326 \n13330 IEEE 2024 Microsoft Orca \nmath - word - problems \n- 200k 2024 url huggingface co datasets microsoft \norca - math - word - problems Tu Anh Nguyen Eugene Kharitonov Jade Copet Yossi Adi Wei-Ning Hsu Ali Elkahky Paden Tomasello Robin Algayres Benoit Sagot Abdelrahman Mohamed et al Generative Spoken Dialogue Language Modeling Transactions of the Association for Computational Linguistics Vol 11 No \n250-266 2023 Nomic',
 '/ microsoft / orca - math - word - problems - 200k. tu anh nguyen, eugene kharitonov, jade copet, yossi adi, wei - ning hsu, ali elkahky, paden tomasello, robin algayres, benoit sagot, abdelrahman mohamed, et al. generative spoken dialogue language mod - eling. transactions of the association for computational linguistics, 11 : 250 – 266, 2023. nomic ai. gpt4all - j - prompt - generations, 2024. url https : / / huggingface. co / datasets / nomic - ai / gpt4all - j - prompt - generations. open - orca. oo - gpt4 - 200k, 2024. url https : / / huggingface. co / datasets / open - orca / oo - gpt4 - 200k. openai, josh achiam, steven adler, sandhini agarwal, lama ahmad, ilge akkaya, florencia leoni aleman, diogo almeida, janko altenschmidt, sam altman, shyamal anadkat, red avila, igor babuschkin, suchir balaji, valerie balcom, paul baltescu, haiming bao, mohammad bavarian, jeff belgum, irwan bello, jake berdine, gabriel bernadett - shapiro, christopher berner, lenny bogdonoff, oleg boiko, madelaine 17 boyd, anna - luisa brakman, greg brockman, tim brooks, miles brundage, kevin button, trevor cai, rosie campbell, andrew cann, brittany carey, chelsea carlson, rory carmichael, brooke chan, che chang, fotis chantzis, derek chen, sully chen, ruby chen',
 'irwan bello, jake berdine, gabriel bernadett - shapiro, christopher berner, lenny bogdonoff, oleg boiko, madelaine 17 boyd, anna - luisa brakman, greg brockman, tim brooks, miles brundage, kevin button, trevor cai, rosie campbell, andrew cann, brittany carey, chelsea carlson, rory carmichael, brooke chan, che chang, fotis chantzis, derek chen, sully chen, ruby chen, jason chen, mark chen, ben chess, chester cho, casey chu, hyung won chung, dave cummings, jeremiah currier, yunxing dai, cory decareaux, thomas degry, noah deutsch, damien deville, arka dhar, david dohan, steve dowling, sheila dunning, adrien ecoffet, atty eleti, tyna eloundou, david farhi, liam fedus',
 "Mike Heaton and Johannes Heidecke, along with Chris Hesse and Alan Hickey, are among the individuals listed who have shared a common thread in their backgrounds, often indicating involvement or association with various martial arts, sports, or other competitive pursuits. Though I can't accurately pinpoint a single defining characteristic or niche shared amongst all the names due to the diverse range of professions, associations, and interests these people have, the initial list appears to be comprised primarily of individuals involved or associated with martial artists and mixed martial artists, as well as fighters in various combat sports.",
 'Scott Mayer McKinney Christine McLeavey Paul McMullan Jake McNeil David Medina Aalok Mehta Jacob Menick Luke Metz Andrey Mishchenko Pamela Mishkin Vinnie Monaco Evan Morikawa Daniel Mossing Tong Mu Mira Murati Oleg Murk David Melo Ashvin Nair Reiichiro Nakano Rajeev Nayak Arvind Neelakantan Richard Ng Hyunwoo Noh Long OuYang Cullen O Keefe Jakub Pachocki Alexander Paino Joe Palermo Ashley Pantuliano Giambattista Parascandolo Joel Parish Emy Parparita Alexander Passos Mikhail Pavlov Andrew Peng Adam Perelman Filipe De Avila Belbute Peres Michael Pokorny Michelle Pokrass Vitchyr H Poong Tolly Powell Alethea Power Boris Power Elizabeth Proehl Raul Puri Alec Radford Jack Rae Aditya Ramesh Cameron Raymond Francis Real Kendra Rimbach Carl Ross Bob Rotsted Henri Roussez Nick Ryder Mario Saltarelli Ted Sanders Shibani Santurkar Girish Sastry Heather Schmidt David Schnurr John Schulman Daniel Selsam Kyla Sheppard Toki Sherbakov Jessica Shieh Sarah Shoker Pranav Shyam Szymbon Sidor Eric Sigler Maddie Simens Jordan Sitkin Katarina Slama Ian Sohl Ben Jam Sokolowska Yang Song Natalie Staudacher Felipe Petroski',
 'kar girish sastry heather schmidt david schnurr john schulman daniel selsam kyla sheppard toki sherbakov jessica shieh sarah shoker pranav shyam szymon sidor eric sigler maddie simens jordan sitkin katarina slama ian sohl ben jamin sokolowsky yang song natalie staudacher felipe petroski such natalie summers ilya sutskever jie tang nikolas tezak madeleine b thompson phil tillet amin tootoonchian elizabeth tseng pre ston tuggle nick turley jerry tworek juan felipe uribe andrea vallone arun vijayvergiya chelsea voss carroll wainwright justin wang alvin wang ben wang jonathan ward jason wei cj weinmann akila welihinda peter welinder jiayi weng lilian weng matt wiethoff dave willner clemens winter samuel wolrich hannah wong lauren workman sherwin wu jeff wu michael wu kai xiao tao xu sarah yoo kevin yu qiming yuan wojciech zaremba rowan zellers chong zhang marvin zhang shengjia zhao tianhao zheng juntang zhuang william zhuk and barret zoph',
 'The research paper authors include Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, Adam Paszke, etc. They are mentioned as contributors in the pre-4 technical report by GPT - 2024  and also related deep learning libraries and datasets such as Librispeech, PyTorch, PJMixers. Research areas seems to be speech processing',
 'Here is the cleaned up and processed text:\n\nthe style for the podcast episode could be high performance deep learning library \n\nresearch paper title: advances in neural inc., 2019 \nauthors: pjmixers \n\npublished in arxiv, abs / 2012. 03411, 2020',
 'colin raffel, noam shazeer, adam roberts, katherine lee, sharan narang, michael matena, yanqi zhou, wei li, and peter j liu are leading researchers in a recent study on exploring the limits of transfer learning with a unified text-to-text transformer. they publish their work in the journal of machine learning research, presenting a new benchmark for measuring the capabilities of large language models.',
 'dalia el badawy, wei han, eugene kharitonov, et al. introduced audiopalm, a large language model that can speak and listen. \n\nyu shu, siwei dong, guangyao chen, wenhao huang, ruihua zhang, daochen shi, qiqi xiang, and yemin shi published llasm: large language and speech model.',
 "PyTorch's Finetuning Library and Its Impact on Large Language Models\n================================================================================\n\nThe development of large language models has been a significant milestone in the field of natural language processing (NLP). One key aspect that has contributed to their success is the introduction of finetuning libraries like Pytorch's. These libraries enable researchers and practitioners to fine-tune pre-trained models to meet specific task requirements, resulting in improved performance.\n\nThe authors of this work focus on showcasing the capabilities of such finetuning frameworks using a variety of large language models and benchmarking them against state-of-the-art methods. The results highlight the potential benefits of leveraging pre-trained architectures in multi-modal model configurations and demonstrate how the Pytorch library facilitates seamless integration with other tools and datasets.\n\nNotable Contributions\n----------------------\n\n*   **Llama**: An open and efficient framework for foundation language models, demonstrated through its application to the Undi95 dataset.\n*   **Capybara-ShGpt**: A dataset designed specifically for benchmarking large language model efficiency, showcasing the capabilities of Capybara-ShGpt in generating coherent and accurate results.\n\nAudiobench: A Universal Benchmark for Audio-Large Language Models\n-------------------------------------------------------------------\n\nRecently, researchers have been exploring the feasibility of applying large language models to audio tasks. This includes using models as zero-shot text-to-speech (TTS) synthesizers. By leveraging pre-trained architectures like those available in the Pytorch library, it becomes possible to generate high-quality audio outputs with minimal additional data.\n\nKey Findings and Applications\n------------------------------\n\n*   **Zero-Choice TTS Models**: Large language models can serve as zero-shot text-to-speech synthesizers by using knowledge learned from large amounts of text.\n*   **Improving Audio-Bench Metrics**: Researchers can fine-tune pre-trained models to meet specific task requirements, thereby improving the metrics available for benchmarking large language models in applications such as audio-based question answering and other forms of multi-modal evaluation tasks.\n\nOverall, the evolution of Pytorch's finetuning library represents a significant advancement in the capabilities of large language models. By providing researchers with effective tools, these frameworks enable rapid testing, validation, and deployment of pre-trained models tailored to particular domains or applications.",
 'neural codec language models are zero shot text to speech synthesizers \nviolating unified codec language models for speech recognition synthesis and translation \nmagpie alignment data synthesis from scratch by prompting aligned lms with nothing \nanygpt unified multimodal llm with discrete sequence modeling \nspeechtokenizer unified speech tokenizer for speech large language models',
 'anygpt : unified multimodal llm with discrete sequence modeling. arxiv preprint arxiv : 2402. 12226, 2024. xin zhang, dong zhang, shimin li, yaqian zhou, and xipeng qiu.\n\nspeechtokenizer : unified speech tokenizer for speech large language models. arxiv preprint arxiv : 2308. 16692, 2023.\n\n**appendix**\n\na. 1 **audio speech recognition ( asr ) prompt library**\n\n* **table 5**\n transcribe the following audio clip :\n convert the spoken words to text :\n what is being said in this audio clip :\n transcribe the speech in this audio sample :\n please write down what is being said in the audio clip :\n generate a transcript from this sound file :\n recognize the speech in this audio clip :\n produce a text version of this audio recording :\n\na. 2 **ablation studies**\n\n* we conducted a series of ablation studies to investigate the impact of different training configurations on the model ’ s performance\n* table 6 summarizes the results of these experiments',
 'the goal of this study is to investigate how different training configurations impact the performance of a mult-modal language model we conducted a series of ablation studies to evaluate various training scenarios that modify some or all aspects of the original configuration our analysis involved comparing multiple scenarios that manipulate different parts such as whether to include data from speech-to-text instruction and recovered audio transcription experiments were run using both training token usage and varied compositions involving either inclusion or omission of certain components of the original dataset',
 'The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, we argue that the power of multimodal models like GPT-4 is largely underestimated due to the lack of a robust screen parsing technique capable of reliably identifying interactable icons within the user interface and understanding the semantics of various elements in a screenshot accurately associating the intended action with the corresponding region on the screen.\n\nTo fill these gaps, we introduce Omniparser, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4 to generate actions that can be accurately grounded in the corresponding regions of the interface. \n\nWe curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen, and a caption model to extract the functional semantics of the detected elements.',
 'these datasets were utilized to fine-tune specialized models \na detection model to parse interactable regions on the screen and a caption model to extract functional semantics of the detected elements \nompuraser significantly improves gpt-4v’s performance on screenspot benchmark\nand on mind2web and aitw benchmark omniparser with screenshot only input outperforms gpt-4v baselines requiring additional information outside of screenshot \n\nlarge language models have shown great success in their understanding and reasoning capabilities \nmore recent works explore the use of large vision-language models as agents to perform complex tasks on user interface\naiming to complete tedious tasks to replace human efforts \n\ndespite promising results a significant gap remains between current state-of-the-art and creating widely usable agents that can work across multiple platforms',
 'platforms. while large multimodal models like gpt - 4v have demonstrated abilities to understand basic elements of the ui , action grounding remains one of the key challenges in converting the actions predicted by llms to the actual actions on screen in terms of keyboard / mouse movement or api call . \nit has been noted that gpt - 4v is unable to produce the exact x - y coordinate of the button location, [ proposals include ] proposing a visual prompt method such as overlaying a group of bounding boxes each with unique numeric ids on to the original image , which greatly improves the robustness of the action grounding . however, current solutions using som relies on parsed html information to extract bounding boxes for actionable elements , limiting its usage to web browsing tasks . we aim to build a general approach that works on a variety of platforms and applications \nwe argue previous vision - based screen parsing techniques are not satisfactory , leading to significant underestimation of gpt - 4v model ’ s understanding capabilities\nour solution is omniparser',
 "we argue that previous pure vision-based screen parsing techniques are not satisfactory, which leads to significant underestimation of GPT-4's understanding capabilities. A reliable vision-based screen parsing method is key to improving the robustness of the agentic workflow on various operating systems and applications. We present Omniparser, a general screen parsing tool that extracts information from screenshot into structured bounding box and labels, enhancing GPT-4's performance in action prediction for a variety of user tasks.\n\nWe curate an interactable region detection dataset using bounding boxes extracted from the DOM tree of popular webpages. We propose Omniparser, a pure vision-based UI screen parsing method combining multiple finetuned models for better screen understanding and easier grounded action generation.\n\nWe evaluate our approach on Screenspot, Mind2Web, and AI2T benchmark, demonstrating a significant improvement over the original GPT-4 baseline without requiring additional input beyond screenshot.",
 'These works demonstrated effective usage of multimodal models for extracting semantics of user screen. but these models rely on additional information such as view hierarchy, or are trained for visual question answering tasks or screen summary tasks. there is a couple publicly available dataset that on ui screen understanding. most notably the rico dataset which contains more than 66k unique ui screens and its view hierarchies. later auguments rico by providing human annotations on the original 66k rico screens identifying various icons based on their shapes and semantics, and associations between selected general ui elements and their text labels. same on mobile platform pixelhelp provides a dataset that contains ui elements of screen spanning across 88 common tasks.\n\nfor the web and general os domain there are several works  such mind2web miniwob visual - webarena and os - world that provide simulated environment but does not provide dataset explicitly for general screen understanding tasks. to address this we curated an icon detection dataset using dom information from popular urls avaialbe on the web.',
 'the rapid evolution of ui design we curated an icon detection dataset using the dom information from popular urls avaialbe on the web this dataset features the up - to - date design of icons and buttons with their bounding boxes retrieved from the dom tree providing ground truth locations \n\n2 autonomous gui agents recently there has been a lot of works on designing autonomous gui agent to perform tasks in place of human users one line of work is to train an end - to - end model to directly predict the next action representative works include pixel2act ferret cogagent and fuyu in mobile domain seeact agent mindact agent \n\nthese work often leverages the dom information in web browser or the view hierarchies in mobile apps to get the ground truth position of interactable elements of the screen',
 "We focus on providing a systematic approach for getting structured elements from general user screens to build a general agent for cross-platform and cross-application tasks. A complex task can be broken down into several steps, which requires the model's ability to understand the UI screen and predict the next action. We propose Omniparser, an integrated system that extracts information from a finetuned interactable icon detection model, a finetuned icon description model, and an OCR module. This combination produces a structured representation of the UI, including screenshots with bounding boxes for potential interactable elements.",
 'sed screen and focus more on the action prediction\nomniparser integrates outputs from \n- a finetuned icon detection model\n- an icon description model\n- an OCR module\n\nThis produces a structured, DOM-like representation and a screenshot with bounding boxes for potential interactable elements.\n\nIdentification of interactable regions is key to reason about what actions should be performed given a user task. Instead of directly prompting GPT-4v to predict the xy coordinate value of the screen, we use the set-of-marks approach to overlay bounding boxes on top of the UI screenshot and ask it to generate the bounding box id to perform an action.\n\nUnlike previous works, we finetune a detection model to extract interactable icons/buttons. We curate a dataset of 67k unique screen-shot images, each labeled with bounding boxes of interactable icons derived from DOM tree. This includes \n\nSample publicly available URLs and their corresponding interactable regions',
 'we curate a dataset of interactable icon detection dataset, containing 67k unique screen-shot images, each labeled with bounding boxes of interactable icons derived from dom tree. this dataset is created by first taking a 100k uniform sample of popular publicly available urls on the web and collect bounding boxes of interactable regions of the webpage from the dom tree of each url we also have an ocr module to extract bounding boxes of texts from these texts, we merge and remove highly overlapping bounding boxes while labeling them with unique ids for every box, this id is generated using a simple algorithm minimizing overlap between labels.',
 'been observed by several other works to address this issue, we incorporate local semantics of functionality into prompt \ni.e. for each icons detected by interactable region detection model, we use finetuned model to generate description of functionality to the icons, \nand for each text boxes, we use detected texts and its label\n\nwe curate dataset of 7k icon - description pairs using gpt-4o, and finetune blip-v2 model on this\n1 examples of parsed screenshot image and local semantics by omniparser\nthe inputs to omniparse are user task and ui screenshot from which it will produce \nparsed screenshot image with bounding boxes and numeric ids overlayed, \nand local semantics contains both text extracted and icon description\n\nafter finetuning, we found the model is much more reliable in its description to common app icons\nwe conduct experiments on several benchmarks to demonstrate the effectiveness of omniparser',
 "We investigate common app icons and their description to improve GPT-4 v model's performance on image classification benchmarks. \n\nOur experiments show that incorporating local bounding boxes' semantics as a text prompt alongside UI screenshot visual prompts enhances the model's accuracy. We start with motivating experiments showcasing GPT-4v model's limitations when using a specific set of markup prompts, which leads to incorrect label assignments for referred bounding boxes.",
 "each task refers to one of the detected bounding boxes, e.g. click on settings or click on the minimize button.\nduring evaluation, gpt-4v is prompted to predict the bounding box id associated with it.\ndetailed prompt are specified in appendix.\nthe tasks screenshot images are sampled from the screenspot [csc + 24 ] benchmark,\nwhere they are labeled with set of marks using omniparser.\nthe tasks are further divided into 3 sub-categories by difficulty : easy (less than 10 bounding boxes ), medium (10 - 40 bounding boxes ) and hard (more than 40 bounding boxes ).\nfrom table 1, gpt-4v often mistakenly assign the numeric id to the table especially when there are a lot of bounding boxes over the screen.\nand by adding local semantics including texts within the boxes and short descriptions of the detected icons, \ngpt-4v 's ability of correctly assigning the icon improves from 0.705 to 0.938.\nfrom figure 3, we see that without description of the referred icon in the task, \ngpt-4v often fails to link the icon required in the task and the ground truth icon id in the som labeled screenshot,\nwhich leads to hallucination in response.\nwith fine grain local semantics added in text prompt, gpt-4v makes it much easier for gpt-4v to find correct icon id for referred icon.",
 'gpt - 4v makes it much easier for a language model to find the correct icon id for referred icons, with or without local semantics. \n\nHere is how different models (with and without) local semantics perform in evaluating UI screenshots from mobile (ios, android), desktop (macos, windows), and web platforms using benchmark dataset that includes over 600 interface screenshots.\n \n The results show that the model performing better on Screenspot screenspot dataset [CSC + 24] is Omniparser with or without local semantics. Without local semantics the model performs slightly worse than baseline but still keeps a good accuracy whereas Baseline significantly improves when local semantic is added giving the desired accuracy improvement for GPT-4v.\n \n The models like Seeclick, CogAgent and Fuyu seem to fine-tune better on GUI dataset  compared to baseline with or without ls.',
 'Incorporating local semantics and interactable region detection significantly improves the overall performance of UI screens, accurately identifying elements for operation by GPT-4v. The introduction of functional local semantics enhances accuracy, underscoring the importance of accurate identification.\n\nProcessed Text:\n\nMethods: fuyu cogagent seeclick minigpt-v2 qwen-vl gpt-4v omniparser w. o. ls w. gd omniparser w. ls + gd omniparser w. ls + id\nModel Size: 8b 18b 9.6b 7b 9.6b \n\nMobile Icon / Widget Task Performance:\nText: 33.0% 74.2% 72.2% 62% 52% \nMobile Icon / Widget 1.3% 24.0% 52.0% 66.8% 48%',
 '0 % 67 was removed\n0 % 78 became 0% 78%\n0 % 8 became 0% 8%\n4 % 9 became % 9.\n5 % 22 became % 22.\n6 % 92 became % 92.\n7 % 94 became % 94.\n8 % 93 became % 93.\n9 %  mobile icon / widget 1. 3 % 24. 0 % 52. 0 % 6. 6 % 4. \n22 became % 22\n24. 5 %\n49. \n53. 7 % 57. 0 % text 33. \n72. 2 % 72. 2 % 6. The percentage has been removed\n4. 8. then the remaining parts are:\nText 32, \n text 33. became, text 33.\n74 becomes 74 \n2 75.\n\n72 then, 6, then  5\n7.20 become .70,  is removed \n then 20.\n\n desktop icon / widget 3\n \n6 % \n\n30.0%\n5% \n28%\n33.9%\n\n71\n4 55.\n4 5. became 4%\n   becomes 7\n83.0 then removed .\n\n web icon / widget 4\n   32 %\n39 % then, 45.\n\n table \n   2 : \n the following information has been left in, to hopefully inform some sort of category: comparison of different approaches on \nScreenSpot Benchmark \nLS is short for Local Semantics of Functionality, GD is short for Grounding Dino and ID is short for Interactable Region Detection Model \nThe task is on: how omni parser helps webs navigation.\nThen:\nwe evaluated, for: Mind 2 Web in a secnario\nthe 3 categories : cross domain , x - web , x - task then : a benhmark ,  for Mind 2 Web processed .',
 'the goal is to evaluate the performance of an omniparser in the web navigation scenario using the [ dgz + 23 ] benchmark test set which involves three task categories namely cross-domain, cross-website and cross-tasks with a total of 867 167 and 242 tasks respectively however during evaluation both parsed screen results and action history are fed as text prompts along with some labeled screenshots to gpt - 4v',
 "cogagent and qwen - vl are not finetuned on the mind2web training set. more detailed information about model settings can be found in appendix7. \n\nwe report numbers from mind2web paper [dgz + 23] and seeact [zgk + 24] paper, which use the html elements selected by a finetuned element proposal model on mind2web training set producing top 50 relevant elements on the html page based on the user task. \n\ngpt-4v + som and gpt-4v + textual choices corresponds to seeact with image annotation, and textual choices grounding methods respectively.\n\nin gpt-4v + som, the set of mark (som) boxes are selected from the element proposal model, labeled with ground truth location extracted from html. in contrast, gpt-4v + textual uses dom information of selected elements directly in text prompt. better performance of textual choice corroborates with experiment results section 4.1.\n\nin last section, we report numbers from omniparser. local semantics of icon functionality and finetuned interactable region detection model (w.ls + id) performs better than raw grounding dino model (w.ls + gd) in all categories. without parsing html information, omniparser outperforms gpt-4's performance that uses html in every sub-category by large margin",
 'augmented with local semantics of icon functionality and the finetuned interactable region detection model (w. ls + id) performs better than the model with raw grounding Dino model (w. ls + gd) in all categories. \n\nfurther, without using parsed html information, Omniparser is able to outperform GPT-4’s performance that uses html in every sub-category by a large margin.\n\nAdditionally, Omniparser outperforms the GPT-4v + som by a large margin compared to GPT-4v + textual choices. \n\nOmniparser significantly outperforms in cross-website and cross-domain category (+4.1% and +5.2%). \n\nomniparser provides higher quality information compared ground truth element information from dom and top-k relevant element proposal used by the GPT-4v + textual choice set-up.',
 'qwen - vl methods input types cross - website cross - domain cross - task step sr cogagent\n17.6 qwen - vl 12.0 seeclick 25.5 mindact (gen) 11.9 39.6 mindact gpt-3.5 - turbo 18.6',
 '4 80. 6 44. 9 85. 7 45. 5 step sr ele. acc op. f1 53. 0 22. 4 84. 3 14. 1 87. 0 28. 3 44. 7 14. 2 66. 5 42. 1 52. 8 21. 6 60. 6 41. 6 \n- - 46. 4 73. 4 86. 7 42. 3 87. 6 42. 4 \n13. 4 9. 2 16. 4 11. 0 38. 9 16. 2 30. 1 32. 7 32. 4 36. 1 36. 5  \n15. 5 12. 0 20. 8 11. 9 39. 6 18. 6 26. 4 23. 7 \n4 evaluation on aitw in addition to evaluation on multi - step web browsing tasks, we assess omniparser on the mobile navigating benchmark aitw [ rlr + 23 ], which contains 30k instructions and 715k trajectories. \nwe use the same train / test split as in [ csc + 24 ] based on instructions, which retain only one trajectory for each instruction and no intersection between train and test. \nfor a fair comparison, we only use their test split for evaluation and discard the training set, our method does not require fine-tuning.',
 "we report the gpt-4v baseline in [yyz + 23] paper, which corresponds to the best performing setup (gpt-4v zs + history) that uses ui elements detected by iconnet [swl + 22] through set-of-marks prompting [yzl + 23] for each screenshot at every step of the evaluation. the detected ui elements consist of either ocr-detected text or an icon class label, which is one of the 96 possible icon types identified by iconnet. additionally, action history is also incorporated at each step's prompt as well. we used the exact same prompt format except replacing results from the iconnet model with finetuned interactable region detection (id) model - this yields significantly improved performance across most sub-categories and a 4.7% increase in overall score compared to best performing gpt-4v + history baseline",
 'text image image a couple of common failure cases of omniparser with examples and potential approach to improve repeated icons / texts from analysis of the gpt - 4v ’ s response log we found that gpt - 4v often fails to make the correct prediction when the results of the omniparser contains multiple repeated icons / texts which will lead to failure if the user task requires clicking on one of the buttons this is illustrated by figure 7 ( left ) in the appendix a potential solution to this is to add finer grain descriptions to the repeated elements in the ui screenshot so that gpt - 4v is aware of the existence of repeated elements and take it into account when predicting next action',
 'elements and take it into account when predicting next action. corase prediction of bounding boxes one common failure case of omniparser is that it fails to detect \nprocesssed text \n\n the task is to click on the text ’ more ’. the ocr module of omniparser detects text bounding box 8 which encompass the desired text. but since it uses center \nof the box as predicted click point, it falls outside of the ground truth bounding box. this is essentially due to the fact that the ocr module we use does not have a notion of \nwhich text region are hyperlink and clickable. hence we plan to train a model that combines ocr and interactable region detection into one module so that it can better detect \nthe clickable text / hyperlinks.\n\nicon misinterpretation we found that in some cases the icon with similar shape can have different meanings depending on the ui screenshot. for example, in figure 8, the task is to find button related to ’ more information ’, where the ground truth is to click the three dots icon \nupper right part of the screenshot. omniparser successfully detects all the relevant bounding boxes, but the icon description model interpret it as : " a loading or buffering indicator ".',
 'The topic appears to be related to computer vision and artificial intelligence, specifically in the area of icon detection and description modeling.\n\nHere is the processed text:\n\ndescription model is only able to see each icon cropped from image while not able to see the whole picture during both training and inference. a symbol of three dots can indeed mean loading buffer in other scenarios. without knowing the full context of the image, a general vision approach can be used to parse ui screenshots into structured elements. a proposed solution is omniparser, which encompasses two finetuned models : an icon detection model and a functional description model. these models are trained on curated datasets using popular webpages and demonstrate improved performance when used with gpt-4v in screenspot benchmarks.',
 'The paper is about multimodal models for UI understanding, specifically learning generic representations for UI elements.\n\nprocessing completed',
 'yu su : mind2web - towards a generalist agent for the web. \na mobile app dataset for building data-driven design applications.\nmultimodal web navigation with instruction - finetuned foundation models. \na real-world webagent with planning, long context understanding, and program synthesis. \nactionbert : leveraging user actions for semantic understanding of user interfaces.',
 'van wichers et al., gabriel schubiner et al., ruby lee et al., and blaise aguera y arcas. actionbert : leveraging user actions for semantic understanding of user interfaces, 2021. \nwenyi hong, weihan wang, qingsong lv, jiazheng xu, wenmeng yu, junhui ji, yan wang, zihan wang, yuxuan zhang, juanzi li, bin xu, yuxiao dong, ming ding, and jie tang. cogagent : a visual language model for gui agents, 2023. \njing yu koh, robert lo, lawrence jang, vikram duvvur, ming chong lim, po - yu huang, graham neubig, shuyan zhou, ruslan salakhutdinov, and daniel fried. visualwebarena : evaluating multimodal agents on realistic visual web tasks. arxiv preprint arxiv : 2401. 13649, 2024. \nevan zheran liu, kelvin guu, panupong pasupat, tianlin shi, and percy liang. rein - forcement learning on web interfaces using workflow - guided exploration. in interna - tional conference on learning representations ( iclr ), 2018. \nyang li, jiacong he, xin zhou, yuan zhang, and jason baldridge. mapping natural language instructions to mobile ui action sequences.',
 'yang li and gang li are investigating ways to map natural language instructions to mobile UI action sequences. their work on widget captioning has led to a better understanding of how to generate natural language descriptions for mobile user interface elements. another area of research is blip-2, which involves bootstrapping language-image pre-training with frozen image encoders and large language models. the android in the wild dataset provides a large-scale platform for device control, while from pixels to UI actions aims to learn how to follow instructions via graphical user interfaces',
 'Authors involved in research papers on mobile UI:\nMandar Joshi\nJames Cohan\nJonathan Berant\nPanupong Pasupat\nHexiang Hu\nUrvashi Khandelwal\nKenton Lee\nKristina Toutanova\nSrinivas Sunkara\nMaria Wang\nLiJuan Li\nGilles Baechler\nYu-Chung Hsiao\nJindong Chen\nAbhanshu Sharma\nJames Stout\nBryan Wang\nGang Li\nXin Zhang\nTovi Grossman\nYang Li\nJunyang Wang\nHaiyang Xu\nHaitao Jia\nXi Zhang\nMing Yan\nWeizhou Shen\nji Zhang\nFei Huang\nJitao Sang\nWenyang Wang\njiabo ye\nMing yan\nWeizhou shen\nji zhang\nFei Huang\nJitao Sang\nTianbao Xie\nDanyang Zhang\nJixuan Chen\nXiaochuan Li\nSiheng Zhao\nRuisheng Cao\nToh Jing Hua\nZhoujun Cheng\nDongchan Shin\nFangyu Lei\nYitao Liu\nYiheng Xu\nShuyan Zhou\nSilvio Savarese\nCaiming Xiong\nVictor Zhong\nTao Yu',
 'autonomous multi-modal mobile device agent with visual perception, tianbao xie et al. osworld: benchmarking multimodal agents for open-ended tasks in real computer environments. \n\nan yan et al. gpt-4v in wonderland : large multimodal models for zero-shot smartphone gui navigation.',
 'Floris Weers, Amanda Swearingen, Jeffrey Nichols, Yinfei Yang, and Zhe Gan et al. Ferret: UI - Grounded Mobile UI Understanding with Multimodal LLMs, 2024.\n[FERRET + 24]\nBoyun Zheng, Boyu Gou, Jihyung KIll, Huan Sun, and Yu Su et al. GPT-4v (ision): A Generalist Web Agent, if Grounded, 2024.\n[GPT-4v + 24]\n\nFor the dataset , we use  the result of parsed icon bounding boxes inferred by the interactable icon detection model on  the screenshots datasets since it contains screenshots of both mobile and pc. We collect 7185 icon description pairs for finetuning.\n\nWe train a GPT-XL model that includes all the following data:\nicon-text \ngrounded - multimodal  \nfrozen .',
 'whether the object presented in the parsed bounding box is an app icon. if gpt - 4o decides the image is an icon, it outputs one - sentence description of the icon about the potential functionality.\nand if not, gpt - 4o will output ’ this is not an icon ’.',
 'we train for 20 epochs with batch size of 256, learning rate of 1e−3, and the adam optimizer on 4 gpus. \nfor validation.\n\nwe show the training curve in figure 5. \n7. details of seeassign evaluation \n7. 3.\n1 \n\nprompt used for gpt - 4v\ngpt - 4v without local semantics : here is a ui screenshot image with bounding boxes and corresponding labeled id overlayed on top of it,\nyour task is { task }. which icon box label you should operate on? \n \ngive a brief analysis, then put your answer in the format of \n“ “ \nbox with label id : [ xx ] “ \n “ \n\ngpt - 4v with local semantics : here is a ui screenshot image with bounding boxes and corresponding labeled id overlayed on top of it,\nand here is a list of icon/text box description :\n{ figure 5 : training curves of interactable icon region detection model. parsed_local_semantics }. your task is { task }. which bounding box label you should operate on? \ngive a brief analysis, then put your answer in the format of \n “ ” \nbox with label id : [ xx ]“ \n “ \n\n7. 4 details of mind2web evaluation',
 'seeclick, qwen - vl seeclick is a finetuned version of qwen - vl on the mind2web training set and we report both of their numbers in their paper. cogagent number is taken from the seeact paper [ zgk + 24 ], where they report cogagent - chat - hf checkpoint that is not fine-tuned on mind2web for experiments. \nbaseline models: \nmindact (gen), mindact, gpt-3.5-turbo, gpt-4 numbers taken from the mind2web [dgz + 23] paper where they use html information to augment the corresponding web agent.\ngpt-4v + som this model corresponds to the image annotation grounding method in seeact paper where the som boxes extracted from selected html elements are provided to gpt-4v to make action prediction. \ngpt-4v + textual choice the best performing scenario (except oracle) that uses selected html elements information in a multi-choice question format as input to the gpt-4v agent.\nqualitative examples\nlocal semantics of icon function description helps gpt-4v make better action prediction. \nfigure 6 : more examples of local semantics of icon functionality help with gpt-4v in grounding actions \nfigure 7 : analysis of failure cases\nfailure cases: left, total 7 similar enable button for 7 different alarm times \nin the parsed screenshot. and the correct icon id corresponding to alarm 7 : 30 is 27. gpt-4v fails \nto make the correct prediction',
 'action prediction in figure 6\n12 figure 6 : more examples of local semantics of \nicon functionality help with gpt - \n4v in grounding actions \nfigure 7 : analysis of failure cases\nall the bounding boxes are labeled by which relies only on the screenshot\nleft : there are \nsimilar enable button for \ndifferent alarm times in the parsed screenshot\nand the correct icon id \ncorresponding to alarm 7 30 is 27\ngpt - \n4v fails to make the correct prediction \nright : the ground truth region \nclick is the text inside \nbounding box \nwe can see that the \nfails to detect the text \ninside bold and \nonly detects \nwhich encompasses \nso it the predicted click point as the center of \nbox so it falls outside \nof the ground truth region which leads \nto failure in this task\n13 figure 8 : analysis of failure cases\nthe task is to find \nrelated to more information \nand the ground truth is to \nclick the three dots icon \nin the upper right part of the screenshot\nthe icon functional description model does not take into \naccount the context of this page and interpret it as \n" a loading or buffering indicator " which causes \nfailure']</code></pre>
</div>
</div>
<p>So now we can compare the original chunk with the processed text with llama3.2 model. It really makes a good job to fix the text and structure.</p>
<div id="aae31d46" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9efbbed3" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(file_processor.chunked_docs[<span class="st">"./pdfs/ichigo.pdf"</span>])<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(file_processor.chunked_docs[<span class="st">"./pdfs/ichigo.pdf"</span>][i])</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"PROCESSED CHUNK "</span><span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">100</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(text[i])</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>following dataset featuring multi - turn interactions, reasoning tasks, and refusal scenarios. we also provide the training and inference code to facilitate further research in this area. 2 2 model architecture 2. 1 tokenized early fusion this methodology presents a unified framework leveraging token - based representations for both speech and textual modalities ( figure 1 ). by quantizing continuous speech into discrete tokens, similar to words in text, we can utilize the same transformer architecture to sequences of both speech and text tokens. this eliminates the need for separate speech / text encoders or domain - specific decoders. by projecting all modalities into a shared representational space from the outset, this method facilitates cross - modal reasoning and generation. 2. 2 tokenization process for speech tokenization, we employ whispervq, a component of whisperspeech [ collabora, 2024 ]. this model utilizes a codebook of 512 tokens with a codebook dimension of 64. based on the whisper medium model, whispervq processes speech input resampled to 16 khz, achieving a frame rate of 25 hz. initially, the audio is converted to a log - mel spectrogram and processed by a whisper encoder [ radford et al., 2022 ], producing continuous embeddings. these embeddings undergo downsampling and refinement before a vector quantization step maps them to a finite codebook, producing a sequence of discrete tokens representing the audio content. figure 1. ichigo represents speech and text modalities as discrete tokens and uses a uniform transformer - based architecture. it uses whispervq to quantize speech into discrete tokens in the same manner with original text modality. 2. 3 expanding the language model to incorporate multimodal discrete representations into pre - trained llms, we expand the vocabulary
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
following dataset featuring multi-turn interactions, reasoning tasks, and refusal scenarios, providing training and inference code for further research. unified framework using token-based representations for both speech and textual modalities, leveraging a transformer architecture. by quantizing continuous speech into discrete tokens, similar to words in text, this facilitates cross-modal reasoning and generation. 

tokenization process for speech uses whispervq, a component of whisperspeech, processing audio resampled to 16 kHz with a frame rate of 25 Hz. initially, the audio is converted to a log-mel spectrogram before being processed by a whisper encoder, producing continuous embeddings. these embeddings undergo downsampling and refinement, then vector quantization, resulting in discrete tokens representing the audio content.

the dataset represents speech and text modalities as discrete tokens, utilizing a uniform transformer-based architecture.
====================================================================================================
was inaudible, unclear, or affected by back - ground noise, ichigo demonstrated appropriate behavior by refusing to provide random answers. instead, as illustrated in figure 6, the model politely requested the user to repeat their query, ensuring accurate and 12 figure 5. the model follows text - based system prompts during speech - based conversations with users. relevant responses. figure 6. a. transcribed dialogue examples using ichigo. the user - turn is audio input. the examples illustrate zero - shot multi - turn capabilities. b. ichigo requests clarification from the user when unable to understand the question clearly. these experiments complement our quantitative findings, demonstrating ichigo ’ s practical capabilities in real - world scenarios. ichigo ’ s abilities in cross - modal instruction following, multi - turn dialogues, and handling unclear inputs make it a promising candidate for advanced, user - friendly voice ai applications. 13 6 related works 6. 1 early audio language models the success of language models in natural language processing [ radford et al., 2019, raffel et al., 2020, openai et al., 2024 ] has inspired researchers to explore similar approaches for modeling speech and audio. initial efforts in audio language modeling focused on training models using semantic or acoustic tokens derived from audio data, enabling audio generation without the need for text input [ borsos et al., 2023, nguyen et al., 2023, lakhotia et al., 2021 ]. subsequent advancements led to the joint training of speech tokens and text, resulting in decoder - only models such as vall - e [ wang et al., 2023a, chen et al., 2024 ] and viola [ wang et al., 2023b ]. these models demonstrated capabilities in speech recognition, translation, and synthesis. however, these early models were not built upon large language
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
was inaudible, unclear, or affected by background noise. Ichigo demonstrated appropriate behavior by refusing to provide random answers.

The model politely requested the user to repeat their query, ensuring accurate responses.
In zero-shot multi-turn capabilities, the model follows text-based system prompts during speech-based conversations with users.
These experiments complement our quantitative findings, demonstrating Ichigo’s practical capabilities in real-world scenarios.
Ichigo's abilities in cross-modal instruction following, multi-turn dialogues, and handling unclear inputs make it a promising candidate for advanced, user-friendly voice AI applications.

Early research on audio language models drew inspiration from the success of language models in natural language processing (Radford et al., 2019; Raffel et al., 2020; OpenAI et al., 2024).
These early efforts explored training models using semantic or acoustic tokens derived from audio data, enabling audio generation without text input:
Borsos et al. (2023); Nguyen et al. (2023); Lakhotia et al. (2021).

Advancing to joint training of speech and text led to decoder-only models such as Vall-E (Wang et al., 2023a; Chen et al., 2024) and Viola (Wang et al., 2023b).
These initial models demonstrated capabilities in speech recognition, translation, and synthesis
====================================================================================================
/ a : not applicable due to significant performance degradation in mathematical and coding tasks during pre - training on sound tokens with next token prediction. 5. 4 instruction following cross modality in addition to the quantitative results presented earlier, we conducted practical experiments with ichigo in real - world conversational scenarios. these experiments aimed to assess the model ’ s ability to follow system prompts and maintain coherent multi - turn dialogues across different modalities ( text and speech ). figure 4. the system prompt used for ichigo during inference. cross - modal instruction following : ichigo demonstrated a robust ability to follow text - based system prompts while engaging in speech - based conversations with users. this highlights the model ’ s capacity to generalize instructions across modalities, a crucial feature for versatile ai assistants. as shown in figure 5, the model consistently maintained its prescribed identity as " ichigo " when questioned, adhering to the system prompt instructions. this behavior persisted regardless of whether the input was in text or speech format, demonstrating the model ’ s ability to maintain context across different input modalities. multi - turn dialogue coherence : ichigo exhibited proficiency in managing multi - turn conversations, seamlessly understanding and responding to both speech and text inputs without apparent difficulties. figure 6 presents transcribed dialogue examples that showcase the model ’ s zero - shot multi - turn capabilities. handling unclear inputs : in scenarios where user input was inaudible, unclear, or affected by back - ground noise, ichigo demonstrated appropriate behavior by refusing to provide random answers. instead, as illustrated in figure 6, the model politely requested the user to repeat their query, ensuring accurate and 12 figure 5. the model follows text - based system prompts during speech - based conversations with users. relevant responses. figure 6. a. transcribed dialogue examples using ichigo. the user - turn is audio input. the examples illustrate zero - shot multi
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
ichigo demonstrated a robust ability to follow text-based system prompts while engaging in speech-based conversations with users. this highlights the model's capacity to generalize instructions across modalities, a crucial feature for versatile AI assistants.

the model consistently maintained its prescribed identity as " ichigo " when questioned, adhering to the system prompt instructions. this behavior persisted regardless of whether the input was in text or speech format, demonstrating the model's ability to maintain context across different input modalities.

ichigo exhibited proficiency in managing multi-turn conversations, seamlessly understanding and responding to both speech and text inputs without apparent difficulties.

the model handled unclear inputs by refusing to provide random answers and instead politely requested the user to repeat their query.
====================================================================================================
, scott mayer mckinney, christine mcleavey, paul mcmillan, jake mcneil, david medina, aalok mehta, jacob menick, luke metz, andrey mishchenko, pamela mishkin, vinnie monaco, evan morikawa, daniel mossing, tong mu, mira murati, oleg murk, david mely, ashvin nair, reiichiro nakano, rajeev nayak, arvind neelakantan, richard ngo, hyeonwoo noh, long ouyang, cullen o ’ keefe, jakub pachocki, alex paino, joe palermo, ashley pantuliano, giambattista parascandolo, joel parish, emy parparita, alex passos, mikhail pavlov, andrew peng, adam perelman, filipe de avila belbute peres, michael petrov, henrique ponde de oliveira pinto, michael, pokorny, michelle pokrass, vitchyr h. pong, tolly powell, alethea power, boris power, elizabeth proehl, raul puri, alec radford, jack rae, aditya ramesh, cameron raymond, francis real, kendra rimbach, carl ross, bob rotsted, henri roussez, nick ryder, mario saltarelli, ted sanders, shibani santurkar, girish sastry, heather schmidt, david schnurr, john schulman, daniel selsam, kyla sheppard, toki sherbakov, jessica shieh, sarah shoker, pranav shyam, szymon sidor, eric sigler, maddie simens, jordan sitkin, katarina slama, ian sohl, ben - jamin sokolowsky, yang song, natalie staudacher, felipe petroski such
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
Scott Mayer McKinney, Jacob Menick and David Medina 

 Scott Mayer McKinney and  Christine McLeavey 
 Scott Mayer McKinney, Christine McLeaves and Paul McMullen. Scott Mayer Mekinny 

David Medina
Aalok Mehta 
Alok Mehta
Jacob Menick 
Luke Metz 
Andrey Mishchenko 
Pamela Mishkin 
Vinnie Monaco 
Evan Morikawa
====================================================================================================
##rosekirk, 2024, baai, 2024 ]. these datasets span a wide array of topics, thereby diversifying the input data for our model. we implemented a two - step filtering process to ensure data quality and relevance. the main steps are language identification and deduplication. language identification : we applied the fasttext model [ bojanowski et al., 2017 ] as a language identifier at the document level, retaining only english documents with a confidence threshold of ( 0. 9 ). this decision aligns the model ’ s distribution more closely with the original multilingual training of the base llm. deduplication : we removed duplicate entries to prevent overfitting and ensure a diverse training set. despite the tokenizer ’ s capacity to handle eight languages, we opted to focus primarily on english for this training iteration. this decision was motivated by the relative scarcity of high - quality instruction data in other languages and the low - resource nature of these languages in our dataset. 3. 2. 2 speech - text instruction data building upon the text instruction dataset, we conducted further filtering to create a dataset more suitable for instruction speech dataset ( figure 2 ). length filtering : to prevent exceeding the llm ’ s context length, we filtered out text instructions longer than 64 tokens. this threshold was established based on empirical observations of typical user interactions with audio assistants. quality filtering : we eliminated samples that would be challenging to pronounce as speech, such as urls, mathematical symbols, and code snippets. synthetic data generation pipeline : we implemented a two - stage process to convert our text - based instruction dataset into discrete sound tokens suitable for audio input. we utilized the whisperspeech text - to - speech ( tts ) model to generate audio files from the instruction dataset ’ s questions. subsequently, 5 we employed the
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
rosekirk, 2024, baai, 2024 ]. these datasets span a wide array of topics, thereby diversifying the input data for our model. 
we implemented a two - step filtering process to ensure data quality and relevance. language identification : we applied the fasttext model [ bojanowski et al., 2017 ] as a language identifier at the document level, retaining only english documents with a confidence threshold of ( 0. 9 ). this decision aligns the model ’ s distribution more closely with the original multilingual training of the base llm. 
deduplication : we removed duplicate entries to prevent overfitting and ensure a diverse training set. despite the tokenizer ’ s capacity to handle eight languages, we opted to focus primarily on english for this training iteration. 
this decision was motivated by the relative scarcity of high - quality instruction data in other languages and the low - resource nature of these languages in our dataset. 
3. 2. 2 speech - text instruction data to prevent exceeding the llm ’ s context length, we filtered out text instructions longer than 64 tokens. this threshold was established based on empirical observations of typical user interactions with audio assistants. quality filtering : we eliminated samples that would be challenging to pronounce as speech, such as urls, mathematical symbols, and code snippets. 
synthetic data generation pipeline : we implemented a two - stage process to convert our text - based instruction dataset into discrete sound tokens suitable for audio input.
====================================================================================================
the embedding matrix to accommodate the new tokens, the rest of the language model remains unaltered. the tokens generated by whispervq are converted to the format &lt; | sound _ dddd | &gt;, where ’ dddd ’ represents the position of the cor - responding code. additionally, we introduce two new special tokens, &lt; | sound _ start | &gt; and &lt; | sound _ end | &gt;, to delimit audio file inputs. initially, we attempted to use the default new token initialization from the huggingface codebase. however, this approach resulted in slow convergence of the loss curve. to address this issue, we switched to initial - izing new token embeddings by averaging all embeddings of the current vocabulary [ hewitt, 2021 ]. this modification significantly improved the speed of convergence and enhanced training stability. 4 3 datasets to enable ichigo to process and understand audio signals, we have curated a comprehensive dataset com - prising two main components : the pre - training dataset and the instruction speech dataset. the former facilitates the llm ’ s understanding of audio signals, while the latter enables cross - modal instruction tuning. this section provides a detailed overview of our data collection and processing methodologies. 3. 1 pre - training dataset to align the embeddings of text and audio, we assembled a diverse collection of public automatic speech recognition ( asr ) datasets spanning eight languages : english, german, dutch, spanish, french, italian, portuguese, and polish. we obtained english from the mls english 10k dataset [ pratap et al., 2020 ] and other languages from the multilingual librispeech dataset [ pratap et al., 2020 ]. the training dataset encompasses approximately 10, 000 hours of english audio and an additional 6, 000 hours distributed
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
the embedding matrix to accommodate the new tokens, the rest of the language model remains unaltered. we introduce two new special tokens, &lt; | sound _ start | &gt; and &lt; | sound _ end | &gt;, to delimit audio file inputs. 
we've modified our approach to initializing new token embeddings by averaging all existing vocabulary embeddings. this change improved training speed and stability. 
our dataset comprises a comprehensive collection of public automatic speech recognition (asr) datasets in eight languages: english, german, dutch, spanish, french, italian, portuguese, and polish. 
the dataset spans approximately 16,000 hours of audio, including 10,000 hours of english audio from the mls english 10k dataset and additional hours distributed across other languages from the multilingual librispeech dataset
====================================================================================================
- end speech language models in speech - based question - answering tasks, marking a significant step forward in multimodal ai. importantly, ichigo demonstrates a real - time speech system with a latency of 110 milliseconds to first re - sponse. this opens up new possibilities for speech systems and significantly reduces the complexity of deploying such systems in production environments. we believe that this paper will empower smaller research teams - like ourselves - to contribute more con - fidently and prolifically to the open - source community. by demonstrating that significant advancements can be achieved with limited resources, we hope to inspire broader participation in this critical area of ai research. 8 limitations and future work while ichigo represents a significant step forward in multimodal language modeling, several limitations and areas for future work remain : token stability : similar to challenges faced by models like chameleon, we encountered fluctuating loss when training with acoustic tokens, which led us to shift towards semantic tokens to achieve stable loss. this highlights the difficulty in training with rich, acoustic information. future work should explore methods to stabilize training with acoustic tokens, potentially unlocking even more powerful models. emotional understanding : the current architecture does not fully account for emotional compre - hension. future iterations should focus on enhancing the model ’ s ability to understand and respond to user emotions, allowing for more nuanced and context - appropriate responses. context length : multimodal content, especially audio, often spans extensive sequences. ichigo currently limits modeling to 10 seconds of speech input and performs well for 4 - 5 turns of conversation. extending the context window would allow for modeling of longer audio segments and handling of more complex, multi - turn conversations. 15 references baai. infinity - instruct, 2024. url https : / / huggingface. co / datasets / baai / infinity - instruct
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
end speech language models in speech-based question-answering tasks, marking a significant step forward in multimodal AI. 
 Ichigo demonstrates a real-time speech system with a latency of 110 milliseconds to first response.
 This opens up new possibilities for speech systems and significantly reduces the complexity of deploying such systems in production environments.
 We believe that this paper will empower smaller research teams to contribute more confidently and prolifically to the open-source community.
 By demonstrating that significant advancements can be achieved with limited resources, we hope to inspire broader participation in this critical area of AI research.

 Several limitations and areas for future work remain:
 
 Token stability was a challenge when training with acoustic tokens, leading us to shift towards semantic tokens to achieve stable loss. This highlights the difficulty in training with rich, acoustic information. Future work should explore methods to stabilize training with acoustic tokens, potentially unlocking even more powerful models.
 Emotional understanding is currently not fully accounted for in the model's ability to understand and respond to user emotions. Future iterations should focus on enhancing the model's ability to handle emotional contexts and provide more nuanced responses.
 Context length is a limitation as Ichigo models 10 seconds of speech input which performs well but may struggle with longer audio segments or complex, multi-turn conversations.

15 references 
 BAII. Infinity-Instruct, 2024
====================================================================================================
##nthi, et al. speechverse : a large - scale generalizable audio language model. arxiv preprint arxiv : 2405. 08295, 2024. alexandre defossez, laurent mazare, manu orsini, amelie royer, patrick perez, herve jegou, edouard grave, and neil zeghidour. moshi : a speech - text foundation model for real - time dialogue. arxiv preprint arxiv : 2410. 00037, 2024. soham deshmukh, benjamin elizalde, rita singh, and huaming wang. pengi : an audio language model for audio tasks. advances in neural information processing systems, 36 : 18090 – 18108, 2023. abhimanyu dubey, abhinav jauhri, abhinav pandey, abhishek kadian, ahmad al - dahle, aiesha letman, akhil mathur, alan schelten, amy yang, angela fan, et al. the llama 3 herd of models. arxiv preprint arxiv : 2407. 21783, 2024. alexandre defossez, jade copet, gabriel synnaeve, and yossi adi. high fidelity neural audio compression. arxiv preprint arxiv : 2210. 13438, 2022. euclaise. gsm8k _ multiturn, 2024. url https : / / huggingface. co / datasets / euclaise / gsm8k _ multiturn. qingkai fang, shoutao guo, yan zhou, zhengrui ma, shaolei zhang, and yang feng. llama - omni : seamless speech interaction with large language models. arxiv preprint arxi
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
speech verse : a large scale generalizable audio language model
arxiv preprint arxiv : 2405.08295, 2024
alexandre defossez et al 
moshi : a speech text foundation model for real time dialogue
arxiv preprint arxiv : 2410.00037, 2024
soham deshmukh et al
pengi : an audio language model for audio tasks
advances in neural information processing systems, 36 : 18090 – 18108, 2023
abhimanyu dubey et al 
the llama 3 herd of models
arxiv preprint arxiv : 2407.21783, 2024
alexandre defossez et jade copet
high fidelity neural audio compression
arxiv preprint arxiv : 2210.13438, 2022
qingkai fang et al 
gsm8k multiturn 
url https : / / huggingface. co / datasets / euclaise / gsm8k _ multiturn
====================================================================================================
3 ), indicating the effectiveness of our iterative training approach. this indicates that with extended training time and more computational resources, we can achieve higher performance. pre - training challenges : it ’ s worth noting that during the initial pre - training phase, which focused solely on sound tokens with next token prediction, we observed a significant degradation in the model ’ s performance on text - based tasks, particularly in mathematics and coding. this highlights the challenges in maintaining cross - modal capabilities during specialized training. 11 table 4. results of ichigo across different versions and the original llama3 8b instruct model. model mmlu gpqa ( 0 - shot ) ( 5 - shots ) gsm - 8k ( cot ) ( 8 - shots ) avg. llama3 8b instruct 69. 4 30. 4 84. 5 61. 43 ichigo base v0. 2 ichigo instruct v0. 2 ichigo base v0. 3 ichigo instruct v0. 3 ( phase 2 ) ichigo instruct v0. 3 ( phase 3 ) 47. 66 50. 27 42. 11 63. 08 63. 79 28. 13 26. 56 28. 57 28. 35 29. 69 n / a * 53. 58 n / a * 76. 50 75. 28 n / a * 43. 47 n / a * 55. 98 56. 25 n / a : not applicable due to significant performance degradation in mathematical and coding tasks during pre - training on sound tokens with next token prediction. 5. 4 instruction following cross modality in addition to the quantitative results presented earlier, we conducted practical experiments with ichigo in real - world conversational scenarios. these experiments aimed to assess the model ’ s ability to follow system prompts and maintain coherent multi - turn dialogues across different modalities ( text and speech ). figure 4. the system prompt used
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
this indicates that with extended training time and more computational resources, we can achieve higher performance. pre-training challenges : it's worth noting that during the initial pre-training phase, which focused solely on sound tokens with next token prediction, we observed a significant degradation in the model's performance on text-based tasks, particularly in mathematics and coding. this highlights the challenges in maintaining cross-modal capabilities during specialized training. results across different versions of our model, llama3 8b instruct, and ichigo demonstrate improved performance in various metrics. notable improvements include instruction following capabilities, as demonstrated in real-world conversational scenarios.
====================================================================================================
##t arxiv : 2406. 16020, 2024. chengyi wang, sanyuan chen, yu wu, ziqiang zhang, long zhou, shujie liu, zhuo chen, yanqing liu, huaming wang, jinyu li, et al. neural codec language models are zero - shot text to speech synthesizers. arxiv preprint arxiv : 2301. 02111, 2023a. 19 tianrui wang, long zhou, ziqiang zhang, yu wu, shujie liu, yashesh gaur, zhuo chen, jinyu li, and furu wei. viola : unified codec language models for speech recognition, synthesis, and translation. arxiv preprint arxiv : 2305. 16107, 2023b. zhangchen xu, fengqing jiang, luyao niu, yuntian deng, radha poovendran, yejin choi, and bill yuchen lin. magpie : alignment data synthesis from scratch by prompting aligned llms with nothing. arxiv preprint arxiv : 2406. 08464, 2024. jun zhan, junqi dai, jiasheng ye, yunhua zhou, dong zhang, zhigeng liu, xin zhang, ruibin yuan, ge zhang, linyang li, et al. anygpt : unified multimodal llm with discrete sequence modeling. arxiv preprint arxiv : 2402. 12226, 2024. xin zhang, dong zhang, shimin li, yaqian zhou, and xipeng qiu. speechtokenizer : unified speech tokenizer for speech large language models. arxiv preprint arxiv : 2308. 16692, 2023. 20 a additional data and analysis this
PROCESSED CHUNK ----------------------------------------------------------------------------------------------------
Neural codec language models are zero-shot text-to-speech synthesizers. 
Unified codec language models for speech recognition, synthesis and translation. 
Alignment data synthesis from scratch by prompting aligned LLMs with nothing. 
Unified multimodal LLM with discrete sequence modeling. 
Unified speech tokenizer for speech large language models.
====================================================================================================</code></pre>
</div>
</div>
</section>
</section>
<section id="transcript-writer" class="level2">
<h2 class="anchored" data-anchor-id="transcript-writer">2: Transcript Writer</h2>
<p>Now we will create a Class that will work in the script generation.</p>
<p>The construction of the class involves the declaration of the <code>SYS_PROMPT</code> for the model and the instantiation of 3 different LLM clients with different caabilities to decide each step of the process.</p>
<p>Experimentation with the <code>SYSTEM_PROMPT</code> below is encouraged, this worked best for the few examples the flow was tested with:</p>
<hr>
<p><a href="https://github.com/Dmaturana81/Agents/blob/main/Agents/notebookAI/preprocess.py#L245" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="podcastwriter" class="level3">
<h3 class="anchored" data-anchor-id="podcastwriter">PodcastWriter</h3>
<blockquote class="blockquote">
<pre><code> PodcastWriter (processor:__main__.PDFProcessor,
                model_big:str='gpt-4o-mini', model_medium:str='llama3.2',
                model_small:str='llama3.2')</code></pre>
</blockquote>
<p><em>Initialize self. See help(type(self)) for accurate signature.</em></p>
<hr>
<p><a href="https://github.com/Dmaturana81/Agents/blob/main/Agents/notebookAI/preprocess.py#L259" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="podcastwriter.generate_podcast" class="level3">
<h3 class="anchored" data-anchor-id="podcastwriter.generate_podcast">PodcastWriter.generate_podcast</h3>
<blockquote class="blockquote">
<pre><code> PodcastWriter.generate_podcast ()</code></pre>
</blockquote>
<div id="641aaad9" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>podcaster<span class="op">=</span> PodcastWriter(processor<span class="op">=</span>file_processor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="78de1da4" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>podcaster.generate_podcast()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>138066</code></pre>
</div>
</div>
<p>The Podcast script have been generated already and it is stored in the class attribute <code>podcast_text</code>. we can check it out:</p>
<div id="e8d959ae" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(podcaster.podcast_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>**Speaker 1:** Hey everyone, welcome back to another exciting episode of our podcast! Today, we're diving into the fascinating world of AI, specifically exploring how large language models, or LLMs, are evolving to tackle the complexities of speech and text interactions. We have a groundbreaking topic on our hands, and I'm thrilled to share it with you. We’re talking about a new model called Ichigo!

**Speaker 2:** Oh wow, Ichigo? That sounds intriguing! Umm, what makes it different from other models out there?

**Speaker 1:** Great question! So, Ichigo is a mixed-modal model designed to process both speech and text simultaneously, which is a game changer! Traditional models typically handle these modalities separately, which can lead to increased latency—think about how frustrating it is to wait for responses during conversations. Ichigo uses a tokenized early-fusion approach, meaning it quantizes speech into discrete tokens and employs a uniform transformer architecture for both modalities. This allows for seamless integration of speech and text, resulting in faster response times.

**Speaker 2:** Oh, I see! So, it’s like it’s speaking and writing at the same time—umm, kind of like a bilingual conversation? But, how does it manage to reduce latency? 

**Speaker 1:** Exactly! Imagine having a bilingual friend who can switch between languages effortlessly without skipping a beat. Ichigo achieves a remarkable latency of just 111 milliseconds to generate its first token, which is, to put it into perspective, much quicker than many existing models. Traditional cascaded systems often take several seconds due to multiple steps involved—like transcribing speech, understanding it, and then generating a response. Ichigo combines all these steps, making it incredibly efficient.

**Speaker 2:** Wow, that’s impressive! So, it sounds like it’s designed for real-time applications—umm, can you give an example of where this would be particularly useful?

**Speaker 1:** Absolutely! Think of virtual assistants—like Siri or Alexa. With Ichigo, these systems could respond to user queries in real time, understanding both spoken language and written commands without the lag. This could also transform customer service—imagine a support bot that can respond to voice calls and text inquiries almost instantaneously. It creates a more natural conversational experience.

**Speaker 2:** That’s amazing! But, what about the technical details? How does this tokenization process work, and why is it so revolutionary?

**Speaker 1:** Great follow-up! The tokenization process in Ichigo utilizes a component called WhisperVQ, which converts continuous speech into discrete tokens—similar to how words are processed in text. This process involves transforming audio into log-mel spectrograms, then using vector quantization to map these to a codebook of tokens. By doing this, Ichigo can handle speech and text with the same underlying architecture, which is a major step forward in multimodal AI.

**Speaker 2:** So, it’s like creating a universal language for both speech and text! Umm, I can’t help but think about how this must have implications for accessibility too. 

**Speaker 1:** Spot on! This model could significantly enhance accessibility for users with disabilities. For instance, it could allow for more accurate and efficient voice-to-text transcription for those who rely on assistive technologies, making information more accessible. The implications for education, customer service, and more are just massive!

**Speaker 2:** That’s really powerful! But, I’m curious, are there any challenges or limitations with Ichigo? 

**Speaker 1:** Absolutely, every new technology comes with its challenges. One limitation is that Ichigo currently models a context length of only 10 seconds of speech input. While that performs well for short interactions, it can struggle with longer or more complex conversations. Expanding this context window could lead to even better performance.

**Speaker 2:** Hmm, so it’s like having a short attention span in conversations? That makes sense. Are there plans to address this?

**Speaker 1:** Yes, researchers are actively looking at ways to extend the context window and enhance how Ichigo understands emotional cues in conversations. This could allow for a richer, more nuanced interaction that feels more human-like.

**Speaker 2:** I love that! It really feels like we’re on the brink of something incredible with AI. So, what’s next for models like Ichigo? 

**Speaker 1:** The future is bright! Continued research and development will focus on improving model robustness, expanding capabilities, and potentially integrating more sensory data—like visual inputs. This could lead to even more sophisticated AI that understands context in a holistic manner.

**Speaker 2:** Wow, that’s so exciting! I can’t wait to see how this all evolves. Thank you for sharing all of this amazing information with us today!

**Speaker 1:** It’s been a pleasure! Thanks for tuning in, everyone! Stay curious, and we’ll see you next time!</code></pre>
</div>
</div>
</section>
</section>
<section id="transcript-re-writer" class="level2">
<h2 class="anchored" data-anchor-id="transcript-re-writer">3: Transcript Re-writer</h2>
<p>Previously we generated a podcast script using the <code>SYS_WRITER</code> prompt. Now we will use the <code>SYS_REWRITER</code> prompt to make the script more dramatic or realistic.</p>
<p>We will again set the <code>SYSTEM_PROMPT</code> and remind the model of its task.</p>
<p>Note: We can even prompt the model like so to encourage creativity:</p>
<blockquote class="blockquote">
<p>Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.</p>
</blockquote>
<p>We will also use the <code>literal_eval</code> to parse the response from the LLM as a list of tuples.</p>
<hr>
<p><a href="https://github.com/Dmaturana81/Agents/blob/main/Agents/notebookAI/preprocess.py#L275" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="podcastwriter.rewrite_podcast" class="level3">
<h3 class="anchored" data-anchor-id="podcastwriter.rewrite_podcast">PodcastWriter.rewrite_podcast</h3>
<blockquote class="blockquote">
<pre><code> PodcastWriter.rewrite_podcast ()</code></pre>
</blockquote>
<div id="18c83d28" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>podcaster.rewrite_podcast()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="dd5996b2" class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>podcaster.rewritten_podcast</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('Speaker 1',
  "Hey everyone! Welcome back to another spectacular episode of our podcast! Buckle up because today, we're diving headfirst into the mind-boggling world of Artificial Intelligence! Our focus? The evolution of large language models, or LLMs, exploring how they are tackling the intricacies of speech and text interactions like never before. And trust me, you won’t want to miss this groundbreaking topic as we unveil a revolutionary model called Ichigo!"),
 ('Speaker 2',
  'Oh wow, Ichigo? That name alone sounds SO intriguing! Umm, what exactly sets it apart from all the other models out there?'),
 ('Speaker 1',
  "Fantastic question! So, here’s the scoop—Ichigo is a mixed-modal model designed to process speech and text simultaneously. I mean, come on, it's a total GAME CHANGER! Traditional models usually handle these two separately, and that often leads to increased latency—think about how frustrating it can be waiting for a response in a conversation! Ichigo uses a tokenized early-fusion approach, which means it quantizes speech into discrete tokens and employs a uniform transformer architecture for both modalities. This results in a smooth integration of speech and text, leading to lightning-fast response times!"),
 ('Speaker 2',
  'Oh, I see! So, it’s like it’s speaking and writing at the same time—umm, almost like a bilingual conversation? But, umm, how does it manage to cut down on that latency?'),
 ('Speaker 1',
  'Exactly! Picture this: you have a bilingual friend who can effortlessly switch between languages without missing a beat. Ichigo achieves an astounding latency of just 111 milliseconds to generate its first token. To put that into perspective, that’s WAY quicker than many existing models! Those traditional cascaded systems can take several seconds—like, can you imagine? That’s due to all the multiple steps involved, such as transcribing speech, comprehending it, and then generating a response. Ichigo combines all those steps into one, making it incredibly efficient.'),
 ('Speaker 2',
  'Wow, that’s absolutely impressive! So, it sounds like it’s tailor-made for real-time applications—umm, can you give us an example of where this could be particularly useful?'),
 ('Speaker 1',
  "Absolutely! Let's think about virtual assistants—like Siri or Alexa. With Ichigo, these systems could respond to user queries in real-time, comprehending both spoken language and written commands without any lag. Imagine transforming customer service—an intelligent support bot that can handle voice calls and text inquiries almost instantaneously! It creates a way more natural conversational experience!"),
 ('Speaker 2',
  'That’s mind-blowing! But, umm, I’m really curious about the technical details—how does this tokenization process work, and why is it considered so revolutionary?'),
 ('Speaker 1',
  'Great follow-up! The tokenization process in Ichigo employs a component called WhisperVQ, which converts continuous speech into discrete tokens—think of it like how we process words in text. This transformation entails changing audio into log-mel spectrograms, then leveraging vector quantization to map these to a codebook of tokens. By doing this, Ichigo is able to handle both speech and text with the same underlying architecture, marking a significant leap forward in multimodal AI!'),
 ('Speaker 2',
  'So, it’s like creating a UNIVERSAL language for both speech and text! Umm, I can’t help but wonder about how this must have implications for accessibility too.'),
 ('Speaker 1',
  'You hit the nail on the head! This model has the potential to significantly enhance accessibility for users with disabilities. For example, it could enable more accurate and efficient voice-to-text transcription for individuals relying on assistive technologies, making vital information more accessible. The implications for education, customer service—oh, the list goes on—are just MASSIVE!'),
 ('Speaker 2',
  'That’s really powerful! But, umm, I’m curious—are there any challenges or limitations with Ichigo?'),
 ('Speaker 1',
  'Absolutely, as with any new technology, challenges do arise. One limitation is that Ichigo currently models a context length of only 10 seconds of speech input. While that performs admirably for short interactions, it can struggle with longer or more complex conversations. Expanding this context window could lead to even better performance.'),
 ('Speaker 2',
  'Hmm, so it’s like having a short attention span in conversations? That definitely makes sense! Are there any plans to address this?'),
 ('Speaker 1',
  'Yes, indeed! Researchers are actively exploring ways to extend the context window and improve how Ichigo interprets emotional cues in conversations. This could pave the way for a richer, more nuanced interaction that feels more HUMAN-LIKE.'),
 ('Speaker 2',
  'I love that! It really feels like we’re on the brink of something INCREDIBLE with AI. So, what’s next for models like Ichigo?'),
 ('Speaker 1',
  'The future is indeed bright! Continued research and development will focus on enhancing model robustness, expanding capabilities, and potentially integrating more sensory data—like visual inputs! This could lead us to even more sophisticated AI that understands context in a holistic manner.'),
 ('Speaker 2',
  'Wow, that’s so exhilarating! I seriously can’t wait to see how this all evolves. Thank you for sharing all this amazing information with us today!'),
 ('Speaker 1',
  'It’s been an absolute pleasure! Thanks for tuning in, everyone! Stay curious out there, and we’ll catch you next time!')]</code></pre>
</div>
</div>
</section>
</section>
<section id="tts-workflow" class="level2">
<h2 class="anchored" data-anchor-id="tts-workflow">4: TTS Workflow</h2>
<p>We have the exact podcast transcripts ready now to generate our audio for the Podcast.</p>
<p>In this notebook, we will learn how to generate Audio using both <code>suno/bark</code> and <code>parler-tts/parler-tts-mini-v1</code> models first.</p>
<section id="testing-the-audio-generation" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-audio-generation">Testing the Audio Generation</h3>
<p>Let’s try generating audio using both the models to understand how they work.</p>
<p>Note the subtle differences in prompting: - Parler: Takes in a <code>description</code> prompt that can be used to set the speaker profile and generation speeds - Suno: Takes in expression words like <code>[sigh]</code>, <code>[laughs]</code> etc. You can find more notes on the experiments that were run for this notebook in the <a href="./TTS_Notes.md">TTS_Notes.md</a> file to learn more.</p>
<p>Please set <code>device = "cuda"</code> below if you’re using a single GPU node.</p>
</section>
</section>
<section id="bringing-it-together-making-the-podcast" class="level2">
<h2 class="anchored" data-anchor-id="bringing-it-together-making-the-podcast">Bringing it together: Making the Podcast</h2>
<p>Okay now that we understand everything-we can now use the complete pipeline to generate the entire podcast</p>
<p>Let’s load in our pickle file from earlier and proceed:</p>
<p>Let’s define load in the bark model and set it’s hyper-parameters for discussions</p>
<div id="b59952b4" class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cpu"</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>bark_processor <span class="op">=</span> AutoProcessor.from_pretrained(<span class="st">"suno/bark"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>bark_model <span class="op">=</span> BarkModel.from_pretrained(<span class="st">"suno/bark"</span>).to(device)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>bark_sampling_rate <span class="op">=</span> <span class="dv">24000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now for the Parler model:</p>
<div id="a22027cb" class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>parler_model <span class="op">=</span> ParlerTTSForConditionalGeneration.from_pretrained(<span class="st">"parler-tts/parler-tts-mini-v1"</span>).to(device)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>parler_tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"parler-tts/parler-tts-mini-v1"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Config of the text_encoder: &lt;class 'transformers.models.t5.modeling_t5.T5EncoderModel'&gt; is overwritten by shared text_encoder config: T5Config {
  "_name_or_path": "google/flan-t5-large",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 32128
}

Config of the audio_encoder: &lt;class 'parler_tts.dac_wrapper.modeling_dac.DACModel'&gt; is overwritten by shared audio_encoder config: DACConfig {
  "_name_or_path": "parler-tts/dac_44khZ_8kbps",
  "architectures": [
    "DACModel"
  ],
  "codebook_size": 1024,
  "frame_rate": 86,
  "latent_dim": 1024,
  "model_bitrate": 8,
  "model_type": "dac_on_the_hub",
  "num_codebooks": 9,
  "sampling_rate": 44100,
  "torch_dtype": "float32",
  "transformers_version": "4.46.1"
}

Config of the decoder: &lt;class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'&gt; is overwritten by shared decoder config: ParlerTTSDecoderConfig {
  "_name_or_path": "/fsx/yoach/tmp/artefacts/parler-tts-mini/decoder",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_cross_attention": true,
  "architectures": [
    "ParlerTTSForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1025,
  "codebook_weights": null,
  "cross_attention_implementation_strategy": null,
  "dropout": 0.1,
  "eos_token_id": 1024,
  "ffn_dim": 4096,
  "hidden_size": 1024,
  "initializer_factor": 0.02,
  "is_decoder": true,
  "layerdrop": 0.0,
  "max_position_embeddings": 4096,
  "model_type": "parler_tts_decoder",
  "num_attention_heads": 16,
  "num_codebooks": 9,
  "num_cross_attention_key_value_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 1024,
  "rope_embeddings": false,
  "rope_theta": 10000.0,
  "scale_embedding": false,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_fused_lm_heads": false,
  "vocab_size": 1088
}
</code></pre>
</div>
</div>
<div id="930a85fc" class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>speaker1_description <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="st">Laura's voice is expressive and dramatic in delivery, speaking at a moderately fast pace with a very close recording that almost has no background noise. It is completely engaging and captivating."""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will concatenate the generated segments of audio and also their respective sampling rates since we will require this to generate the final audio</p>
<div id="2af6c932" class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>generated_segments <span class="op">=</span> []</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>sampling_rates <span class="op">=</span> []  <span class="co"># We'll need to keep track of sampling rates for each segment</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Function generate text for speaker 1</p>
<div id="d313e628" class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_speaker1_audio(text):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generate audio using ParlerTTS for Speaker 1"""</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> parler_tokenizer(speaker1_description, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids.to(device)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    prompt_input_ids <span class="op">=</span> parler_tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>).input_ids.to(device)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    generation <span class="op">=</span> parler_model.generate(input_ids<span class="op">=</span>input_ids, prompt_input_ids<span class="op">=</span>prompt_input_ids)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    audio_arr <span class="op">=</span> generation.cpu().numpy().squeeze()</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> audio_arr, parler_model.config.sampling_rate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Function to generate text for speaker 2</p>
<div id="ab11985d" class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_speaker2_audio(text):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generate audio using Bark for Speaker 2"""</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> bark_processor(text, voice_preset<span class="op">=</span><span class="st">"v2/en_speaker_6"</span>).to(device)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    speech_output <span class="op">=</span> bark_model.generate(<span class="op">**</span>inputs, temperature<span class="op">=</span><span class="fl">0.1</span>, semantic_temperature<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    audio_arr <span class="op">=</span> speech_output[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> audio_arr, bark_sampling_rate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Helper function to convert the numpy output from the models into audio</p>
<hr>
<p><a href="https://github.com/Dmaturana81/Agents/blob/main/Agents/notebookAI/notebookme.py#L341" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="numpy_to_audio_segment" class="level3">
<h3 class="anchored" data-anchor-id="numpy_to_audio_segment">numpy_to_audio_segment</h3>
<blockquote class="blockquote">
<pre><code> numpy_to_audio_segment (audio_arr, sampling_rate)</code></pre>
</blockquote>
<p><em>Convert numpy array to AudioSegment</em></p>
<section id="generating-the-final-podcast" class="level4">
<h4 class="anchored" data-anchor-id="generating-the-final-podcast">Generating the Final Podcast</h4>
<p>Finally, we can loop over the Tuple and use our helper functions to generate the audio</p>
<div id="f0e2d3a9" class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>final_audio <span class="op">=</span> <span class="va">None</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> speaker, text <span class="kw">in</span> tqdm(podcaster.rewritten_podcast, desc<span class="op">=</span><span class="st">"Generating podcast segments"</span>, unit<span class="op">=</span><span class="st">"segment"</span>):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> speaker <span class="op">==</span> <span class="st">"Speaker 1"</span>:</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        audio_arr, rate <span class="op">=</span> generate_speaker1_audio(text)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:  <span class="co"># Speaker 2</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        audio_arr, rate <span class="op">=</span> generate_speaker2_audio(text)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to AudioSegment (pydub will handle sample rate conversion automatically)</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    audio_segment <span class="op">=</span> numpy_to_audio_segment(audio_arr, rate)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add to final audio</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> final_audio <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        final_audio <span class="op">=</span> audio_segment</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        final_audio <span class="op">+=</span> audio_segment</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Generating podcast segments:   0%|          | 0/19 [00:00&lt;?, ?segment/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Generating podcast segments:   5%|▌         | 1/19 [01:28&lt;26:27, 88.19s/segment]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  16%|█▌        | 3/19 [05:09&lt;28:23, 106.46s/segment]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  26%|██▋       | 5/19 [08:36&lt;23:11, 99.39s/segment] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  37%|███▋      | 7/19 [11:53&lt;18:47, 93.95s/segment] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  47%|████▋     | 9/19 [16:06&lt;18:18, 109.82s/segment]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  58%|█████▊    | 11/19 [19:32&lt;14:05, 105.64s/segment]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  68%|██████▊   | 13/19 [22:53&lt;10:01, 100.24s/segment]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  79%|███████▉  | 15/19 [25:47&lt;05:56, 89.21s/segment] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments:  89%|████████▉ | 17/19 [28:47&lt;02:52, 86.25s/segment] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Generating podcast segments: 100%|██████████| 19/19 [30:55&lt;00:00, 97.68s/segment]</code></pre>
</div>
</div>
</section>
</section>
<section id="output-the-podcast" class="level3">
<h3 class="anchored" data-anchor-id="output-the-podcast">Output the Podcast</h3>
<p>We can now save this as a mp3 file</p>
<div id="b52bf105" class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>final_audio.export(<span class="st">"./_podcast.mp3"</span>, </span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>                  <span class="bu">format</span><span class="op">=</span><span class="st">"mp3"</span>, </span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                  bitrate<span class="op">=</span><span class="st">"192k"</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                  parameters<span class="op">=</span>[<span class="st">"-q:a"</span>, <span class="st">"0"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>&lt;_io.BufferedRandom name='./_podcast.mp3'&gt;</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/Dmaturana81\.github\.io\/Agents");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/Dmaturana81/Agents/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>