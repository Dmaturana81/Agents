"""Solution to proeprocess text document to do a podcast"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb.

# %% auto 0
__all__ = ['DEFAULT_MODEL', 'SYS_PROMPT', 'client', 'SYS_WRITER', 'MODEL', 'validate_pdf', 'PDFProcessor', 'PodcastWriter']

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 4
from fastcore.utils import Path, patch

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 8
from unstructured.partition.pdf import partition_pdf
from typing import Optional

from langchain.text_splitter import CharacterTextSplitter, SentenceTransformersTokenTextSplitter

import os
from openai import OpenAI

from ..base.llm_clients import BaseClient
import warnings


warnings.filterwarnings('ignore')

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 9
DEFAULT_MODEL = "llama3.2"

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 10
def validate_pdf(file_path: str | Path  ) -> bool:
    """Validate if the file exists and is a PDF

    Arguments:
        file_path -- Path to the file to validate

    Returns:
        bool -- True if the file exists and is a PDF, False otherwise
    """
    file_path = Path(file_path) if isinstance(file_path, str) else file_path    
    if not file_path.exists():
        print(f"Error: File not found at path: {file_path}")
        return False
    if not file_path.suffix.lower() == ".pdf":
        print("Error: File is not a PDF")
        return False
    return True

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 11
class PDFProcessor:
    """Class to handle PDF processing including text extraction and chunking"""
    chunked_docs: dict[str, list[str]] = {}
    processed_docs: dict[str, str] = {}
    paths: list[str] = []
    client: BaseClient
    SYS_PROMPT = """
You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.

The raw data is messed up with new lines, Latex math, other symbols, and you will see fluff that we can remove completely. Basically take away any text that you think might be useless in a podcast author's transcript.

Remember, the podcast could be on any topic whatsoever so first try to identify the topic and then remove the fluff.

Please be smart with what you remove and be creative ok?

Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED

Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.

PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES

ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?
Here is the text:
"""

    def __init__(self):
        """Initialize with optional path(s) to PDF file(s)
        
        Args:
            paths: Single path, list of paths, or None
        """
        self.client = BaseClient.from_name("llama3.2", "ollama")

            
    @property   
    def num_files(self) -> int:
        return len(self.paths)
    
    @property
    def file_paths(self) -> list[Path]:
        return self.paths
    
    @property
    def file_names(self) -> list[str]:
        return [path.split("/")[-1] for path in self.paths]
    
    @property
    def chunks_num(self) -> int:
        if not self.chunked_docs:
            self.run()
        return sum(len(chunk) for chunk in self.chunked_docs.values())
    
    def add_filepath(self, path: str | Path) -> None:
        """Add one or more PDF file paths
        
        Args:
            paths: Single path or list of paths to add
        """
        path = path if isinstance(path, str) else path.as_posix()
        if validate_pdf(path):
            self.paths.append(path)
            self.processed_docs[path] = self.extract_text(path)
            self.chunked_docs[path] = self.chunk_text(self.processed_docs[path])
        else:
            raise ValueError(f"Invalid PDF file: {path}")
        
    def remove_filepath(self, path: str | Path) -> None:
        """Remove a PDF file path
        
        Args:
            path: Path to remove
        """
        path = Path(path) if isinstance(path, str) else path
        if path in self.paths:
            self.paths.remove(path)
            del self.processed_docs[path.as_posix()]
            del self.chunked_docs[path.as_posix()]
        else:
            raise ValueError(f"File not found in paths: {path}")
            
    def extract_text(self, path:Path | str, resolution="fast") -> list[str]:
        """Extract text from all PDFs
        
        Returns:
            List of extracted text documents
        """
        elements = partition_pdf(filename=str(path), strategy=resolution)
        text = "\n".join([str(element) for element in elements])
        return text
    
    def chunk_text(self, text: str, chunk_size: int = 50000, 
                overlap: float = 100) -> list[list[str]]:
        """Chunk documents into smaller pieces using SentenceTransformersTokenTextSplitter
        
        Args:
            documents: List of document texts to chunk
            chunk_size: Maximum number of tokens per chunk
            overlap: Percentage of overlap between chunks (0.0 to 1.0)
            
        Returns:
            List of chunked documents
        """
        splitter = SentenceTransformersTokenTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap
        )
        
        chunks = splitter.split_text(text)
        return chunks
    
    def run(self):
        documents = self.extract_text()
        chunked_docs = self.chunk_text(documents)
        processed_texts = self.process_chunks()
        return self.cleanned_text
    

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 12
@patch
def process_chunks(self:PDFProcessor, files: list[str] | None = None):
    """Process a chunk of text and return both input and output for verification"""

    processed_text = []
    files = files if files else self.file_paths
    for file in files:
        for chunk in self.chunked_docs[file]:
            conversation = [
                {"role": "system", "content": self.SYS_PROMPT},
                {"role": "user", "content": chunk},
            ]
            generation = client.chat.completions.create(
                model=DEFAULT_MODEL,
                messages=conversation
            )
            # Print chunk information for monitoring
            #print(f"\n{'='*40} Chunk {chunk_num} {'='*40}")
            processed_text.append(generation.choices[0].message.content)

    self.cleanned_text = " ".join(processed_text)    
    return processed_text        



# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 20
SYS_PROMPT = """
You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.

The raw data is messed up with new lines, Latex math, other symbols, and you will see fluff that we can remove completely. Basically take away any text that you think might be useless in a podcast author's transcript.

Remember, the podcast could be on any topic whatsoever so first try to identify the topic and then remove the fluff.

Please be smart with what you remove and be creative ok?

Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED

Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.

PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES

ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?
Here is the text:
"""

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 23
client = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")


# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 30
SYS_WRITER = """
You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. 
We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.
You have won multiple podcast awards for your writing.
Your job is to write word by word, even "umm, hmmm, right" interruptions by the second speaker based on the text provided. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. 
Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc
Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes
Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions

The following rules are very important:
- Make sure the tangents speaker 2 provides are quite wild or interesting. 
- Ensure there are interruptions during explanations or there are "hmm" and "umm" injected throughout from the second speaker. 
- It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait
- ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: 
- DO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH
- DO NOT GIVE CHAPTER TITLES
- IT SHOULD STRICTLY BE THE DIALOGUES
- THE PODCAST DURATION IS 30 MINUTES
"""

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 33
MODEL = "gpt-4o-mini"
#| export

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 40
from openai import OpenAI

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 43
class PodcastWriter:
    SYS_WRITER: str = SYS_WRITER
    SYS_REWRITER: str = SYS_REWRITER
    processor: PDFProcessor 
    model_big: str = "gpt-4o-mini"
    model_medium: str = "llama3."
    model_small: str = "llama3.2"

    def __init__(self, processor: PDFProcessor):
        self.processor = processor


# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 44
@patch
def generate_podcast(self:PodcastWriter):
    text = self.processor.cleanned_text
    messages = [
        {"role": "system", "content": self.SYS_WRITER},
        {"role": "user", "content": text},
    ]

    outputs = client.chat.completions.create(
        model=self.model_big,
        messages=messages,
        temperature=1,
    )
    self.podcast_text = outputs.choices[0].message.content

# %% ../../nbs/Step-1 PDF-Pre-Processing-Logic.ipynb 53
@patch
def rewrite_podcast(self:PodcastWriter):
    text = self.podcast_text
    messages = [
        {"role": "system", "content": self.SYS_REWRITER},
        {"role": "user", "content": text},
    ]

    outputs = client.chat.completions.create(
        model=self.model_medium,
        messages=messages,
        temperature=1,
    )
    self.rewritten_podcast = outputs.choices[0].message.content
    return self.rewritten_podcast

