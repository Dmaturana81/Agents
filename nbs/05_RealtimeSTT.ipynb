{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31msniffio._impl.AsyncLibraryNotFoundError: unknown async library, or not in async context. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS-parler\n",
    "\n",
    "> Planning agent using instructor library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents.realtimetts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRealtimeSTT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioToTextRecorder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      4\u001b[0m     recorder \u001b[38;5;241m=\u001b[39m AudioToTextRecorder()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Agents-uluWm89Z/lib/python3.12/site-packages/RealtimeSTT/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_recorder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioToTextRecorder\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_recorder_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioToTextRecorderClient\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Agents-uluWm89Z/lib/python3.12/site-packages/RealtimeSTT/audio_recorder.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signal\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msystem_signal\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaster_whisper\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenwakeword\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Agents-uluWm89Z/lib/python3.12/site-packages/faster_whisper/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decode_audio\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranscribe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WhisperModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m available_models, download_model, format_timestamp\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Agents-uluWm89Z/lib/python3.12/site-packages/faster_whisper/audio.py:15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryIO, Union\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mav\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_audio\u001b[39m(\n\u001b[1;32m     20\u001b[0m     input_file: Union[\u001b[38;5;28mstr\u001b[39m, BinaryIO],\n\u001b[1;32m     21\u001b[0m     sampling_rate: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m,\n\u001b[1;32m     22\u001b[0m     split_stereo: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Agents-uluWm89Z/lib/python3.12/site-packages/av/__init__.py:20\u001b[0m\n\u001b[1;32m     12\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), os\u001b[38;5;241m.\u001b[39mpardir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav.libs\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpathsep\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# MUST import the core before anything else in order to initalize the underlying\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# library that is being wrapped.\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mav\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time_base, library_versions\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Capture logging (by importing it).\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mav\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "#| export\n",
    "#| eval:false\n",
    "from RealtimeSTT import AudioToTextRecorder\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    recorder = AudioToTextRecorder()\n",
    "    recorder.start()\n",
    "    input(\"Press Enter to stop recording...\")\n",
    "    recorder.stop()\n",
    "    print(\"Transcription: \", recorder.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-31 15:29:35.402] [ctranslate2] [thread 1195168] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: eak now⠹ speak now⠸ speak now⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording transcribing⠙ transcribing Vamos a practicar.\n",
      "RealtimeSTT shutting down\n",
      "Error in callback <function Halo.__init__.<locals>.clean_up> (for post_run_cell), with arguments args (<ExecutionResult object at 302a65880, execution_count=3 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 302a65a90, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "\n",
      "# if ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/matu/Xcode/Agents/nbs/04_tts.parler%20copy.ipynb#X20sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up> (for post_run_cell), with arguments args (<ExecutionResult object at 302a65880, execution_count=3 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 302a65a90, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "\n",
      "# if ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/matu/Xcode/Agents/nbs/04_tts.parler%20copy.ipynb#X20sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up> (for post_run_cell), with arguments args (<ExecutionResult object at 302a65880, execution_count=3 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 302a65a90, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "\n",
      "# if ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/matu/Xcode/Agents/nbs/04_tts.parler%20copy.ipynb#X20sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up> (for post_run_cell), with arguments args (<ExecutionResult object at 302a65880, execution_count=3 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 302a65a90, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "\n",
      "# if ..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/matu/Xcode/Agents/nbs/04_tts.parler%20copy.ipynb#X20sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "from RealtimeSTT import AudioToTextRecorder\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with AudioToTextRecorder() as recorder:\n",
    "    print(\"Transcription: \", recorder.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matu/.local/share/virtualenvs/Agents-uluWm89Z/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Flash attention 2 is not installed\n"
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "#| export\n",
    "\n",
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval:false\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matu/.local/share/virtualenvs/Agents-uluWm89Z/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'parler_tts.dac_wrapper.modeling_dac.DACModel'> is overwritten by shared audio_encoder config: DACConfig {\n",
      "  \"_name_or_path\": \"parler-tts/dac_44khZ_8kbps\",\n",
      "  \"architectures\": [\n",
      "    \"DACModel\"\n",
      "  ],\n",
      "  \"codebook_size\": 1024,\n",
      "  \"frame_rate\": 86,\n",
      "  \"latent_dim\": 1024,\n",
      "  \"model_bitrate\": 8,\n",
      "  \"model_type\": \"dac_on_the_hub\",\n",
      "  \"num_codebooks\": 9,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": false,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval:false\n",
    "\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval:false\n",
    "\n",
    "prompt = \"Ola, tudo bem?\"\n",
    "description = \"A brazilian customer service representative, female speaker. The voice is slightly expressive and animated, with a moderate speed and pitch. The recording is of very high quality and in portuguese, with the speaker's voice sounding clear and very close up.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval:false\n",
    "\n",
    "input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval:false\n",
    "\n",
    "generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval:false\n",
    "\n",
    "sf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31msniffio._impl.AsyncLibraryNotFoundError: unknown async library, or not in async context. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
